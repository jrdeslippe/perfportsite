{
    "docs": [
        {
            "location": "/", 
            "text": "Portability Across DOE Office of Science HPC Facilities\n\n\nAs the HPC community prepares for exascale and the semiconductor industry approaches the end of Moore's Law in terms of transistor size, we have entered a \nperiod of time of increased diversity in computer architecture for High Performance Computing (HPC) with relatively new designs joining mature \nprocessor and memory \ntechnologies. These technologies include GPUs, Many-Core Processors, ARM, FPGAs, and ASICs, as well as new memory technologies in the form of High-Bandwidth \nMemory (HBM) often incorporated on the processor die as well as Non-Volatile memory (NVRAM) and Solid-State Disk (SSD) technology for accelerated IO. \n\n\nThe DOE Office of Science operates three world-leading HPC facilities located at the Argonne Leadership Computing Facility (ALCF), the National Energy Research \nScientific Computing Center (NERSC) at Lawrence Berkeley Lab, and the Oak Ridge Leadership Computing Center (OLCF). These facilities field three of the most \npowerful supercomputers in world. These machines are used by scientists throughout the DOE Office of Science and the world for solving a \nnumber of important problems in domains including materials science and chemistry to nuclear, particle, and astrophysics. \n\n\nThese facilities, with their latest systems, have begun the transition for DOE users to energy-efficient HPC architectures. The facilities are currently \nfielding systems with two-distinct \"pre-exascale\" architectures that we discuss in detail here. \n\n\n\n\n\n\n\n\nSystem\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nOLCF\n\n\n\n\n\n\nArchitecture\n\n\nCPU + GPU\n\n\n\n\n\n\nScale\n\n\n18,688 Nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem\n\n\nCori\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nNERSC\n\n\n\n\n\n\nArchitecture\n\n\nXeon-Phi\n\n\n\n\n\n\nScale\n\n\n9688 Nodes\n\n\n\n\n\n\nNotes\n\n\nSSD Burst-Buffer IO layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem\n\n\nTheta\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nALCF\n\n\n\n\n\n\nArchitecture\n\n\nXeon-Phi\n\n\n\n\n\n\nScale\n\n\n3624 Nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two processor architectures deployed on these systems are the CPU+GPU hybrid architecture on Titan and the \"self-hosted\" Xeon-Phi processors \n(code named \"Knights Landing\"). These two architectures, while seemingly quite different at first, have a number of similarities that we believe \nrepresent general trends in exascale-like architectures:\n\n\n\n\nIncrease parallelism (Cores, Threads, Warps/SMs/Blocks)\n\n\nVectorization (AVX512 8 Wide Vector Units, 32 Wide Warps)\n\n\nSmall Amount High-bandwidth Coupled with Large Amounts of Traditional DDR\n\n\n\n\nWhile the details of the architectures are distinct, and vendor specific programming libraries/languages (CUDA, AVX512 Intrinsics etc.) exist to address \nspecific architecture features; the commonalities are significant enough that a number of portable programming approaches are emerging for writing code that \nsupports both architectures. \n\n\nThis website is intended to be a living/growing documentation hub and guide for applications teams targeting systems at multiple DOE office of science \nfacilities. In these pages, we \ndiscuss the differences between the systems, the software environment, and the job-submission process. We discuss how to define and measure performance \nportability and we provide recommendations based on case studies for the most promising performance-portable programming approaches. A \n\nsummary\n of the high level findings and recommendations will be maintained.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#portability-across-doe-office-of-science-hpc-facilities", 
            "text": "As the HPC community prepares for exascale and the semiconductor industry approaches the end of Moore's Law in terms of transistor size, we have entered a \nperiod of time of increased diversity in computer architecture for High Performance Computing (HPC) with relatively new designs joining mature \nprocessor and memory \ntechnologies. These technologies include GPUs, Many-Core Processors, ARM, FPGAs, and ASICs, as well as new memory technologies in the form of High-Bandwidth \nMemory (HBM) often incorporated on the processor die as well as Non-Volatile memory (NVRAM) and Solid-State Disk (SSD) technology for accelerated IO.   The DOE Office of Science operates three world-leading HPC facilities located at the Argonne Leadership Computing Facility (ALCF), the National Energy Research \nScientific Computing Center (NERSC) at Lawrence Berkeley Lab, and the Oak Ridge Leadership Computing Center (OLCF). These facilities field three of the most \npowerful supercomputers in world. These machines are used by scientists throughout the DOE Office of Science and the world for solving a \nnumber of important problems in domains including materials science and chemistry to nuclear, particle, and astrophysics.   These facilities, with their latest systems, have begun the transition for DOE users to energy-efficient HPC architectures. The facilities are currently \nfielding systems with two-distinct \"pre-exascale\" architectures that we discuss in detail here.      System  Titan      Location  OLCF    Architecture  CPU + GPU    Scale  18,688 Nodes            System  Cori      Location  NERSC    Architecture  Xeon-Phi    Scale  9688 Nodes    Notes  SSD Burst-Buffer IO layer            System  Theta      Location  ALCF    Architecture  Xeon-Phi    Scale  3624 Nodes         The two processor architectures deployed on these systems are the CPU+GPU hybrid architecture on Titan and the \"self-hosted\" Xeon-Phi processors \n(code named \"Knights Landing\"). These two architectures, while seemingly quite different at first, have a number of similarities that we believe \nrepresent general trends in exascale-like architectures:   Increase parallelism (Cores, Threads, Warps/SMs/Blocks)  Vectorization (AVX512 8 Wide Vector Units, 32 Wide Warps)  Small Amount High-bandwidth Coupled with Large Amounts of Traditional DDR   While the details of the architectures are distinct, and vendor specific programming libraries/languages (CUDA, AVX512 Intrinsics etc.) exist to address \nspecific architecture features; the commonalities are significant enough that a number of portable programming approaches are emerging for writing code that \nsupports both architectures.   This website is intended to be a living/growing documentation hub and guide for applications teams targeting systems at multiple DOE office of science \nfacilities. In these pages, we \ndiscuss the differences between the systems, the software environment, and the job-submission process. We discuss how to define and measure performance \nportability and we provide recommendations based on case studies for the most promising performance-portable programming approaches. A  summary  of the high level findings and recommendations will be maintained.", 
            "title": "Portability Across DOE Office of Science HPC Facilities"
        }, 
        {
            "location": "/facilities/overview/", 
            "text": "Office of Science Computing Facilities\n\n\nThe \nAdvanced Scientific Computing Research\n\nprogram in DOE Office of Science sponsors three computing facilities - \nthe\nArgonne Leadership Computing Facility\n (ALCF), the\n\nOak Ridge Leadership Computing Facility\n (OLCF),\nand the \nNational Energy Research Scientific Computing\nCenter\n (NERSC). Below we summarize the technical\nspecifications of the current or upcoming computing systems deployed at each\nfacility.\n\n\n\n\n\n\n\n\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nFacility\n\n\nNERSC\n\n\nALCF\n\n\nOLCF\n\n\n\n\n\n\nModel\n\n\nCray XC40\n\n\nCray XC40\n\n\nCray XK7\n\n\n\n\n\n\nProcessor\n\n\nIntel Xeon Phi 7250 (\"Knights Landing\")\n\n\nIntel Xeon Phi 7230 (\"Knights Landing\")\n\n\nAMD Opteron 6274 (\"Interlagos\")\n\n\n\n\n\n\nAccelerator\n\n\n(none)\n\n\n(none)\n\n\nNVIDIA Tesla K20X (\"Kepler\")\n\n\n\n\n\n\n# Nodes\n\n\n9 688\n\n\n3 624\n\n\n18 688\n\n\n\n\n\n\nPerf. per Node\n\n\n2.6 TF\n\n\n2.6 TF\n\n\n1.4 TF\n\n\n\n\n\n\nPeak Perf.\n\n\n30 PF\n\n\n10 PF\n\n\n27 PF", 
            "title": "Overview"
        }, 
        {
            "location": "/facilities/overview/#office-of-science-computing-facilities", 
            "text": "The  Advanced Scientific Computing Research \nprogram in DOE Office of Science sponsors three computing facilities -  the\nArgonne Leadership Computing Facility  (ALCF), the Oak Ridge Leadership Computing Facility  (OLCF),\nand the  National Energy Research Scientific Computing\nCenter  (NERSC). Below we summarize the technical\nspecifications of the current or upcoming computing systems deployed at each\nfacility.      Cori  Theta  Titan      Facility  NERSC  ALCF  OLCF    Model  Cray XC40  Cray XC40  Cray XK7    Processor  Intel Xeon Phi 7250 (\"Knights Landing\")  Intel Xeon Phi 7230 (\"Knights Landing\")  AMD Opteron 6274 (\"Interlagos\")    Accelerator  (none)  (none)  NVIDIA Tesla K20X (\"Kepler\")    # Nodes  9 688  3 624  18 688    Perf. per Node  2.6 TF  2.6 TF  1.4 TF    Peak Perf.  30 PF  10 PF  27 PF", 
            "title": "Office of Science Computing Facilities"
        }, 
        {
            "location": "/facilities/tools/", 
            "text": "Performance Analysis Tools\n\n\nEvaluating application performance portability across diverse computing\narchitectures often requires the aid of performance analysis tools. Such tools\nprovide detailed information and statistics characterizing an application's\nusage of the architecture, and can guide the developer as she optimizes\nbottlenecks to achieve higher performance.\n\n\nEach ASCR facility is equipped with a wide range of tools for measuring\napplication performance. The applications running at the three facilities\nexhibit a broad range of demands from computer architectures - some are limited\nby memory bandwidth, others by latency, and others still by the CPU itself. The\nperformance measurement tools available at the ASCR facilities can measure in\ndetail how an application uses each of these resources. They include, but are\nnot limited to, the list provided below. The description of each tool is copied\nfrom its official documentation.\n\n\n\n\nAllinea MAP\n:\n  Allinea MAP is the profiler for parallel, multithreaded or single threaded C,\n  C++, Fortran and F90 codes. It provides in depth analysis and bottleneck\n  pinpointing to the source line.\n\n\nCray Performance Measurement and Analysis\n  Tools\n:\n  The Cray Performance Measurement and Analysis Tools (or CrayPat) are a suite\n  of utilities that enable the user to capture and analyze performance data\n  generated during the execution of a program on a Cray system. The information\n  collected and analysis produced by use of these tools can help the user to\n  find answers to two fundamental programming questions: \nHow fast is my\n  program running?\n and \nHow can I make it run faster?\n\n\nHPCToolkit\n: HPCToolkit is an integrated suite of\n  tools for measurement and analysis of program performance on computers\n  ranging from multicore desktop systems to the nation's largest\n  supercomputers. By using statistical sampling of timers and hardware\n  performance counters, HPCToolkit collects accurate measurements of a\n  program's work, resource consumption, and inefficiency and attributes them\n  to the full calling context in which they occur. HPCToolkit works with\n  multilingual, fully optimized applications that are statically or\n  dynamically linked.\n\n\nIntel Advisor\n:\n  Intel Advisor is used early in the process of adding vectorization into your\n  code, or while converting parts of a serial program to a parallel\n  (multithreaded) program. It helps you explore and locate areas in which the\n  optimizations might provide significant benefit. It also helps you predict the\n  costs and benefits of adding vectorization or parallelism to those parts of\n  your program, allowing you to experiment.\n\n\nIntel VTune Amplifier\n:\n  Intel VTune Amplifier is a performance analysis tool targeted for users\n  developing serial and multithreaded applications.\n\n\nnvprof\n:\n  nvprof enables the collection of a timeline of CUDA-related activities on both\n  CPU and GPU, including kernel execution, memory transfers, memory set and CUDA\n  API calls and events or metrics for CUDA kernels.\n\n\nTuning and Analysis Utilities (TAU)\n:\n  TAU Performance System is a portable profiling and tracing toolkit for\n  performance analysis of parallel programs written in Fortran, C, C++, UPC,\n  Java, Python.\n\n\n\n\nUsing Tools on ASCR Facility Systems\n\n\nBelow are brief instructions and links to documentation or presentations on\nusing some of the performance analysis tools on the current systems.\n\n\n\n\nJump to:\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\nCori\n\n\nAllinea MAP\n\n\nNERSC's \ndocumentation on\nMAP\n\nexplains the software environment setup, how to run MAP on Cori using the GUI\nclient or command-line mode. It also discusses looking at profiling results in\nthe GUI.\n\n\nCrayPAT\n\n\nNERSC's \ndocumentation on\nCrayPAT\n\nexplains how to set up and use CrayPAT on Cori. It includes hot to use the\nCray Apprentice2 GUI to visualize performance data and Cray Reveal for\nloopmark and source code analysis.\n\n\nIntel Advisor\n\n\nNERSC's \ndocumentation on\nAdvisor\n\nexplains how to use it on Cori, including how to launch jobs and how to use\nthe GUI to view results.\n\n\nIntel VTune Amplifier\n\n\nNERSC's \ndocumentation on\nVTune\n\nexplains how to use VTune Amplifier XE on Cori, including module setup,\nlinking with \n-dynamic\n, and compiling with \n-g\n. It also has example job\nscripts for collecting different kinds of profiling data and a section on\nusing the VTUne GUI.  ```\n\n\nTheta\n\n\nAllinea MAP\n\n\nRyan Huylguin's\n\npresentation\n\nexplains the basic setup and gives example \naprun\n syntax for running under\nAllinea MAP. You use this syntax in the \naprun\n command in your Cobalt job\nscript.\n\n\nCrayPAT\n\n\nA Cray\n\npresentation\n\ngiven at ALCF describes how to load appropriate modules and build your code to\nuse CrayPAT, both the \"lite\" and full versions. You then run your code using\nthe normal Cobalt job script. Note that you must first load the module to\nselect the Cray programming environment (and compile/recompile your code with\nthat environment). The default is the Intel environment, so if you have not\nchanged it here is a command to switch to the Cray environment:\n\n\nmodule swap PrgEnv-intel PrgEnv-cray\n\n\n\n\nHPCToolkit\n\n\nMark Krentel's \npresentation\n\nhas quick start information for using HPCToolkit on Theta. In the \naprun\n\ncommand in your job script, you insert \nhpcstruct\n before your executable\nprogram name.\n\n\nIntel VTune Amplifier\n\n\nALCF's \ndocumentation\n has\nbasic steps to use VTune on Theta, with an example run script. It also\nexplains how to selectively profile a subset of all MPI ranks.\n\n\nTuning and Analysis Utilities (TAU)\n\n\nFor all TAU usage modes, you should first load the TAU module:\n\n\nmodule load tau\n\n\n\n\nThe Hands-On section of Sameer Shende's\n\npresentation\n\nillustrates using TAU on Theta via a Cobalt interactive session (\nqsub\n-I\n). You may also run a normal batch job, inserting the \ntau_exec\n command\nbefore your executable program name in the \naprun\n command in your Cobalt\nbatch script. To use TAU without recompiling your code, you must have linked\nit as a dynamic executable (link using \n-dynamic\n), and you should have\ncompiled and linked with \n-g\n. To use with compiler and/or explicit\nsource-code instrumentation, you should compile using the TAU compiler\nwrappers as explained in the presentation.\n\n\nTitan\n\n\nCrayPAT\n\n\nOLCF's \ndocumentation on\nCrayPAT\n includes a\n10-step usage guide for basic analysis of your program on Titan using\nCrayPAT. It explains using the \npat_report\n tool for text reports and\nApprentice2 for GUI analysis. There are more details on the \nOLCF CrayPAT\nsoftware page\n.\n\n\nNVPROF\n\n\nOLCF's \ndocumentation on accelerator performance\ntools\n explains\nhow to set up your environment and run using the NVPROF profiler to gather\nperformance data from the GPUs.\n\n\nTuning and Analysis Utilities (TAU)\n\n\nOLCF's \ndocumentation on accelerator performance\ntools\n briefly\nexplains how use TAU profiling and tracing tools for CPU-GPU hybrid\nprograms. There are more details on the \nOLCF TAU software\npage\n.", 
            "title": "Tools"
        }, 
        {
            "location": "/facilities/tools/#performance-analysis-tools", 
            "text": "Evaluating application performance portability across diverse computing\narchitectures often requires the aid of performance analysis tools. Such tools\nprovide detailed information and statistics characterizing an application's\nusage of the architecture, and can guide the developer as she optimizes\nbottlenecks to achieve higher performance.  Each ASCR facility is equipped with a wide range of tools for measuring\napplication performance. The applications running at the three facilities\nexhibit a broad range of demands from computer architectures - some are limited\nby memory bandwidth, others by latency, and others still by the CPU itself. The\nperformance measurement tools available at the ASCR facilities can measure in\ndetail how an application uses each of these resources. They include, but are\nnot limited to, the list provided below. The description of each tool is copied\nfrom its official documentation.   Allinea MAP :\n  Allinea MAP is the profiler for parallel, multithreaded or single threaded C,\n  C++, Fortran and F90 codes. It provides in depth analysis and bottleneck\n  pinpointing to the source line.  Cray Performance Measurement and Analysis\n  Tools :\n  The Cray Performance Measurement and Analysis Tools (or CrayPat) are a suite\n  of utilities that enable the user to capture and analyze performance data\n  generated during the execution of a program on a Cray system. The information\n  collected and analysis produced by use of these tools can help the user to\n  find answers to two fundamental programming questions:  How fast is my\n  program running?  and  How can I make it run faster?  HPCToolkit : HPCToolkit is an integrated suite of\n  tools for measurement and analysis of program performance on computers\n  ranging from multicore desktop systems to the nation's largest\n  supercomputers. By using statistical sampling of timers and hardware\n  performance counters, HPCToolkit collects accurate measurements of a\n  program's work, resource consumption, and inefficiency and attributes them\n  to the full calling context in which they occur. HPCToolkit works with\n  multilingual, fully optimized applications that are statically or\n  dynamically linked.  Intel Advisor :\n  Intel Advisor is used early in the process of adding vectorization into your\n  code, or while converting parts of a serial program to a parallel\n  (multithreaded) program. It helps you explore and locate areas in which the\n  optimizations might provide significant benefit. It also helps you predict the\n  costs and benefits of adding vectorization or parallelism to those parts of\n  your program, allowing you to experiment.  Intel VTune Amplifier :\n  Intel VTune Amplifier is a performance analysis tool targeted for users\n  developing serial and multithreaded applications.  nvprof :\n  nvprof enables the collection of a timeline of CUDA-related activities on both\n  CPU and GPU, including kernel execution, memory transfers, memory set and CUDA\n  API calls and events or metrics for CUDA kernels.  Tuning and Analysis Utilities (TAU) :\n  TAU Performance System is a portable profiling and tracing toolkit for\n  performance analysis of parallel programs written in Fortran, C, C++, UPC,\n  Java, Python.", 
            "title": "Performance Analysis Tools"
        }, 
        {
            "location": "/facilities/tools/#using-tools-on-ascr-facility-systems", 
            "text": "Below are brief instructions and links to documentation or presentations on\nusing some of the performance analysis tools on the current systems.   Jump to:  Cori  Theta  Titan", 
            "title": "Using Tools on ASCR Facility Systems"
        }, 
        {
            "location": "/facilities/tools/#allinea-map", 
            "text": "NERSC's  documentation on\nMAP \nexplains the software environment setup, how to run MAP on Cori using the GUI\nclient or command-line mode. It also discusses looking at profiling results in\nthe GUI.", 
            "title": "Allinea MAP"
        }, 
        {
            "location": "/facilities/tools/#craypat", 
            "text": "NERSC's  documentation on\nCrayPAT \nexplains how to set up and use CrayPAT on Cori. It includes hot to use the\nCray Apprentice2 GUI to visualize performance data and Cray Reveal for\nloopmark and source code analysis.", 
            "title": "CrayPAT"
        }, 
        {
            "location": "/facilities/tools/#intel-advisor", 
            "text": "NERSC's  documentation on\nAdvisor \nexplains how to use it on Cori, including how to launch jobs and how to use\nthe GUI to view results.", 
            "title": "Intel Advisor"
        }, 
        {
            "location": "/facilities/tools/#intel-vtune-amplifier", 
            "text": "NERSC's  documentation on\nVTune \nexplains how to use VTune Amplifier XE on Cori, including module setup,\nlinking with  -dynamic , and compiling with  -g . It also has example job\nscripts for collecting different kinds of profiling data and a section on\nusing the VTUne GUI.  ```", 
            "title": "Intel VTune Amplifier"
        }, 
        {
            "location": "/facilities/tools/#allinea-map_1", 
            "text": "Ryan Huylguin's presentation \nexplains the basic setup and gives example  aprun  syntax for running under\nAllinea MAP. You use this syntax in the  aprun  command in your Cobalt job\nscript.", 
            "title": "Allinea MAP"
        }, 
        {
            "location": "/facilities/tools/#craypat_1", 
            "text": "A Cray presentation \ngiven at ALCF describes how to load appropriate modules and build your code to\nuse CrayPAT, both the \"lite\" and full versions. You then run your code using\nthe normal Cobalt job script. Note that you must first load the module to\nselect the Cray programming environment (and compile/recompile your code with\nthat environment). The default is the Intel environment, so if you have not\nchanged it here is a command to switch to the Cray environment:  module swap PrgEnv-intel PrgEnv-cray", 
            "title": "CrayPAT"
        }, 
        {
            "location": "/facilities/tools/#hpctoolkit", 
            "text": "Mark Krentel's  presentation \nhas quick start information for using HPCToolkit on Theta. In the  aprun \ncommand in your job script, you insert  hpcstruct  before your executable\nprogram name.", 
            "title": "HPCToolkit"
        }, 
        {
            "location": "/facilities/tools/#intel-vtune-amplifier_1", 
            "text": "ALCF's  documentation  has\nbasic steps to use VTune on Theta, with an example run script. It also\nexplains how to selectively profile a subset of all MPI ranks.", 
            "title": "Intel VTune Amplifier"
        }, 
        {
            "location": "/facilities/tools/#tuning-and-analysis-utilities-tau", 
            "text": "For all TAU usage modes, you should first load the TAU module:  module load tau  The Hands-On section of Sameer Shende's presentation \nillustrates using TAU on Theta via a Cobalt interactive session ( qsub\n-I ). You may also run a normal batch job, inserting the  tau_exec  command\nbefore your executable program name in the  aprun  command in your Cobalt\nbatch script. To use TAU without recompiling your code, you must have linked\nit as a dynamic executable (link using  -dynamic ), and you should have\ncompiled and linked with  -g . To use with compiler and/or explicit\nsource-code instrumentation, you should compile using the TAU compiler\nwrappers as explained in the presentation.", 
            "title": "Tuning and Analysis Utilities (TAU)"
        }, 
        {
            "location": "/facilities/tools/#craypat_2", 
            "text": "OLCF's  documentation on\nCrayPAT  includes a\n10-step usage guide for basic analysis of your program on Titan using\nCrayPAT. It explains using the  pat_report  tool for text reports and\nApprentice2 for GUI analysis. There are more details on the  OLCF CrayPAT\nsoftware page .", 
            "title": "CrayPAT"
        }, 
        {
            "location": "/facilities/tools/#nvprof", 
            "text": "OLCF's  documentation on accelerator performance\ntools  explains\nhow to set up your environment and run using the NVPROF profiler to gather\nperformance data from the GPUs.", 
            "title": "NVPROF"
        }, 
        {
            "location": "/facilities/tools/#tuning-and-analysis-utilities-tau_1", 
            "text": "OLCF's  documentation on accelerator performance\ntools  briefly\nexplains how use TAU profiling and tracing tools for CPU-GPU hybrid\nprograms. There are more details on the  OLCF TAU software\npage .", 
            "title": "Tuning and Analysis Utilities (TAU)"
        }, 
        {
            "location": "/facilities/comparison/", 
            "text": "Comparison of Systems\n\n\nBelow we compare in depth the Cori, Theta and Titan systems, software environment and job submission process to aid office of science users in utilizing \nmultiple resources.\n\n\nHardware In-Depth\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nFacility\n\n\nNERSC\n\n\nALCF\n\n\nOLCF\n\n\n\n\n\n\nModel\n\n\nCray XC40\n\n\nCray XC40\n\n\nCray XK7\n\n\n\n\n\n\nProcessor\n\n\nIntel Xeon Phi 7250 (\"Knights Landing\")\n\n\nIntel Xeon Phi 7230 (\"Knights Landing\")\n\n\nAMD Opteron 6274 (\"Interlagos\")\n\n\n\n\n\n\nProcessor Cores\n\n\n68\n\n\n64\n\n\n16 CPU cores (2668 (896) SP (DP) CUDA cores on K20X GPU)\n\n\n\n\n\n\nProcessor Base Frequency\n\n\n1.4 GHz\n\n\n1.3 GHz\n\n\n2.2 GHz\n\n\n\n\n\n\nProcessor Max Frequency\n\n\n1.6 GHz\n\n\n1.5 GHz\n\n\n3.1 GHz (disabled)\n\n\n\n\n\n\nOn-Device Memory\n\n\n16 GB MCDRAM\n\n\n16 GB MCDRAM\n\n\n(6 GB GDDR5 on K20X GPU)\n\n\n\n\n\n\nProcessor DRAM\n\n\n96 GB DDR4\n\n\n192 GB DDR4\n\n\n32 GB DDR3\n\n\n\n\n\n\nAccelerator\n\n\n(none)\n\n\n(none)\n\n\nNVIDIA Tesla K20X (\"Kepler\")\n\n\n\n\n\n\nNodes\n\n\n9 688\n\n\n3 624\n\n\n18 688\n\n\n\n\n\n\nPerf. Per Node\n\n\n2.6 TF\n\n\n2.6 TF\n\n\n1.4 TF\n\n\n\n\n\n\nNode local storage\n\n\n(none)\n\n\n128 GB SSD\n\n\n(none)\n\n\n\n\n\n\nExternal Burst Buffer\n\n\n1.8 PB\n\n\n(none)\n\n\n(none)\n\n\n\n\n\n\nParallel File System\n\n\n30 PB Lustre\n\n\n10 PB Lustre\n\n\n28 PB Lustre\n\n\n\n\n\n\nInterconnect\n\n\nCray Aries\n\n\nCray Aries\n\n\nCray Gemini\n\n\n\n\n\n\nTopology\n\n\nDragonfly\n\n\nDragonfly\n\n\n3D torus\n\n\n\n\n\n\nPeak Perf\n\n\n30 PF\n\n\n10 PF\n\n\n27 PF\n\n\n\n\n\n\n\n\nSoftware Environment\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nSoftware environment management\n\n\nmodules\n\n\nmodules\n\n\nmodules\n\n\n\n\n\n\nBatch Job Scheduler\n\n\nSlurm\n\n\nCobalt\n\n\nPBS\n\n\n\n\n\n\nCompilers\n\n\n\n\n\n\n\n\n\n\n\n\nIntel\n\n\n(\ndefault\n) \nmodule load PrgEnv-intel\n\n\n(\ndefault\n) \nmodule load PrgEnv-intel\n\n\nmodule load PrgEnv-intel\n\n\n\n\n\n\nCray\n\n\nmodule load PrgEnv-cray\n\n\nmodule load PrgEnv-cray\n\n\nmodule load PrgEnv-cray\n\n\n\n\n\n\nGNU\n\n\nmodule load PrgEnv-gnu\n\n\nmodule load PrgEnv-gnu\n\n\nmodule load PrgEnv-gnu\n\n\n\n\n\n\nPGI\n\n\nn/a\n\n\nn/a\n\n\n(\ndefault\n) \nmodule load PrgEnv-pgi\n\n\n\n\n\n\nCLANG\n\n\nn/a\n\n\nmodule load PrgEnv-llvm\n\n\nn/a\n\n\n\n\n\n\nInterpreters\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngcc + MKL: \nmodule load R\n \n Cray: \nmodule load cray-R\n\n\nmodule load cray-R\n\n\nmodule load r\n\n\n\n\n\n\nPython 2\n\n\nAnaconda + Intel MKL: \nmodule load python/2.7-anaconda\n\n\nCray: \nmodule load cray-python\n Intel: \nmodule load intelpython26\n\n\nmodule load python_anaconda\n\n\n\n\n\n\nPython 3\n\n\nAnaconda + Intel MKL: \nmodule load python/3.5-anaconda\n\n\nIntel: \nmodule load intelpython35\n\n\nmodule load python_anaconda3\n\n\n\n\n\n\nLibraries\n\n\n\n\n\n\n\n\n\n\n\n\nFFT\n\n\nFFTW: \nmodule load fftw\n \n Cray FFTW: \nmodule load cray-fftw\n \n Intel MKL: \nautomatic with Intel compilers\n\n\nFFTW: \nmodule load fftw\n \n Cray FFTW: \nmodule load cray-fftw\n \n Intel MKL: \nautomatic with Intel compilers\n\n\nFFTW: \nmodule load fftw\n \n Cray FFTW: \nmodule load cray-fftw\n\n\n\n\n\n\nCray LibSci\n\n\n(\ndefault\n) \nmodule load cray-libsci\n\n\nmodule load cray-libsci\n\n\nmodule load cray-libsci\n\n\n\n\n\n\nIntel MKL\n\n\nautomatic with Intel compilers\n\n\nautomatic with Intel compilers\n\n\nautomatic with Intel compilers\n\n\n\n\n\n\nTrilinos\n\n\nmodule load cray-trilinos\n\n\nmodule load cray-trilinos\n\n\nmodule load cray-trilinos\n\n\n\n\n\n\nPETSc\n\n\nmodule load cray-petsc\n\n\nmodule load cray-petsc\n\n\nmodule load cray-petsc\n\n\n\n\n\n\nSHMEM\n\n\nmodule load cray-shmem\n\n\nmodule load cray-shmem\n\n\nmodule load cray-shmem\n\n\n\n\n\n\nmemkind\n\n\nmodule load cray-memkind\n\n\nmodule load cray-memkind\n\n\nn/a\n\n\n\n\n\n\nI/O Libraries\n\n\n\n\n\n\n\n\n\n\n\n\nHDF5\n\n\nmodule load cray-hdf5\n\n\nmodule load cray-hdf5\n\n\nmodule load cray-hdf5\n\n\n\n\n\n\nNetCDF\n\n\nmodule load cray-netcdf\n\n\nmodule load cray-netcdf\n\n\nmodule load cray-netcdf\n\n\n\n\n\n\nParallel NetCDF\n\n\nmodule load cray-parallel-netcdf\n\n\nmodule load cray-parallel-netcdf\n\n\nmodule load cray-parallel-netcdf\n\n\n\n\n\n\nPerformance Tools and APIs\n\n\n\n\n\n\n\n\n\n\n\n\nIntel VTune Amplifier\n\n\nmodule load vtune\n\n\nsource /opt/intel/vtune_amplifier_xe/amplxe-vars.sh\n\n\nn/a\n\n\n\n\n\n\nCrayPAT\n\n\nmodule load perftools-base \n module load perftools\n\n\nmodule load perftools\n\n\nmodule load perftools\n\n\n\n\n\n\nPAPI\n\n\nmodule load papi\n\n\nmodule load papi\n\n\nmodule load papi\n\n\n\n\n\n\nDarshan\n\n\n(\ndefault\n) \nmodule load darshan\n\n\nmodule load cray-memkind\n\n\nmodule load darshan\n\n\n\n\n\n\nOther Packages and Frameworks\n\n\n\n\n\n\n\n\n\n\n\n\nShifter\n\n\n(\npart of base system\n)\n\n\nmodule load shifter\n\n\nn/a\n\n\n\n\n\n\n\n\nCompiler Wrappers\n\n\nUse these wrappers to properly cross-compile your source code for the compute\nnodes of the systems, and bring in appropriate headers for MPI, etc.\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nC++\n\n\nCC\n\n\nCC\n\n\nCC\n\n\n\n\n\n\nC\n\n\ncc\n\n\ncc\n\n\ncc\n\n\n\n\n\n\nFortran\n\n\nftn\n\n\nftn\n\n\nftn\n\n\n\n\n\n\n\n\nJob Submission\n\n\nTheta\n\n\nJob Script\n\n\n#!/bin/bash\n\n\n#COBALT -t 30\n\n\n#COBALT --attrs mcdram=cache:numa=quad\n\n\n#COBALT -A \nyourALCFProjectName\n\n\necho\n \nStarting Cobalt job script\n\n\nexport\n \nn_nodes\n=\n$COBALT_JOBSIZE\n\n\nexport\n \nn_mpi_ranks_per_node\n=\n32\n\n\nexport\n \nn_mpi_ranks\n=\n$((\n$n_nodes\n \n*\n \n$n_mpi_ranks_per_node\n))\n\n\nexport\n \nn_openmp_threads_per_rank\n=\n4\n\n\nexport\n \nn_hyperthreads_per_core\n=\n2\n\n\nexport\n \nn_hyperthreads_skipped_between_ranks\n=\n4\n\naprun -n \n$n_mpi_ranks\n -N \n$n_mpi_ranks_per_node\n \n\\\n\n  --env \nOMP_NUM_THREADS\n=\n$n_openmp_threads_per_rank\n -cc depth \n\\\n\n  -d \n$n_hyperthreads_skipped_between_ranks\n \n\\\n\n  -j \n$n_hyperthreads_per_core\n \n\\\n\n  \nexecutable\n \nexecutable args\n\n\n\n\n\nThe \n#COBALT -t 30\n line indicates 30 minutes runtime. Generally, \n#COBALT\n\nlines are equivalent to specifying \nqsub\n command-line arguments.\n\n\nJob Submit Command\n\n\nqsub -n 512 ./theta_script.sh\n\n\nThe \n-n 512\n argument requests 512 nodes.\n\n\nTitan\n\n\nJob Script\n\n\n#!/bin/bash\n\n\n#PBS -A \nyourOLCFProjectName\n\n\n#PBS -N test\n\n\n#PBS -j oe\n\n\nexport\n \nn_nodes\n=\n$JOBSIZE\n\n\nexport\n \nn_mpi_ranks_per_node\n=\n8\n\n\nexport\n \nn_mpi_ranks\n=\n$((\n$n_nodes\n \n*\n \n$n_mpi_ranks_per_node\n))\n\n\n\ncd\n \n$MEMBERWORK\n/\nyourOLCFProjectName\n\ndate\n\n\nexport\n \nOMP_NUM_THREADS\n=\n2\n\n\naprun -n \n$n_mpi_ranks\n -N \n$n_mpi_ranks_per_node\n \n\\\n\n  -d \n2\n  \nexecutable\n \nexecutable args\n\n\n\n\n\nJob Submit Command\n\n\nqsub -l nodes=512 ./theta_script.sh\n\n\nThe \n-l nodes=512\n argument requests 512 nodes (this can also be put in the batch script).\n\n\nCori\n\n\nNERSC provides a \npage in the MyNERSC\nwebsite\n which generates job scripts\nautomatically based on specified runtime configurations. An example script is\nshown below, in which a code uses 512 nodes of Xeon Phi with MCDRAM configured\nin \"flat\" mode, with 4 MPI processes per node and 34 OpenMP threads per MPI\nprocess, using 2 hyper-threads per physical core of Xeon Phi:\n\n\nJob Script\n\n\n#!/bin/bash\n\n\n#SBATCH -N 512\n\n\n#SBATCH -C knl,quad,flat\n\n\n#SBATCH -p debug\n\n\n#SBATCH -J myapp_run1\n\n\n#SBATCH --mail-user=johndoe@nersc.gov\n\n\n#SBATCH --mail-type=ALL\n\n\n#SBATCH -t 00:30:00\n\n\n\n#OpenMP settings:\n\n\nexport\n \nOMP_NUM_THREADS\n=\n34\n\n\nexport\n \nOMP_PLACES\n=\nthreads\n\nexport\n \nOMP_PROC_BIND\n=\nspread\n\n\n\n#run the application:\n\nsrun -n \n2048\n -c \n68\n --cpu_bind\n=\ncores numactl -p \n1\n myapp.x", 
            "title": "Comparison"
        }, 
        {
            "location": "/facilities/comparison/#comparison-of-systems", 
            "text": "Below we compare in depth the Cori, Theta and Titan systems, software environment and job submission process to aid office of science users in utilizing \nmultiple resources.", 
            "title": "Comparison of Systems"
        }, 
        {
            "location": "/facilities/comparison/#hardware-in-depth", 
            "text": "System-  Cori  Theta  Titan      Facility  NERSC  ALCF  OLCF    Model  Cray XC40  Cray XC40  Cray XK7    Processor  Intel Xeon Phi 7250 (\"Knights Landing\")  Intel Xeon Phi 7230 (\"Knights Landing\")  AMD Opteron 6274 (\"Interlagos\")    Processor Cores  68  64  16 CPU cores (2668 (896) SP (DP) CUDA cores on K20X GPU)    Processor Base Frequency  1.4 GHz  1.3 GHz  2.2 GHz    Processor Max Frequency  1.6 GHz  1.5 GHz  3.1 GHz (disabled)    On-Device Memory  16 GB MCDRAM  16 GB MCDRAM  (6 GB GDDR5 on K20X GPU)    Processor DRAM  96 GB DDR4  192 GB DDR4  32 GB DDR3    Accelerator  (none)  (none)  NVIDIA Tesla K20X (\"Kepler\")    Nodes  9 688  3 624  18 688    Perf. Per Node  2.6 TF  2.6 TF  1.4 TF    Node local storage  (none)  128 GB SSD  (none)    External Burst Buffer  1.8 PB  (none)  (none)    Parallel File System  30 PB Lustre  10 PB Lustre  28 PB Lustre    Interconnect  Cray Aries  Cray Aries  Cray Gemini    Topology  Dragonfly  Dragonfly  3D torus    Peak Perf  30 PF  10 PF  27 PF", 
            "title": "Hardware In-Depth"
        }, 
        {
            "location": "/facilities/comparison/#software-environment", 
            "text": "System-  Cori  Theta  Titan      Software environment management  modules  modules  modules    Batch Job Scheduler  Slurm  Cobalt  PBS    Compilers       Intel  ( default )  module load PrgEnv-intel  ( default )  module load PrgEnv-intel  module load PrgEnv-intel    Cray  module load PrgEnv-cray  module load PrgEnv-cray  module load PrgEnv-cray    GNU  module load PrgEnv-gnu  module load PrgEnv-gnu  module load PrgEnv-gnu    PGI  n/a  n/a  ( default )  module load PrgEnv-pgi    CLANG  n/a  module load PrgEnv-llvm  n/a    Interpreters       R  gcc + MKL:  module load R    Cray:  module load cray-R  module load cray-R  module load r    Python 2  Anaconda + Intel MKL:  module load python/2.7-anaconda  Cray:  module load cray-python  Intel:  module load intelpython26  module load python_anaconda    Python 3  Anaconda + Intel MKL:  module load python/3.5-anaconda  Intel:  module load intelpython35  module load python_anaconda3    Libraries       FFT  FFTW:  module load fftw    Cray FFTW:  module load cray-fftw    Intel MKL:  automatic with Intel compilers  FFTW:  module load fftw    Cray FFTW:  module load cray-fftw    Intel MKL:  automatic with Intel compilers  FFTW:  module load fftw    Cray FFTW:  module load cray-fftw    Cray LibSci  ( default )  module load cray-libsci  module load cray-libsci  module load cray-libsci    Intel MKL  automatic with Intel compilers  automatic with Intel compilers  automatic with Intel compilers    Trilinos  module load cray-trilinos  module load cray-trilinos  module load cray-trilinos    PETSc  module load cray-petsc  module load cray-petsc  module load cray-petsc    SHMEM  module load cray-shmem  module load cray-shmem  module load cray-shmem    memkind  module load cray-memkind  module load cray-memkind  n/a    I/O Libraries       HDF5  module load cray-hdf5  module load cray-hdf5  module load cray-hdf5    NetCDF  module load cray-netcdf  module load cray-netcdf  module load cray-netcdf    Parallel NetCDF  module load cray-parallel-netcdf  module load cray-parallel-netcdf  module load cray-parallel-netcdf    Performance Tools and APIs       Intel VTune Amplifier  module load vtune  source /opt/intel/vtune_amplifier_xe/amplxe-vars.sh  n/a    CrayPAT  module load perftools-base   module load perftools  module load perftools  module load perftools    PAPI  module load papi  module load papi  module load papi    Darshan  ( default )  module load darshan  module load cray-memkind  module load darshan    Other Packages and Frameworks       Shifter  ( part of base system )  module load shifter  n/a", 
            "title": "Software Environment"
        }, 
        {
            "location": "/facilities/comparison/#compiler-wrappers", 
            "text": "Use these wrappers to properly cross-compile your source code for the compute\nnodes of the systems, and bring in appropriate headers for MPI, etc.     System-  Cori  Theta  Titan      C++  CC  CC  CC    C  cc  cc  cc    Fortran  ftn  ftn  ftn", 
            "title": "Compiler Wrappers"
        }, 
        {
            "location": "/facilities/comparison/#job-submission", 
            "text": "", 
            "title": "Job Submission"
        }, 
        {
            "location": "/facilities/comparison/#theta", 
            "text": "", 
            "title": "Theta"
        }, 
        {
            "location": "/facilities/comparison/#job-script", 
            "text": "#!/bin/bash  #COBALT -t 30  #COBALT --attrs mcdram=cache:numa=quad  #COBALT -A  yourALCFProjectName  echo   Starting Cobalt job script  export   n_nodes = $COBALT_JOBSIZE  export   n_mpi_ranks_per_node = 32  export   n_mpi_ranks = $(( $n_nodes   *   $n_mpi_ranks_per_node ))  export   n_openmp_threads_per_rank = 4  export   n_hyperthreads_per_core = 2  export   n_hyperthreads_skipped_between_ranks = 4 \naprun -n  $n_mpi_ranks  -N  $n_mpi_ranks_per_node   \\ \n  --env  OMP_NUM_THREADS = $n_openmp_threads_per_rank  -cc depth  \\ \n  -d  $n_hyperthreads_skipped_between_ranks   \\ \n  -j  $n_hyperthreads_per_core   \\ \n   executable   executable args   The  #COBALT -t 30  line indicates 30 minutes runtime. Generally,  #COBALT \nlines are equivalent to specifying  qsub  command-line arguments.", 
            "title": "Job Script"
        }, 
        {
            "location": "/facilities/comparison/#job-submit-command", 
            "text": "qsub -n 512 ./theta_script.sh \nThe  -n 512  argument requests 512 nodes.", 
            "title": "Job Submit Command"
        }, 
        {
            "location": "/facilities/comparison/#titan", 
            "text": "", 
            "title": "Titan"
        }, 
        {
            "location": "/facilities/comparison/#job-script_1", 
            "text": "#!/bin/bash  #PBS -A  yourOLCFProjectName  #PBS -N test  #PBS -j oe  export   n_nodes = $JOBSIZE  export   n_mpi_ranks_per_node = 8  export   n_mpi_ranks = $(( $n_nodes   *   $n_mpi_ranks_per_node ))  cd   $MEMBERWORK / yourOLCFProjectName \ndate export   OMP_NUM_THREADS = 2 \n\naprun -n  $n_mpi_ranks  -N  $n_mpi_ranks_per_node   \\ \n  -d  2    executable   executable args", 
            "title": "Job Script"
        }, 
        {
            "location": "/facilities/comparison/#job-submit-command_1", 
            "text": "qsub -l nodes=512 ./theta_script.sh \nThe  -l nodes=512  argument requests 512 nodes (this can also be put in the batch script).", 
            "title": "Job Submit Command"
        }, 
        {
            "location": "/facilities/comparison/#cori", 
            "text": "NERSC provides a  page in the MyNERSC\nwebsite  which generates job scripts\nautomatically based on specified runtime configurations. An example script is\nshown below, in which a code uses 512 nodes of Xeon Phi with MCDRAM configured\nin \"flat\" mode, with 4 MPI processes per node and 34 OpenMP threads per MPI\nprocess, using 2 hyper-threads per physical core of Xeon Phi:", 
            "title": "Cori"
        }, 
        {
            "location": "/facilities/comparison/#job-script_2", 
            "text": "#!/bin/bash  #SBATCH -N 512  #SBATCH -C knl,quad,flat  #SBATCH -p debug  #SBATCH -J myapp_run1  #SBATCH --mail-user=johndoe@nersc.gov  #SBATCH --mail-type=ALL  #SBATCH -t 00:30:00  #OpenMP settings:  export   OMP_NUM_THREADS = 34  export   OMP_PLACES = threads export   OMP_PROC_BIND = spread #run the application: \nsrun -n  2048  -c  68  --cpu_bind = cores numactl -p  1  myapp.x", 
            "title": "Job Script"
        }, 
        {
            "location": "/perfport/overview/", 
            "text": "Overview\n\n\nOverview\n\n\nAs shown on the detailed \nfacility comparison page\n, the Cori, Theta and Titan systems have a lot \nin common including interconnect (Cray Gemini or Cray Aries) and software environment. The most striking difference between the systems from a portability \npoint \nof view is the node-level architecture. \nCori and Theta both containing Intel Knights Landing (KNL) powered nodes while Titan sports a heterogeneous architecture with an AMD 16-Core CPU \ncoupled with an NVIDIA K20X GPU (where the majority of the compute capacity lies) on each node. We compare just the important node memory hierarchy and \nparallelism features in the following table:\n\n\n\n\n\n\n\n\nNode\n\n\nDDR Memory\n\n\nDevice Memory\n\n\nCores/SMs (DP)\n\n\nVector-Width/Warp-Size (DP)\n\n\n\n\n\n\n\n\n\n\nCori (KNL)\n\n\n96 GB\n\n\n16 GB\n\n\n68\n\n\n8\n\n\n\n\n\n\nTheta (KNL)\n\n\n192 GB\n\n\n16 GB\n\n\n64\n\n\n8\n\n\n\n\n\n\nTitan (K20X)\n\n\n32GB (Opteron)\n\n\n6 GB\n\n\nCPU - 8 Bulldozer modules; GPU -  14 SMs\n\n\n32\n\n\n\n\n\n\n\n\nwhere DP stands for Double Precision and SM stands for Streaming Multiprocessor. Before we dive in to performance portability challenges, lets first look deeper at the \nKNL and Kepler and architectures to discuss general progamming concepts and optimization strategy for each. \n\n\nGeneral Optimization Concepts on KNL\n\n\nA KNL processor (Intel Xeon-Phi processor of the Knight's Landing generation) has between 64 and 72 indepenent processing cores grouped in pairs (called tiles) on a 2D mesh\nacross the processor die as pictured below. Each of these cores has an independent 64 KB L1 cache and shares a 1MB L2 cache with its neighboring core on the same tile. Each core \nsupports up to 4 hyperthreads, that allow the core to context switch in order to continue computing if a thread is stalled. Each core contains two vector-processing-units (VPU) that \ncan execute AVX512 vector instructions from 512 bit registers (8 double precision SIMD (single instruction multiple data) lanes). Each VPU is capable of \nexecuting fused-multiply-add (FMA) \ninstructions each cycle; so \nthat the \ntotal possible FLOPs/cycle on a core is 32. \n\n\n\n\nFor applications with high arithmetic intensity (FLOPs computed per bytes transferred from memory), a performance increase factor of 64-72 can be gained by \ncode \nthat has optimal \nscaling across the cores of the processor and an additional, factor of 32\nin performance can therefore be gained by vectorizable code (using both VPUs) that has multiply-add capability. Generating vectorizable/SIMD code can be \nchallenging. In many applications at NERSC and ALCF, we rely on the compiler to generate vector code. Though, there are ways to aid the compiler with \ndirectives (e.g. \nOMP SIMD\n) and to write vector code explicitly via AVX512 intrinsics or assembly.\n\n\nFor applications with lower arithmetic intensities, the KNL has a number of features that can be exploited. Firstly, the L1 and L2 caches are available and \nit is good programming practice to block or tile ones algorithm in order to increase data reuse out of these cache levels. In addition, the KNL has 16GB of \nhigh-bandwidth memory (MCDRAM) located right on the package. The MCDRAM has an available bandwidth of around 450GB, 4-5x that of the traditional DDR on the \nnode (96GB available on Cori nodes and 192GB available on Theta nodes). The MCDRAM can be configured as allocatable memory (that users can explicitly \nallocate data to) or as a transparent last level cache. \n\n\nWhen, utilizing the MCDRAM as a cache, an application developer may want to add an additional level of blocking/tiling to their algorithm, but should \nconsider that the MCDRAM as a cache has a number limitations - most importantly there is no hardware prefetching or associativity (mean each address in DDR \nhas exactly one address in the MCDRAM cache it can reside and collisions are more likely). \n\n\nWhen instead managing the MCDRAM has an allocatable memory domain, applications can either choose to use it for all their allocations by default, or can \nchoose to allocate specific arrays in the MCDRAM. The latter generally requires non-portable compiler directives (e.g. \n!$DIR FASTMEM\n near the allocate \nstatements) or special malloc calls (e.g. \nhbwmalloc\n). \n\n\nFor many codes, getting optimal performance on KNL requires all of the above: good thread/rank scaling on the many-cores, efficient vectorizable code and effective use of the \nlow-level caches as well as the MCDRAM.\n\n\nGeneral Optimization Concepts on GPUs\n\n\nChallenges For Portability\n\n\nAt least three major challenges need to be overcome by viable performance portability approaches for KNL and GPU systems:\n\n\n\n\n\n\nHow to express parallelism in a portable way across both the KNL processor cores and vector-lanes and across the 896 SIMT threads that the K20x \nCUDA cores support. \n\n\n\n\n\n\nHow to maintain a data layout and access pattern for both a KNL system with multiple levels of caches (L1, shared L2) to block against and a GPU system with\nsignificantly smaller caches shared by more threads where aplication\ncache blocking is generally discouraged. \n\n\n\n\n\n\nHow to express data movement and data locality across the memory hierarchy containing both host and device memory in portable way. \n\n\n\n\n\n\nIn the following pages we discuss how to measure a successful performance portable implementation, what are the available approaches and some case-studies \nfrom \nthe Office of Science workload. First however, we turn our attention to defining what performance-portability means.", 
            "title": "Overview"
        }, 
        {
            "location": "/perfport/overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/perfport/overview/#overview_1", 
            "text": "As shown on the detailed  facility comparison page , the Cori, Theta and Titan systems have a lot \nin common including interconnect (Cray Gemini or Cray Aries) and software environment. The most striking difference between the systems from a portability \npoint \nof view is the node-level architecture. \nCori and Theta both containing Intel Knights Landing (KNL) powered nodes while Titan sports a heterogeneous architecture with an AMD 16-Core CPU \ncoupled with an NVIDIA K20X GPU (where the majority of the compute capacity lies) on each node. We compare just the important node memory hierarchy and \nparallelism features in the following table:     Node  DDR Memory  Device Memory  Cores/SMs (DP)  Vector-Width/Warp-Size (DP)      Cori (KNL)  96 GB  16 GB  68  8    Theta (KNL)  192 GB  16 GB  64  8    Titan (K20X)  32GB (Opteron)  6 GB  CPU - 8 Bulldozer modules; GPU -  14 SMs  32     where DP stands for Double Precision and SM stands for Streaming Multiprocessor. Before we dive in to performance portability challenges, lets first look deeper at the \nKNL and Kepler and architectures to discuss general progamming concepts and optimization strategy for each.", 
            "title": "Overview"
        }, 
        {
            "location": "/perfport/overview/#general-optimization-concepts-on-knl", 
            "text": "A KNL processor (Intel Xeon-Phi processor of the Knight's Landing generation) has between 64 and 72 indepenent processing cores grouped in pairs (called tiles) on a 2D mesh\nacross the processor die as pictured below. Each of these cores has an independent 64 KB L1 cache and shares a 1MB L2 cache with its neighboring core on the same tile. Each core \nsupports up to 4 hyperthreads, that allow the core to context switch in order to continue computing if a thread is stalled. Each core contains two vector-processing-units (VPU) that \ncan execute AVX512 vector instructions from 512 bit registers (8 double precision SIMD (single instruction multiple data) lanes). Each VPU is capable of \nexecuting fused-multiply-add (FMA) \ninstructions each cycle; so \nthat the \ntotal possible FLOPs/cycle on a core is 32.    For applications with high arithmetic intensity (FLOPs computed per bytes transferred from memory), a performance increase factor of 64-72 can be gained by \ncode \nthat has optimal \nscaling across the cores of the processor and an additional, factor of 32\nin performance can therefore be gained by vectorizable code (using both VPUs) that has multiply-add capability. Generating vectorizable/SIMD code can be \nchallenging. In many applications at NERSC and ALCF, we rely on the compiler to generate vector code. Though, there are ways to aid the compiler with \ndirectives (e.g.  OMP SIMD ) and to write vector code explicitly via AVX512 intrinsics or assembly.  For applications with lower arithmetic intensities, the KNL has a number of features that can be exploited. Firstly, the L1 and L2 caches are available and \nit is good programming practice to block or tile ones algorithm in order to increase data reuse out of these cache levels. In addition, the KNL has 16GB of \nhigh-bandwidth memory (MCDRAM) located right on the package. The MCDRAM has an available bandwidth of around 450GB, 4-5x that of the traditional DDR on the \nnode (96GB available on Cori nodes and 192GB available on Theta nodes). The MCDRAM can be configured as allocatable memory (that users can explicitly \nallocate data to) or as a transparent last level cache.   When, utilizing the MCDRAM as a cache, an application developer may want to add an additional level of blocking/tiling to their algorithm, but should \nconsider that the MCDRAM as a cache has a number limitations - most importantly there is no hardware prefetching or associativity (mean each address in DDR \nhas exactly one address in the MCDRAM cache it can reside and collisions are more likely).   When instead managing the MCDRAM has an allocatable memory domain, applications can either choose to use it for all their allocations by default, or can \nchoose to allocate specific arrays in the MCDRAM. The latter generally requires non-portable compiler directives (e.g.  !$DIR FASTMEM  near the allocate \nstatements) or special malloc calls (e.g.  hbwmalloc ).   For many codes, getting optimal performance on KNL requires all of the above: good thread/rank scaling on the many-cores, efficient vectorizable code and effective use of the \nlow-level caches as well as the MCDRAM.", 
            "title": "General Optimization Concepts on KNL"
        }, 
        {
            "location": "/perfport/overview/#general-optimization-concepts-on-gpus", 
            "text": "", 
            "title": "General Optimization Concepts on GPUs"
        }, 
        {
            "location": "/perfport/overview/#challenges-for-portability", 
            "text": "At least three major challenges need to be overcome by viable performance portability approaches for KNL and GPU systems:    How to express parallelism in a portable way across both the KNL processor cores and vector-lanes and across the 896 SIMT threads that the K20x \nCUDA cores support.     How to maintain a data layout and access pattern for both a KNL system with multiple levels of caches (L1, shared L2) to block against and a GPU system with\nsignificantly smaller caches shared by more threads where aplication\ncache blocking is generally discouraged.     How to express data movement and data locality across the memory hierarchy containing both host and device memory in portable way.     In the following pages we discuss how to measure a successful performance portable implementation, what are the available approaches and some case-studies \nfrom \nthe Office of Science workload. First however, we turn our attention to defining what performance-portability means.", 
            "title": "Challenges For Portability"
        }, 
        {
            "location": "/perfport/definition/", 
            "text": "Definition\n\n\nDefinition\n\n\nThe 2016 DOE Center of Excellence (COE) meeting in Phoenix brought together engineers from the DOE's Office of Science and National Nuclear Security Agency \nas well as vendor staff (from Intel, NVIDIA, IBM, Cray and others) to share portability lessons and best practices from their respective app-readiness \nprograms. One of the high-level take-away messages from the meeting is that \"there is not yet a universally accepted definition of 'performance \nportability'\". \nThere is generally agreement on what performance-portability \"basically means\" but the exact details differ in everyone's idea for the term. A number of \nattendees \ngave the following definitions:\n\n\n\n\n\n\n\"For the purposes of this meeting, it is the ability to run an application with acceptable performance across KNL and GPU-based systems with a single \nversion of source code.\" (Rob Neely)\n\n\n\n\n\n\n\"An application is performance portable if it achieves a consistent level of performance (e.g. defined by execution time or\nother figure of merit (not percentage of peak flops across platforms)) relative to the best known implementation on each platform.\" (John Pennycook, Intel)\n\n\n\n\n\n\n\"Hard portability = no code changes and no tuning. Software portability = simple code mods with no algorithmic changes. Non-portable = algorithmic changes\" (Adrian Pope, Vitali Morozov)\n\n\n\n\n\n\n(Performance portability means) the same source code will run productively on a variety of different architectures\" (Larkin)\n\n\n\n\n\n\n\"Code is performance portable when the application team says its performance portable!\" (Richards)\n\n\n\n\n\n\nFor our purposes, we combine a few the ideas above into the following working definition:\n\n\n\n\nAn application is performance portable if it achieves a consistent ratio of the actual time to solution to either the best-known or the theoretical best time to \nsolution on each platform with minimal platform specific code required.\n\n\n\n\nWe discuss the details on how to begin to quantify the level to which a code meets this definition on the \n\nMeasurement Techniques\n page.", 
            "title": "Definition"
        }, 
        {
            "location": "/perfport/definition/#definition", 
            "text": "", 
            "title": "Definition"
        }, 
        {
            "location": "/perfport/definition/#definition_1", 
            "text": "The 2016 DOE Center of Excellence (COE) meeting in Phoenix brought together engineers from the DOE's Office of Science and National Nuclear Security Agency \nas well as vendor staff (from Intel, NVIDIA, IBM, Cray and others) to share portability lessons and best practices from their respective app-readiness \nprograms. One of the high-level take-away messages from the meeting is that \"there is not yet a universally accepted definition of 'performance \nportability'\". \nThere is generally agreement on what performance-portability \"basically means\" but the exact details differ in everyone's idea for the term. A number of \nattendees \ngave the following definitions:    \"For the purposes of this meeting, it is the ability to run an application with acceptable performance across KNL and GPU-based systems with a single \nversion of source code.\" (Rob Neely)    \"An application is performance portable if it achieves a consistent level of performance (e.g. defined by execution time or\nother figure of merit (not percentage of peak flops across platforms)) relative to the best known implementation on each platform.\" (John Pennycook, Intel)    \"Hard portability = no code changes and no tuning. Software portability = simple code mods with no algorithmic changes. Non-portable = algorithmic changes\" (Adrian Pope, Vitali Morozov)    (Performance portability means) the same source code will run productively on a variety of different architectures\" (Larkin)    \"Code is performance portable when the application team says its performance portable!\" (Richards)    For our purposes, we combine a few the ideas above into the following working definition:   An application is performance portable if it achieves a consistent ratio of the actual time to solution to either the best-known or the theoretical best time to \nsolution on each platform with minimal platform specific code required.   We discuss the details on how to begin to quantify the level to which a code meets this definition on the  Measurement Techniques  page.", 
            "title": "Definition"
        }, 
        {
            "location": "/perfport/measurements/", 
            "text": "Measuring Performance Portability\n\n\nAs discussed in the previous section, performance portability can be an elusive topic to quantify \nand different engineers often provide different definitions or measurement techniques. Below we characterize a few well-defined approaches.\n\n\nMeasuring Portability\n\n\nMeasuring 'portability' itself is somewhat more well defined. One can, in principle, measure the \ntotal lines of code used in common across different architectures vs. the amount of code intended \nfor a single architecture via \nIFDEF\n pre-processing statements, separate routines and the like. A code with 0% \narchitecture specific code being completely portable and a code with a 100% architecture specific \ncode being essentially made up of multiple applications for each architecture. \n\n\nOne subtlety that this approach hides is that it is possible that shared source-code requires more lines than source-code intended for a single architecture \nor, in some cases, even two sets of separate source-code intended for multiple architectures. We ignore this case for now, assuming that using a portable \napproach to express an algorithm doesn't significant change the amount of code required. \n\n\nMeasuring Performance\n\n\n'Performance', even on a single architecture, is a bit less simple to define and measure. In \npractice, scientists generally care about the quality and quantity of scientific output they \nproduce. This typically maps for them to relative performance concepts, such as how much faster \ncan a particular run or set of runs finish today than yesterday or on this machine than that. The \ndrawback of trying to measure performance in this way is that the baseline is arbitrary - i.e. you \ndon't know how well your code is performing on any architecture compared to how it 'should' be \nperforming if it were well optimized.\n\n\nOne may attempt to define absolute performance as a measure of the actual floating point operations (or integer operations) per second \n(FLOPS) of an \napplication during execution compared to the theoretical peak performance of the system or fraction of the system in use, as reported on the Top 500 \nlist - \nTop500.org\n - or as reported in the system specs.\n\n\nHowever, this is a poor measure of application performance (and a particularly poor measure to use when trying to quantify performance portability) for a \ncouple reasons:\n\n\n\n\n\n\nThe application or algorithm may be fundamentally limited by an aspect of the HPC system other than the compute capability (which is limited to the number \nof cores/threads, clock-speed and vector/instruction-sets)\n\n\n\n\n\n\nThe application or algorithm may be fundamentally limited by \ndifferent\n aspects of the system on different HPC system. \n\n\n\n\n\n\nAs an example, an implementation of an algorithm that is limited by memory bandwidth may be achieving the best performance it theoretically can on systems \nwith different architectures but could be achieving widely varying percentage of peaks FLOPS on the different systems. \n\n\nInstead we advocate for one of two approaches for defining performance against expected or optimal performance on the system for an algorithm:\n\n\n1. Compare against a known, well-recognized (potentially non-portable), implementation.\n\n\nSome applications, algorithms or methods have well-recognized optimal (often hand-tuned) implementations on different architectures. These can be used as a \nbaseline for defining relative performance of portable versions. Our Chroma application case-study shows this approach. \nSee \nhere\n \n\n\nMany performance tools exist at ALCF, NERSC and OLCF for the purposes of profiling applications or regions of applications and determining performance \nlimiters \nwhen comparing different implementation of an algorithm or method. See the comprehensive list \nhere\n with links to detailed \ninstructions and example use-cases at each site. \n\n\n2. Use the roofline approach to compare actual to expected performance\n\n\nAs discussed above, the major limitation of defining performance relative to the peak FLOPS capability of the machine is that applications in practice are \nlimited by many different aspects of an HPC system. \n\n\nThe roofline performance model and extensions to the roofline model attempt to take these different possibilities into account. In the \nroofline approach, one defines various \ntheoretical performance ceilings for an algorithm or implementation based on its properties. In the simplest model, one may classify an algorithm based on \nits DRAM arithmetic-intensity - the ratio of the FLOPs performed vs the data moved from main-memory (DRAM) to the processor over the course of \nexecution, which can be measured for a given application as described on the subpages. Below, we show the performance ceilings provided by the roofline \nmodel on KNL for applications as a function of their DRAM arithmetic-intensity:\n\n\n\n\nHere the blue line represents the maximum performance that can be achieved on a KNL node for an application running out of the KNL High-Bandwidth Memory \n(HBM) with a given\nDRAM-AI (the x-axis value). For low\nvalues of DRAM-AI, the performance is limited by the diagonal ceiling, meaning that memory-bandwidth is the limiting hardware feature. The location of the \ndiagonal lines are typically computed empirically from the available bandwidth reported by, for example, stream triad like kernels.\n\n\nFor high values of DRAM-AI,\nmemory bandwidth no longer limits performance and one can, in principle, achieve the max compute performance on the system. However, for such cases we \nmay draw \nother ceilings that represent common limitations in algorithms or implementations of algorithms. The dashed-dotted green line labeled \"-ILP\" is the\nperformance ceiling for applications that: 1. do not provide a balance of multiply and add instructions, or simply don't use the Fused Multiply Add (FMA)\ninstructions on the processor and 2. don't have enough instruction level parallelism to keep both VPUs on the KNL busy. The dashed purple line labeled \n\"-Vectorization\" is performance ceiling of an algorithm or implementation that, in addition to the above two deficiencies, lacks vectorization (a \ncombined factor of 32 reduction in the ceiling).\n\n\nFor applications that are limited by other system properties, it is possible to extend the roofline model to include related ceilings. For example, we \ncommonly extend the roofline approach to use arithmetic-intensities based on data movement from different levels of cache (e.g. L1, L2 on the KNL), in order to \ndiscover the relevant limiting cache level. The figure below shows an example of such an extended roofline plot for an application limited by the L2 cache \nlevel.\n\n\n\n\nIn addition, for applications with non-stream like memory access patterns, lower memory-ceilings may be computed. Many codes use strided or indirect-addressed \n(scatter/gather) patterns where memory latency may be limiting resource. In such cases, it is possible to quickly craft a few line benchmark to compute the \nrelevant ceiling. We should some example ceiling parameters for different access patterns determined empirically in the following tables:\n\n\nFor polynomial access pattern: x[i] = (x[i]+c0):\n\n\n\n\n\n\n\n\nSystem\n\n\nDRAM\n\n\nL2\n\n\nL1\n\n\nGEMM\n\n\n\n\n\n\n\n\n\n\nTitan (Kepler)\n\n\n161 GB/s\n\n\n559 GB/s\n\n\n-\n\n\n1226 GF/s\n\n\n\n\n\n\nSummit Dev (4 Pascal)\n\n\n1930 GB/s\n\n\n6507 GB/s\n\n\n-\n\n\n17095 GF/s\n\n\n\n\n\n\nCori (KNL)\n\n\n413 GB/s\n\n\n1965 GB/s\n\n\n6443 GB/s\n\n\n2450 GF/s\n\n\n\n\n\n\n\n\nFor non-contiguous accesses on KNL as an example:\n\n\n\n\n\n\n\n\nAccess Pattern\n\n\nKNL Effective Bandwidth (Cache Mode)\n\n\n\n\n\n\n\n\n\n\nDot Product\n\n\n219 GB/s\n\n\n\n\n\n\nStride 2 Dot Product\n\n\n96 GB/s\n\n\n\n\n\n\nStride 100 Dot Product\n\n\n31 GB/s\n\n\n\n\n\n\nStride 10000 Dot Product\n\n\n20 GB/s\n\n\n\n\n\n\n\n\nFinally, one may additional define an AI value and roofline-ceiling for algorithms with data coming from off-node due to internode \ncommunication. The relevant bandwidth here is the injection bandwidth of the node:\n\n\n\n\n\n\n\n\nSystem:\n\n\nCori/Theta\n\n\nTitan\n\n\nSummit\n\n\n\n\n\n\n\n\n\n\nInjection Bandwidth:\n\n\n8 GB/s\n\n\n6.4 GB/s\n\n\n23 GB/s\n\n\n\n\n\n\n\n\nThe value of the roofline approach is that the relative performance of an application kernel to relevant ceilings (those related to fundamental limitations \nin an algorithm \nthat cannot be overcome via optimization) allow us to define an absolute performance ratio for each architecture in order to quantity absolute performance \nand performance portability that accurately reflects the different limiting hardware characteristics of each system. On the following pages, we discuss how \nto collect roofline data on both KNL and GPU powered nodes.", 
            "title": "Measurement Techniques"
        }, 
        {
            "location": "/perfport/measurements/#measuring-performance-portability", 
            "text": "As discussed in the previous section, performance portability can be an elusive topic to quantify \nand different engineers often provide different definitions or measurement techniques. Below we characterize a few well-defined approaches.", 
            "title": "Measuring Performance Portability"
        }, 
        {
            "location": "/perfport/measurements/#measuring-portability", 
            "text": "Measuring 'portability' itself is somewhat more well defined. One can, in principle, measure the \ntotal lines of code used in common across different architectures vs. the amount of code intended \nfor a single architecture via  IFDEF  pre-processing statements, separate routines and the like. A code with 0% \narchitecture specific code being completely portable and a code with a 100% architecture specific \ncode being essentially made up of multiple applications for each architecture.   One subtlety that this approach hides is that it is possible that shared source-code requires more lines than source-code intended for a single architecture \nor, in some cases, even two sets of separate source-code intended for multiple architectures. We ignore this case for now, assuming that using a portable \napproach to express an algorithm doesn't significant change the amount of code required.", 
            "title": "Measuring Portability"
        }, 
        {
            "location": "/perfport/measurements/#measuring-performance", 
            "text": "'Performance', even on a single architecture, is a bit less simple to define and measure. In \npractice, scientists generally care about the quality and quantity of scientific output they \nproduce. This typically maps for them to relative performance concepts, such as how much faster \ncan a particular run or set of runs finish today than yesterday or on this machine than that. The \ndrawback of trying to measure performance in this way is that the baseline is arbitrary - i.e. you \ndon't know how well your code is performing on any architecture compared to how it 'should' be \nperforming if it were well optimized.  One may attempt to define absolute performance as a measure of the actual floating point operations (or integer operations) per second \n(FLOPS) of an \napplication during execution compared to the theoretical peak performance of the system or fraction of the system in use, as reported on the Top 500 \nlist -  Top500.org  - or as reported in the system specs.  However, this is a poor measure of application performance (and a particularly poor measure to use when trying to quantify performance portability) for a \ncouple reasons:    The application or algorithm may be fundamentally limited by an aspect of the HPC system other than the compute capability (which is limited to the number \nof cores/threads, clock-speed and vector/instruction-sets)    The application or algorithm may be fundamentally limited by  different  aspects of the system on different HPC system.     As an example, an implementation of an algorithm that is limited by memory bandwidth may be achieving the best performance it theoretically can on systems \nwith different architectures but could be achieving widely varying percentage of peaks FLOPS on the different systems.   Instead we advocate for one of two approaches for defining performance against expected or optimal performance on the system for an algorithm:", 
            "title": "Measuring Performance"
        }, 
        {
            "location": "/perfport/measurements/#1-compare-against-a-known-well-recognized-potentially-non-portable-implementation", 
            "text": "Some applications, algorithms or methods have well-recognized optimal (often hand-tuned) implementations on different architectures. These can be used as a \nbaseline for defining relative performance of portable versions. Our Chroma application case-study shows this approach.  See \nhere    Many performance tools exist at ALCF, NERSC and OLCF for the purposes of profiling applications or regions of applications and determining performance \nlimiters \nwhen comparing different implementation of an algorithm or method. See the comprehensive list  here  with links to detailed \ninstructions and example use-cases at each site.", 
            "title": "1. Compare against a known, well-recognized (potentially non-portable), implementation."
        }, 
        {
            "location": "/perfport/measurements/#2-use-the-roofline-approach-to-compare-actual-to-expected-performance", 
            "text": "As discussed above, the major limitation of defining performance relative to the peak FLOPS capability of the machine is that applications in practice are \nlimited by many different aspects of an HPC system.   The roofline performance model and extensions to the roofline model attempt to take these different possibilities into account. In the \nroofline approach, one defines various \ntheoretical performance ceilings for an algorithm or implementation based on its properties. In the simplest model, one may classify an algorithm based on \nits DRAM arithmetic-intensity - the ratio of the FLOPs performed vs the data moved from main-memory (DRAM) to the processor over the course of \nexecution, which can be measured for a given application as described on the subpages. Below, we show the performance ceilings provided by the roofline \nmodel on KNL for applications as a function of their DRAM arithmetic-intensity:   Here the blue line represents the maximum performance that can be achieved on a KNL node for an application running out of the KNL High-Bandwidth Memory \n(HBM) with a given\nDRAM-AI (the x-axis value). For low\nvalues of DRAM-AI, the performance is limited by the diagonal ceiling, meaning that memory-bandwidth is the limiting hardware feature. The location of the \ndiagonal lines are typically computed empirically from the available bandwidth reported by, for example, stream triad like kernels.  For high values of DRAM-AI,\nmemory bandwidth no longer limits performance and one can, in principle, achieve the max compute performance on the system. However, for such cases we \nmay draw \nother ceilings that represent common limitations in algorithms or implementations of algorithms. The dashed-dotted green line labeled \"-ILP\" is the\nperformance ceiling for applications that: 1. do not provide a balance of multiply and add instructions, or simply don't use the Fused Multiply Add (FMA)\ninstructions on the processor and 2. don't have enough instruction level parallelism to keep both VPUs on the KNL busy. The dashed purple line labeled \n\"-Vectorization\" is performance ceiling of an algorithm or implementation that, in addition to the above two deficiencies, lacks vectorization (a \ncombined factor of 32 reduction in the ceiling).  For applications that are limited by other system properties, it is possible to extend the roofline model to include related ceilings. For example, we \ncommonly extend the roofline approach to use arithmetic-intensities based on data movement from different levels of cache (e.g. L1, L2 on the KNL), in order to \ndiscover the relevant limiting cache level. The figure below shows an example of such an extended roofline plot for an application limited by the L2 cache \nlevel.   In addition, for applications with non-stream like memory access patterns, lower memory-ceilings may be computed. Many codes use strided or indirect-addressed \n(scatter/gather) patterns where memory latency may be limiting resource. In such cases, it is possible to quickly craft a few line benchmark to compute the \nrelevant ceiling. We should some example ceiling parameters for different access patterns determined empirically in the following tables:  For polynomial access pattern: x[i] = (x[i]+c0):     System  DRAM  L2  L1  GEMM      Titan (Kepler)  161 GB/s  559 GB/s  -  1226 GF/s    Summit Dev (4 Pascal)  1930 GB/s  6507 GB/s  -  17095 GF/s    Cori (KNL)  413 GB/s  1965 GB/s  6443 GB/s  2450 GF/s     For non-contiguous accesses on KNL as an example:     Access Pattern  KNL Effective Bandwidth (Cache Mode)      Dot Product  219 GB/s    Stride 2 Dot Product  96 GB/s    Stride 100 Dot Product  31 GB/s    Stride 10000 Dot Product  20 GB/s     Finally, one may additional define an AI value and roofline-ceiling for algorithms with data coming from off-node due to internode \ncommunication. The relevant bandwidth here is the injection bandwidth of the node:     System:  Cori/Theta  Titan  Summit      Injection Bandwidth:  8 GB/s  6.4 GB/s  23 GB/s     The value of the roofline approach is that the relative performance of an application kernel to relevant ceilings (those related to fundamental limitations \nin an algorithm \nthat cannot be overcome via optimization) allow us to define an absolute performance ratio for each architecture in order to quantity absolute performance \nand performance portability that accurately reflects the different limiting hardware characteristics of each system. On the following pages, we discuss how \nto collect roofline data on both KNL and GPU powered nodes.", 
            "title": "2. Use the roofline approach to compare actual to expected performance"
        }, 
        {
            "location": "/perfport/measurements/knl/", 
            "text": "Using SDE and VTUNE to calculate performance data\n\n\nNERSC has extensive documentation and examples for collecting Arithmetic Intensity (AI) and FLOPs using the Intel SDE and VTune \ntools. You can find instructions for this \n\nhere\n.\n\n\nUsing Vector Advisor to automate roofline data collection\n\n\nIn addition to VTune and SDE, Intel provides a useful tool exploring performance of an application against expectations called Intel Vector Advisor. The tool can automate the collection and presentation of roofline model data. The following guide describes how this is done:\n\n\nCompiling\n\n\n\n\n\n\nCompile code with the -g flag. \n\n\n\n\n\n\nOptimizations flags can (and should) be included. \n\n\n\n\n\n\nUsing the -dynamic flag for dynamic linking is recommended.\n\n\n\n\n\n\nRunning\n\n\n\n\n\n\nWe recommend running applications using the command line interface advixe-cl. \n\n\n\n\n\n\nTo access the binaries, load the advisor module using module load advisor.\n\n\n\n\n\n\nAlways run vector advisor out of the Lustre filesystems at NERSC and ALCF. Runs out of GPFS filesystems are known to fail.\n\n\n\n\n\n\nSince version 2017/update 2 the roofline is a standard feature and does not require setting any additional environment variables. \n\n\n\n\n\n\nTo collect roofline data, Advisor needs to do two collection runs, survey and tripcounts. Survey is a quick pass with no noticeable overhead that is used \nto count the application run time. Tripcounts has significant overhead.\n\n\n\n\n\n\nsrun \nsrun options\n advixe-cl -collect survey -project-dir \nsame data directory\n -- \nexecutable\n\n\n\n\nsrun \nsrun options\n advixe-cl -collect tripcounts -flops-and-masks -project-dir \nsame data directory\n -- \nexecutable\n\n\n\n\nThere are a number of additional flags that can be used to speed up the collection including \n\n-no-stack-stitching\n and\n\n-no-auto-finalize\n which can be used to skip the expensive data finalization step. This is recommended on KNL systems where the cores of the compute nodes are less \npowerful than the cores of the login nodes. When the results are opened in the GUI, they will be finalized on the node running the GUI.\n\n\nUsing the GUI\n\n\n\n\n\n\nLoad advisor using  module load advisor.\n\n\n\n\n\n\nOpen the GUI with advixe-gui\n\n\n\n\n\n\n\n\nThe GUI opens to the welcome screen. Click on open result and navigate to the data directory you gave to advixe-cl\n\n\n\n\nA new result opens to the summary page. This page gives useful overall metrics for the application and highlights the top five time-consuming loops.\n\n\n\n\nNavigate to the Survey \n Roofline tab using the tabs on the top of the screen. The survey lists the loops of the application and provides information on \nvectorization, flop rate, arithmetic intensity, etc.\n\n\n\n\nClick on the Roofline bar on the left to switch to the roofline view. This view shows the measured rooflines of the system and the loops of the application \nin the cache-aware roofline.\n\n\n*By default, the documentation above describes how to use Intel Advisor to gather L1-roofline data (that is where the AI is defined based on L1 data traffic), you can contact NERSC \nconsultants at \n for information on experimental DRAM roofline capabilities.", 
            "title": "Collecting Roofline on KNL"
        }, 
        {
            "location": "/perfport/measurements/knl/#using-sde-and-vtune-to-calculate-performance-data", 
            "text": "NERSC has extensive documentation and examples for collecting Arithmetic Intensity (AI) and FLOPs using the Intel SDE and VTune \ntools. You can find instructions for this  here .", 
            "title": "Using SDE and VTUNE to calculate performance data"
        }, 
        {
            "location": "/perfport/measurements/knl/#using-vector-advisor-to-automate-roofline-data-collection", 
            "text": "In addition to VTune and SDE, Intel provides a useful tool exploring performance of an application against expectations called Intel Vector Advisor. The tool can automate the collection and presentation of roofline model data. The following guide describes how this is done:", 
            "title": "Using Vector Advisor to automate roofline data collection"
        }, 
        {
            "location": "/perfport/measurements/knl/#compiling", 
            "text": "Compile code with the -g flag.     Optimizations flags can (and should) be included.     Using the -dynamic flag for dynamic linking is recommended.", 
            "title": "Compiling"
        }, 
        {
            "location": "/perfport/measurements/knl/#running", 
            "text": "We recommend running applications using the command line interface advixe-cl.     To access the binaries, load the advisor module using module load advisor.    Always run vector advisor out of the Lustre filesystems at NERSC and ALCF. Runs out of GPFS filesystems are known to fail.    Since version 2017/update 2 the roofline is a standard feature and does not require setting any additional environment variables.     To collect roofline data, Advisor needs to do two collection runs, survey and tripcounts. Survey is a quick pass with no noticeable overhead that is used \nto count the application run time. Tripcounts has significant overhead.    srun  srun options  advixe-cl -collect survey -project-dir  same data directory  --  executable   srun  srun options  advixe-cl -collect tripcounts -flops-and-masks -project-dir  same data directory  --  executable   There are a number of additional flags that can be used to speed up the collection including  -no-stack-stitching  and -no-auto-finalize  which can be used to skip the expensive data finalization step. This is recommended on KNL systems where the cores of the compute nodes are less \npowerful than the cores of the login nodes. When the results are opened in the GUI, they will be finalized on the node running the GUI.", 
            "title": "Running"
        }, 
        {
            "location": "/perfport/measurements/knl/#using-the-gui", 
            "text": "Load advisor using  module load advisor.    Open the GUI with advixe-gui     The GUI opens to the welcome screen. Click on open result and navigate to the data directory you gave to advixe-cl   A new result opens to the summary page. This page gives useful overall metrics for the application and highlights the top five time-consuming loops.   Navigate to the Survey   Roofline tab using the tabs on the top of the screen. The survey lists the loops of the application and provides information on \nvectorization, flop rate, arithmetic intensity, etc.   Click on the Roofline bar on the left to switch to the roofline view. This view shows the measured rooflines of the system and the loops of the application \nin the cache-aware roofline.  *By default, the documentation above describes how to use Intel Advisor to gather L1-roofline data (that is where the AI is defined based on L1 data traffic), you can contact NERSC \nconsultants at   for information on experimental DRAM roofline capabilities.", 
            "title": "Using the GUI"
        }, 
        {
            "location": "/perfport/measurements/gpu/", 
            "text": "Measuring Roofline Quantities on NVIDIA GPUs\n\n\nIt is possible to measure roofline quantities for a kernel on a GPU using the NVProf tool which was described \nhere\n. \n\n\nIn order to plot roofline data, we need to compute arithmetic intensity as well as FLOPS which involves three quantities:\n\n\n\n\nNumber of floating point operations\n\n\nData volume moved to and from DRAM or cache\n\n\nThe runtime in seconds\n\n\n\n\nThese can be collected with NVProf using the following steps:\n\n\n1. Use NVProf to collect the time spent\n\n\nYou can use NVProf to collect time spent in a kernel you are interested in by executing something like the following:\n\n\ncommand: nvprof --print-gpu-trace ./build/bin/hpgmg-fv \n6\n \n8\n\noutput: \nTime\n(\n%\n)\n      Time     Calls    Avg           Min                Max           Name\n \n51\n.96%  \n2\n.52256s   \n1764\n  \n1\n.4300ms  \n1\n.4099ms  \n1\n.4479ms      void smooth_kernel\nint\n=\n7\n, \nint\n=\n16\n, \nint\n=\n4\n, \nint\n=\n16\n(\nlevel_type, int, int, double, double, int, double*, double*\n)\n\n\n\n\n\n2. Use the NVProf metric summary mode\n\n\nYou can use this mode and specify the target kernel to collect information such as:\n\n\n\n\nFloating point ops\n\n\nDRAM R/W transactions\n\n\nDRAM R/W throughput\n\n\n\n\nAn example NVProf command to execute is:\n\n\nnvprof  --kernels \nsmooth_kernel\n --metrics flop_count_dp  --metrics dram_read_throughput  --metrics dram_write_throughput --metrics dram_read_transactions --metrics \ndram_write_transactions ./build/bin/hpgmg-fv \n6\n \n8\n \n\n\n\n\nThis will produce output like the following for each kernel:\n\n\n    Invocations          Metric Name                     Metric Description                            Min           Max           Avg\n\n    Kernel: void smooth_kernel\nint=7, int=32, int=4, int=16\n(level_type, int, int, double, double, int, double*, double*)\n       1764                  flop_count_dp               Floating Point Operations(Double Precision)   240648192    240648192      240648192\n       1764                  dram_read_throughput        Device Memory Read Throughput                 299.98GB/s   307.48GB/s     303.72GB/s\n       1764                  dram_write_throughput       Device Memory Write Throughput                40.102GB/s   41.099GB/s     40.578GB/s\n       1764                  dram_read_transactions      Device Memory Read Transactions               4537918      4599890        4567973\n       1764                  dram_write_transactions     Device Memory Write Transactions              606387       611691         610299\n\n\n\n\nYou may instead replace the DRAM metrics with L2 metrics to compute a cache-based roofline. For example, replace \ndram_write_throughput\n with \n\nl2_write_throughput\n. You can find other available metrics \nhere\n.\n\n\nTo compute Arithmetic Intensity you can then use the following equivalent methods:\n\n\nMethod I:  \n\n\nFP / ( DR + DW ) * (size of transaction = 32 Bytes)\n\n\nMethod II:\n\n\nFP / (TR + TW) * time taken by kernel (computed by step 1)\n\n\nwhere,\n\n\nFP = double precision ops \n\n\nDR/DW= dram read/write transactions\n\n\nTR/TW= dram read/write throughput", 
            "title": "Collecting Roofline on GPUs"
        }, 
        {
            "location": "/perfport/measurements/gpu/#1-use-nvprof-to-collect-the-time-spent", 
            "text": "You can use NVProf to collect time spent in a kernel you are interested in by executing something like the following:  command: nvprof --print-gpu-trace ./build/bin/hpgmg-fv  6   8 \noutput: \nTime ( % )       Time     Calls    Avg           Min                Max           Name\n  51 .96%   2 .52256s    1764    1 .4300ms   1 .4099ms   1 .4479ms      void smooth_kernel int = 7 ,  int = 16 ,  int = 4 ,  int = 16 ( level_type, int, int, double, double, int, double*, double* )", 
            "title": "1. Use NVProf to collect the time spent"
        }, 
        {
            "location": "/perfport/measurements/gpu/#2-use-the-nvprof-metric-summary-mode", 
            "text": "You can use this mode and specify the target kernel to collect information such as:   Floating point ops  DRAM R/W transactions  DRAM R/W throughput   An example NVProf command to execute is:  nvprof  --kernels  smooth_kernel  --metrics flop_count_dp  --metrics dram_read_throughput  --metrics dram_write_throughput --metrics dram_read_transactions --metrics \ndram_write_transactions ./build/bin/hpgmg-fv  6   8    This will produce output like the following for each kernel:      Invocations          Metric Name                     Metric Description                            Min           Max           Avg\n\n    Kernel: void smooth_kernel int=7, int=32, int=4, int=16 (level_type, int, int, double, double, int, double*, double*)\n       1764                  flop_count_dp               Floating Point Operations(Double Precision)   240648192    240648192      240648192\n       1764                  dram_read_throughput        Device Memory Read Throughput                 299.98GB/s   307.48GB/s     303.72GB/s\n       1764                  dram_write_throughput       Device Memory Write Throughput                40.102GB/s   41.099GB/s     40.578GB/s\n       1764                  dram_read_transactions      Device Memory Read Transactions               4537918      4599890        4567973\n       1764                  dram_write_transactions     Device Memory Write Transactions              606387       611691         610299  You may instead replace the DRAM metrics with L2 metrics to compute a cache-based roofline. For example, replace  dram_write_throughput  with  l2_write_throughput . You can find other available metrics  here .  To compute Arithmetic Intensity you can then use the following equivalent methods:  Method I:    FP / ( DR + DW ) * (size of transaction = 32 Bytes)  Method II:  FP / (TR + TW) * time taken by kernel (computed by step 1)  where,  FP = double precision ops   DR/DW= dram read/write transactions  TR/TW= dram read/write throughput", 
            "title": "2. Use the NVProf metric summary mode"
        }, 
        {
            "location": "/perfport/strategy/", 
            "text": "Strategy\n\n\nCode Structure and Practices\n\n\nBefore diving into a performance portability strategy, there are many things an application can do to make their code more \nportabile and ultimately more productive by applying recommended software engineering practices. These include:\n\n\n\n\nDeveloping in a well-defined version-controlled environment\n\n\nDocumenting code so other developers can quickly join and contribute to a project\n\n\nMaintaining a rigours test-suite and automated regression test framework on multiple platforms\n\n\nDeveloping code in a modular way that abstracts performance-critical regions\n\n\n\n\nDeveloping a Performance Portability Strategy\n\n\nWe noted in the introduction that the KNL and NVIDIA GPU architectures had a lot in common, including wide \"vectors\" or \"warps,\" as well as multiple tiers of memory, including on-package memory. However, before diving into to any particular performance-portability programming model, it is important to develop a high-level strategy for how a given problem best maps to these similar architecture features. In exploring various approaches to performance portability, we have found that different strategies can exist for exploiting these similarities. \n\n\nThreads and Vectors\n\n\nOne of the main challenges in providing a performance portability layer between KNL and GPU architectures is that vector parallelism on GPUs is expressed as \nSIMT (Single Instruction Multiple Threads) whereas a CPU contains both SMT (Simultaneous Multi-Threading) across cores/threads and SIMD (Single Instruction \nMultiple Data) across the lanes of the VPU (Vector Processing Unit). One of the challenges to be grappled with in using a performance portable approach \nis that SIMT parallelism lies somewhere in between SMT and SIMD parallelism in terms of expressibility, flexibility, and performance limitations:\n\n\n\n\n\n\nSMT\n: Each SMT thread can perform independent instructions on independent data in independent registers. The work for each thread may be expressed in a \nscalar way.\n\n\n\n\n\n\nSIMD\n: Each SIMD \"vector lane\" performs the same instruction on data from a single register. Vector lanes may be masked out to support branching, and \ngather/scatter instructions are supported to bring non-contiguous data in memory into a vector register. However, these often come with significant performance \ncosts.\n\n\n\n\n\n\nSIMT\n: SIMT shares some qualities of SMT in the sense that each thread (running on what is referred to as a \"core\" on a GPU) has its own registers, and \nwork for each thread can be expressed in a \"scalar\" way. However, like SIMD and unlike SMT, the threads are not completely \nindependent. On current GPU architectures, (e.g. the K20X found in Titan) typically 32 threads form a warp in which the same instruction is being executed each cycle. Thus, if the work between threads diverges \nsignificantly, there is a significant reduction in performance. In addition, optimal performance is typically achieved when the threads in the warp are \noperating on contiguous data. \n\n\n\n\n\n\nHow do you use these different types of parallelism in order to enable portable programming? We identify three particular strategies, each with advantages and disadvantages. It is important when beginning to develop a performance portability plan to consider which of these strategies best maps to your application.\n\n\nStrategy 1\n\n\nThe first strategy to performance portability is to equate SIMT threads on a GPU with SMT threads on CPU or a KNL. This has the advantage of allowing \nthe programmer to fully express the SIMT parallelism, but it does lead to a couple of challenges:\n\n\n\n\n\n\nSMT threads typically operate on independent regions of data (e.g. the programmer typically breaks an array into the biggest contiguous chunks possible \nand gives each thread a chunk to work on). On the other hand, with SIMT threads, consecutive threads in a warp are ideally given \nconsecutive elements of an array to work on. This leads to the idea of coalescing - where data given to a single thread is \nstrided by the warp size (typically 32). The concept of coalescing is an artifact of viewing SIMT threads like SMT threads \ninstead of viewing them like vector lanes. \n\n\n\n\n\n\nAnother level of parallelism on the KNL (the vector parallelism) is then left on the table to exploit some other way (e.g. through the compiler, possibly\nwith the aid of hints).\n\n\n\n\n\n\nThis strategy is likely most appropriate if additional vector parallelism can be easily exploited by the compiler on the KNL, if vectorization doesn't \naffect performance or if one intends to manually intervene to ensure vector parallelism is exploited on the KNL. Frameworks like Kokkos (with the concept of \n\"views\") can help handle the coalescing issue in a portable way. \n\n\nStrategy 2\n\n\nThe second strategy is to instead equate the SIMT threads with SIMD lanes (or a combination of SMT threads and SIMD lanes) on the KNL architecture. \nThis has the benefit of allowing the programmer to fully express all parallelism available on the GPU and KNL, but also has a significant drawback: \n\n\n\n\nBecause SIMD parallelism on the CPU/KNL is typically more restrictive and less flexible to express (requiring statements like \nOMP SIMD\n on relatively \nstraightforward loops), the programmer loses a lot of the flexibility the GPU architecture allows. \n\n\n\n\nThis is generally the strategy taken by applications using OpenMP for performance portability in its current implementation. We see in the case studies that to use OpenMP to \noffload work to the GPU, one currently needs an \nOMP SIMD\n directive (when compiler support exists at all). \n\n\nA Mixed Strategy\n\n\nOne may, in principle, define layers of parallelism to map groups of SIMT threads on a GPU (e.g., on separate SMs) to different SMTs on a CPU and leave \nadditional \nparallelism available to map to CPU vector lanes. Some of the performant models have support for this type of mapping. Kokkos for example supports this concept and can then insert \nOMP SIMD\n \npragmas on top of the innermost parallelizable loops when executing on a KNL or CPU. In practice, CPU performance will still ultimately depend on the \ncompilers ability to generate \nefficient vector code, meaning the vector/(inner SIMT) code here is limited to relatively simple vectorizable loops compared to what the SIMT model alone \ncan, in \ntheory, support. For example, Kokkos \nviews\n handle coalescing,\nbut vector parallelism on the CPU or KNL is left up to the compiler (with mixed results). As we see in the QCD case-study, the \ndevelopers may need to intervene (and potentially add new layers of parallelism to the code, e.g. multiple right hand sides) to make sure their code \ncan effectively use the wide AVX512 units on the KNL. \n\n\nMemory Management\n\n\nAs we mention above, the KNL and GPU architectures both have high-bandwidth, on-device memory as well as lower bandwidth access to traditional DDR on the \nnode. With support of unified virtual memory (UVM) on recent GPUs, both kinds of memory spaces can be accessed from all the components \nof GPU or KNL nodes. One difference is that \nbecause host memory is still separated from the GPU via PCI-express or NVLink, the gap in latency and bandwidth compared to the device memory can be \nsignificantly higher. \n\n\nIn principle, directives like OpenMP's \nmap\n function could be used to portably move data. The reality is that compiler implementations don't support this \nat present - mostly ignoring this when running on a CPU or KNL system (or failing to work at all on these systems). \n\n\nKokkos allows the user to define separate host and devmem domains, which does support this functionality in a portable way, in principle, but data placement in MCDRAM on the KNL is compiler dependent which makes this a challenge to support.\n\n\nIn the most common node configuration at NERSC and ALCF, the KNLs are configured in cache mode where the on-chip MCDRAM is treated as a transparent last-level \ncache instead of as an addressable memory domain. In this case, applications running see only a single memory domain and explicit management of the \nMCDRAM is not required. At this time, an equivalent option doesn't exist on GPU architectures.", 
            "title": "Strategy"
        }, 
        {
            "location": "/perfport/strategy/#strategy", 
            "text": "", 
            "title": "Strategy"
        }, 
        {
            "location": "/perfport/strategy/#code-structure-and-practices", 
            "text": "Before diving into a performance portability strategy, there are many things an application can do to make their code more \nportabile and ultimately more productive by applying recommended software engineering practices. These include:   Developing in a well-defined version-controlled environment  Documenting code so other developers can quickly join and contribute to a project  Maintaining a rigours test-suite and automated regression test framework on multiple platforms  Developing code in a modular way that abstracts performance-critical regions", 
            "title": "Code Structure and Practices"
        }, 
        {
            "location": "/perfport/strategy/#developing-a-performance-portability-strategy", 
            "text": "We noted in the introduction that the KNL and NVIDIA GPU architectures had a lot in common, including wide \"vectors\" or \"warps,\" as well as multiple tiers of memory, including on-package memory. However, before diving into to any particular performance-portability programming model, it is important to develop a high-level strategy for how a given problem best maps to these similar architecture features. In exploring various approaches to performance portability, we have found that different strategies can exist for exploiting these similarities.", 
            "title": "Developing a Performance Portability Strategy"
        }, 
        {
            "location": "/perfport/strategy/#threads-and-vectors", 
            "text": "One of the main challenges in providing a performance portability layer between KNL and GPU architectures is that vector parallelism on GPUs is expressed as \nSIMT (Single Instruction Multiple Threads) whereas a CPU contains both SMT (Simultaneous Multi-Threading) across cores/threads and SIMD (Single Instruction \nMultiple Data) across the lanes of the VPU (Vector Processing Unit). One of the challenges to be grappled with in using a performance portable approach \nis that SIMT parallelism lies somewhere in between SMT and SIMD parallelism in terms of expressibility, flexibility, and performance limitations:    SMT : Each SMT thread can perform independent instructions on independent data in independent registers. The work for each thread may be expressed in a \nscalar way.    SIMD : Each SIMD \"vector lane\" performs the same instruction on data from a single register. Vector lanes may be masked out to support branching, and \ngather/scatter instructions are supported to bring non-contiguous data in memory into a vector register. However, these often come with significant performance \ncosts.    SIMT : SIMT shares some qualities of SMT in the sense that each thread (running on what is referred to as a \"core\" on a GPU) has its own registers, and \nwork for each thread can be expressed in a \"scalar\" way. However, like SIMD and unlike SMT, the threads are not completely \nindependent. On current GPU architectures, (e.g. the K20X found in Titan) typically 32 threads form a warp in which the same instruction is being executed each cycle. Thus, if the work between threads diverges \nsignificantly, there is a significant reduction in performance. In addition, optimal performance is typically achieved when the threads in the warp are \noperating on contiguous data.     How do you use these different types of parallelism in order to enable portable programming? We identify three particular strategies, each with advantages and disadvantages. It is important when beginning to develop a performance portability plan to consider which of these strategies best maps to your application.", 
            "title": "Threads and Vectors"
        }, 
        {
            "location": "/perfport/strategy/#strategy-1", 
            "text": "The first strategy to performance portability is to equate SIMT threads on a GPU with SMT threads on CPU or a KNL. This has the advantage of allowing \nthe programmer to fully express the SIMT parallelism, but it does lead to a couple of challenges:    SMT threads typically operate on independent regions of data (e.g. the programmer typically breaks an array into the biggest contiguous chunks possible \nand gives each thread a chunk to work on). On the other hand, with SIMT threads, consecutive threads in a warp are ideally given \nconsecutive elements of an array to work on. This leads to the idea of coalescing - where data given to a single thread is \nstrided by the warp size (typically 32). The concept of coalescing is an artifact of viewing SIMT threads like SMT threads \ninstead of viewing them like vector lanes.     Another level of parallelism on the KNL (the vector parallelism) is then left on the table to exploit some other way (e.g. through the compiler, possibly\nwith the aid of hints).    This strategy is likely most appropriate if additional vector parallelism can be easily exploited by the compiler on the KNL, if vectorization doesn't \naffect performance or if one intends to manually intervene to ensure vector parallelism is exploited on the KNL. Frameworks like Kokkos (with the concept of \n\"views\") can help handle the coalescing issue in a portable way.", 
            "title": "Strategy 1"
        }, 
        {
            "location": "/perfport/strategy/#strategy-2", 
            "text": "The second strategy is to instead equate the SIMT threads with SIMD lanes (or a combination of SMT threads and SIMD lanes) on the KNL architecture. \nThis has the benefit of allowing the programmer to fully express all parallelism available on the GPU and KNL, but also has a significant drawback:    Because SIMD parallelism on the CPU/KNL is typically more restrictive and less flexible to express (requiring statements like  OMP SIMD  on relatively \nstraightforward loops), the programmer loses a lot of the flexibility the GPU architecture allows.    This is generally the strategy taken by applications using OpenMP for performance portability in its current implementation. We see in the case studies that to use OpenMP to \noffload work to the GPU, one currently needs an  OMP SIMD  directive (when compiler support exists at all).", 
            "title": "Strategy 2"
        }, 
        {
            "location": "/perfport/strategy/#a-mixed-strategy", 
            "text": "One may, in principle, define layers of parallelism to map groups of SIMT threads on a GPU (e.g., on separate SMs) to different SMTs on a CPU and leave \nadditional \nparallelism available to map to CPU vector lanes. Some of the performant models have support for this type of mapping. Kokkos for example supports this concept and can then insert  OMP SIMD  \npragmas on top of the innermost parallelizable loops when executing on a KNL or CPU. In practice, CPU performance will still ultimately depend on the \ncompilers ability to generate \nefficient vector code, meaning the vector/(inner SIMT) code here is limited to relatively simple vectorizable loops compared to what the SIMT model alone \ncan, in \ntheory, support. For example, Kokkos  views  handle coalescing,\nbut vector parallelism on the CPU or KNL is left up to the compiler (with mixed results). As we see in the QCD case-study, the \ndevelopers may need to intervene (and potentially add new layers of parallelism to the code, e.g. multiple right hand sides) to make sure their code \ncan effectively use the wide AVX512 units on the KNL.", 
            "title": "A Mixed Strategy"
        }, 
        {
            "location": "/perfport/strategy/#memory-management", 
            "text": "As we mention above, the KNL and GPU architectures both have high-bandwidth, on-device memory as well as lower bandwidth access to traditional DDR on the \nnode. With support of unified virtual memory (UVM) on recent GPUs, both kinds of memory spaces can be accessed from all the components \nof GPU or KNL nodes. One difference is that \nbecause host memory is still separated from the GPU via PCI-express or NVLink, the gap in latency and bandwidth compared to the device memory can be \nsignificantly higher.   In principle, directives like OpenMP's  map  function could be used to portably move data. The reality is that compiler implementations don't support this \nat present - mostly ignoring this when running on a CPU or KNL system (or failing to work at all on these systems).   Kokkos allows the user to define separate host and devmem domains, which does support this functionality in a portable way, in principle, but data placement in MCDRAM on the KNL is compiler dependent which makes this a challenge to support.  In the most common node configuration at NERSC and ALCF, the KNLs are configured in cache mode where the on-chip MCDRAM is treated as a transparent last-level \ncache instead of as an addressable memory domain. In this case, applications running see only a single memory domain and explicit management of the \nMCDRAM is not required. At this time, an equivalent option doesn't exist on GPU architectures.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/perfport/libraries/", 
            "text": "Libraries\n\n\nThe use of scientific libraries to achieve a measure of portable performance \nhas been used across many earlier computational platforms. \nThe ability to use higher-level abstractions via libraries allows developers to \nconcentrate their effort on algorithmic development, freeing them from having to \ndevote considerable effort to maximizing performance for many mathematical primitives. The most popular scientific libraries include \npackages designed to solve problems in linear algebra (both dense and sparse), compute fast Fourier transforms (FFT), multigrid \nmethods, and initial value problems for ordinary differential \nequations, along with other examples. Some of the best known scientific libraries include:\n\n\nThese tasks often represent a good measure of the computational work to be found in many scientific \ncodes. For some codes, almost all of the computational intensity can be found in the use of a\nlibrary for, e.g., eigenvalue solution or FFTs. If such a code makes use of libraries for their solution, \nportability is very often assured. Indeed, even if a particular library has not been ported to a new\narchitecture at a given time, the library source code is often available and can be compiled by the\nuser on the new platform. However, the best performance is realized when either the library maintainers or\nthe machine vendor (or both) undertake development to optimize a given library on a particular platform. \nThis obvious advantage has been realized by vendors, and for many of the libraries referred to earlier, this \noptimization is done as a matter of course.\n\n\nConversely, performance cannot be guaranteed with the same degree of certainty. First, although libraries often \ndo encapsulate a good measure of the required work, in most cases this is not \nall\n of the work, including \nwhat is often strictly serial work. This fundamental constraint is sometimes exacerbated by the\nfact that architecture-specific implementations are evolving, despite the best efforts of both vendors and\nlibrary maintainers. \n\n\nCodes with obvious \"hot spots\" can often make immediate use of libraries to achieve performance portability. \nThis is often easiest for codes written in Fortran and C, whereas bindings to many libraries in C++ can \nbe lacking or somewhat arcane to use. One of the biggest concerns in using libraries for extant codes is \nthe frequent requirement to recast data structures used in the code to the format used by the library. \nThe best approach to ameliorate this problem is often to simply use a memory copy: The relative cost of the \ncopy compared to the work done in the library is often small, and the use of a localized copy obviates the\nneed to change data structures pervasively throughout the code.  \n\n\nSome popular scientific libraries available on ASCR facilities\n\n\n\n\n\n\nBLAS/LAPACK\n - dense linear algebra\n\n\n\n\n\n\nBLAS and LAPACK are often contained in vendor-supplied library collections, like:\n\n\n\n\n\n\nMKL\n (Theta, Cori)\n\n\n\n\n\n\nCray LibSci\n (Theta, Cori, Titan) \n\n\n\n\n\n\n\n\n\n\nIn addition, other platform-specific implementations are available, like:\n\n\n\n\n\n\nMAGMA\n (GPU; Titan)\n\n\n\n\n\n\nPLASMA\n (multicore; Theta, Cori)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFFTW\n - Fast Fourier Transform\n\n\n\n\nLike LAPACK/BLAS, FFTW-like APIs can be found in MKL and ACML\n\n\n\n\n\n\n\n\nPETSc\n - PDE solvers\n\n\n\n\nPETSc is much more like a framework, and often requires more extensive code changes to use efficiently \n\n\n\n\n\n\n\n\nBenefits and Challenges\n\n\nBenefits\n\n\n\n\nOften encapsulate much of the computational intensity found in scientific codes\n\n\nCan allow immediate portability under some circumstances\n\n\nPerformance becomes a task for library authors/maintainers\n\n\n\n\nChallenges\n\n\n\n\nLimited set of portable libraries at present\n\n\nMay not capture all the important/expensive tasks in a given code\n\n\nOften require recasting data structures to match library requirements\n\n\nOpaque interior threading models", 
            "title": "Libraries"
        }, 
        {
            "location": "/perfport/libraries/#libraries", 
            "text": "The use of scientific libraries to achieve a measure of portable performance \nhas been used across many earlier computational platforms. \nThe ability to use higher-level abstractions via libraries allows developers to \nconcentrate their effort on algorithmic development, freeing them from having to \ndevote considerable effort to maximizing performance for many mathematical primitives. The most popular scientific libraries include \npackages designed to solve problems in linear algebra (both dense and sparse), compute fast Fourier transforms (FFT), multigrid \nmethods, and initial value problems for ordinary differential \nequations, along with other examples. Some of the best known scientific libraries include:  These tasks often represent a good measure of the computational work to be found in many scientific \ncodes. For some codes, almost all of the computational intensity can be found in the use of a\nlibrary for, e.g., eigenvalue solution or FFTs. If such a code makes use of libraries for their solution, \nportability is very often assured. Indeed, even if a particular library has not been ported to a new\narchitecture at a given time, the library source code is often available and can be compiled by the\nuser on the new platform. However, the best performance is realized when either the library maintainers or\nthe machine vendor (or both) undertake development to optimize a given library on a particular platform. \nThis obvious advantage has been realized by vendors, and for many of the libraries referred to earlier, this \noptimization is done as a matter of course.  Conversely, performance cannot be guaranteed with the same degree of certainty. First, although libraries often \ndo encapsulate a good measure of the required work, in most cases this is not  all  of the work, including \nwhat is often strictly serial work. This fundamental constraint is sometimes exacerbated by the\nfact that architecture-specific implementations are evolving, despite the best efforts of both vendors and\nlibrary maintainers.   Codes with obvious \"hot spots\" can often make immediate use of libraries to achieve performance portability. \nThis is often easiest for codes written in Fortran and C, whereas bindings to many libraries in C++ can \nbe lacking or somewhat arcane to use. One of the biggest concerns in using libraries for extant codes is \nthe frequent requirement to recast data structures used in the code to the format used by the library. \nThe best approach to ameliorate this problem is often to simply use a memory copy: The relative cost of the \ncopy compared to the work done in the library is often small, and the use of a localized copy obviates the\nneed to change data structures pervasively throughout the code.", 
            "title": "Libraries"
        }, 
        {
            "location": "/perfport/libraries/#some-popular-scientific-libraries-available-on-ascr-facilities", 
            "text": "BLAS/LAPACK  - dense linear algebra    BLAS and LAPACK are often contained in vendor-supplied library collections, like:    MKL  (Theta, Cori)    Cray LibSci  (Theta, Cori, Titan)       In addition, other platform-specific implementations are available, like:    MAGMA  (GPU; Titan)    PLASMA  (multicore; Theta, Cori)        FFTW  - Fast Fourier Transform   Like LAPACK/BLAS, FFTW-like APIs can be found in MKL and ACML     PETSc  - PDE solvers   PETSc is much more like a framework, and often requires more extensive code changes to use efficiently", 
            "title": "Some popular scientific libraries available on ASCR facilities"
        }, 
        {
            "location": "/perfport/libraries/#benefits-and-challenges", 
            "text": "", 
            "title": "Benefits and Challenges"
        }, 
        {
            "location": "/perfport/libraries/#benefits", 
            "text": "Often encapsulate much of the computational intensity found in scientific codes  Can allow immediate portability under some circumstances  Performance becomes a task for library authors/maintainers", 
            "title": "Benefits"
        }, 
        {
            "location": "/perfport/libraries/#challenges", 
            "text": "Limited set of portable libraries at present  May not capture all the important/expensive tasks in a given code  Often require recasting data structures to match library requirements  Opaque interior threading models", 
            "title": "Challenges"
        }, 
        {
            "location": "/perfport/directives/openacc/", 
            "text": "OpenACC\n\n\nOpenACC is a set of standardized, high-level pragmas that enable C/C++ and Fortran programmers \nto exploit parallel (co)processors, especially GPUs. OpenACC pragmas can be used to annotate \ncodes to enable data location, data transfer, and loop or code block parallelism.\n\n\nThough OpenACC has much in common with OpenMP, the syntax of the directives is different. \nMore importantly, OpenACC can best be described as having \na \ndescriptive\n model, in contrast to the more \nprescriptive\n model presented by OpenMP.\nThis difference in philosophy can most readily be seen by, e.g.,  comparing the \nacc loop\n directive\nto the OpenMP implementation of the equivalent construct. In OpenMP, the programmer has responsibility \nto specify how the parallelism in a loop is distributed (e.g., via \ndistribute\n and \nschedule\n clauses). \nIn OpenACC, the runtime determines how to decompose the iterations across gangs or workers and vectors.\nAt an even higher level, an OpenACC programmer can use the \nacc kernels\n construct to allow the compiler complete freedom \nto map the available parallelism in a code block to the available hardware.\n\n\nOpenACC at a glance\n\n\nSome of the most important  data and control clauses for two of the most \nused constructs in OpenACC programming - \n$acc parallel\n and \n$acc kernels\n - are \nlisted below. The data placement and movement clauses also appear in \n$acc data\n constructs.\n\n$acc loop\n provides control of parallelism similarly to \n$acc parallel\n but provides loop-level control. \n\n\nMuch more detail can be found at:\n\n\n\n\n\n\nopenacc.org\n\n\n\n\n\n\nOpenACC Best Practices Guide\n\n\n\n\n\n\nNVIDIA OpenACC resources\n\n\n\n\n\n\nOLCF Accelerator Programming Guide; Directive Programming\n\n\n\n\n\n\nOLCF Accelerator Programming Tutorials\n (includes examples of interoperability with CUDA and GPU libraries like CuFFT)\n\n\n\n\n\n\n\n\n\n\n\n\nconstruct\n\n\nimportant clauses\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n$acc parallel\n\n\n\n\n\n\n\n\n\n\n\n\nnum_gangs(expression)\n\n\nControls how many parallel gangs are created\n\n\n\n\n\n\n\n\nnum_workers(expression)\n\n\nControls how many workers are created in each gang\n\n\n\n\n\n\n\n\nvector_length(list)\n\n\nControls vector length of each worker\n\n\n\n\n\n\n\n\nprivate(list)\n\n\nA copy of each variable in list is allocated to each gang\n\n\n\n\n\n\n\n\nfirstprivate(list)\n\n\nprivate variables initialized from host\n\n\n\n\n\n\n\n\nreduction(operator:list)\n\n\nprivate variables combined across gangs\n\n\n\n\n\n\n$acc kernels\n\n\n\n\n\n\n\n\n\n\n\n\ncopy(list)\n\n\nAllocates memory on GPU and copies data from host to GPU when entering region and copies data to the host when exiting region\n\n\n\n\n\n\n\n\ncopyin(list)\n\n\nAllocates memory on GPU and copies data from host to GPU when entering region\n\n\n\n\n\n\n\n\ncopyout(list)\n\n\nAllocates memory on GPU and copies data to the host when exiting region\n\n\n\n\n\n\n\n\ncreate(list)\n\n\nAllocates memory on GPU but does not copy\n\n\n\n\n\n\n\n\npresent(list)\n\n\nData is already present on GPU from another containing data region\n\n\n\n\n\n\n\n\nHow to use OpenACC on ASCR facilities\n\n\nOLCF\n\n\nUsing C/C++\n\n\nPGI Compiler\n\n\n$ module load cudatoolkit\n$ cc -acc vecAdd.c -o vecAdd.out\n\n\n\n\nCray Compiler\n\n\n$ module switch PrgEnv-pgi PrgEnv-cray\n$ module load craype-accel-nvidia35\n$ cc -h pragma=acc vecAdd.c -o vecAdd.out\n\n\n\n\nUsing Fortran\n\n\nPGI Compiler\n\n\n$ module load cudatoolkit\n$ ftn -acc vecAdd.f90 -o vecAdd.out\n\n\n\n\nCray Compiler\n\n\n$ module switch PrgEnv-pgi PrgEnv-cray\n$ module load craype-accel-nvidia35\n$ ftn -h acc vecAdd.f90 -o vecAdd.out\n\n\n\n\nBenefits and Challenges\n\n\nBenefits\n\n\n\n\nAvailable for many different languages\n\n\nInteroperable with other approaches (e.g. CUDA or OpenMP)\n\n\nAllows performance optimization\n\n\nControlled by well-defined standards bodies\n\n\n\n\nChallenges\n\n\n\n\nRelatively few compiler implementations at present (versus OpenMP)\n\n\nEvolving standards\n\n\nDescriptive approach sometimes impedes very high performance for a given kernel", 
            "title": "OpenACC"
        }, 
        {
            "location": "/perfport/directives/openacc/#openacc", 
            "text": "OpenACC is a set of standardized, high-level pragmas that enable C/C++ and Fortran programmers \nto exploit parallel (co)processors, especially GPUs. OpenACC pragmas can be used to annotate \ncodes to enable data location, data transfer, and loop or code block parallelism.  Though OpenACC has much in common with OpenMP, the syntax of the directives is different. \nMore importantly, OpenACC can best be described as having \na  descriptive  model, in contrast to the more  prescriptive  model presented by OpenMP.\nThis difference in philosophy can most readily be seen by, e.g.,  comparing the  acc loop  directive\nto the OpenMP implementation of the equivalent construct. In OpenMP, the programmer has responsibility \nto specify how the parallelism in a loop is distributed (e.g., via  distribute  and  schedule  clauses). \nIn OpenACC, the runtime determines how to decompose the iterations across gangs or workers and vectors.\nAt an even higher level, an OpenACC programmer can use the  acc kernels  construct to allow the compiler complete freedom \nto map the available parallelism in a code block to the available hardware.", 
            "title": "OpenACC"
        }, 
        {
            "location": "/perfport/directives/openacc/#openacc-at-a-glance", 
            "text": "Some of the most important  data and control clauses for two of the most \nused constructs in OpenACC programming -  $acc parallel  and  $acc kernels  - are \nlisted below. The data placement and movement clauses also appear in  $acc data  constructs. $acc loop  provides control of parallelism similarly to  $acc parallel  but provides loop-level control.   Much more detail can be found at:    openacc.org    OpenACC Best Practices Guide    NVIDIA OpenACC resources    OLCF Accelerator Programming Guide; Directive Programming    OLCF Accelerator Programming Tutorials  (includes examples of interoperability with CUDA and GPU libraries like CuFFT)       construct  important clauses  description      $acc parallel       num_gangs(expression)  Controls how many parallel gangs are created     num_workers(expression)  Controls how many workers are created in each gang     vector_length(list)  Controls vector length of each worker     private(list)  A copy of each variable in list is allocated to each gang     firstprivate(list)  private variables initialized from host     reduction(operator:list)  private variables combined across gangs    $acc kernels       copy(list)  Allocates memory on GPU and copies data from host to GPU when entering region and copies data to the host when exiting region     copyin(list)  Allocates memory on GPU and copies data from host to GPU when entering region     copyout(list)  Allocates memory on GPU and copies data to the host when exiting region     create(list)  Allocates memory on GPU but does not copy     present(list)  Data is already present on GPU from another containing data region", 
            "title": "OpenACC at a glance"
        }, 
        {
            "location": "/perfport/directives/openacc/#how-to-use-openacc-on-ascr-facilities", 
            "text": "", 
            "title": "How to use OpenACC on ASCR facilities"
        }, 
        {
            "location": "/perfport/directives/openacc/#olcf", 
            "text": "", 
            "title": "OLCF"
        }, 
        {
            "location": "/perfport/directives/openacc/#using-cc", 
            "text": "PGI Compiler  $ module load cudatoolkit\n$ cc -acc vecAdd.c -o vecAdd.out  Cray Compiler  $ module switch PrgEnv-pgi PrgEnv-cray\n$ module load craype-accel-nvidia35\n$ cc -h pragma=acc vecAdd.c -o vecAdd.out", 
            "title": "Using C/C++"
        }, 
        {
            "location": "/perfport/directives/openacc/#using-fortran", 
            "text": "PGI Compiler  $ module load cudatoolkit\n$ ftn -acc vecAdd.f90 -o vecAdd.out  Cray Compiler  $ module switch PrgEnv-pgi PrgEnv-cray\n$ module load craype-accel-nvidia35\n$ ftn -h acc vecAdd.f90 -o vecAdd.out", 
            "title": "Using Fortran"
        }, 
        {
            "location": "/perfport/directives/openacc/#benefits-and-challenges", 
            "text": "", 
            "title": "Benefits and Challenges"
        }, 
        {
            "location": "/perfport/directives/openacc/#benefits", 
            "text": "Available for many different languages  Interoperable with other approaches (e.g. CUDA or OpenMP)  Allows performance optimization  Controlled by well-defined standards bodies", 
            "title": "Benefits"
        }, 
        {
            "location": "/perfport/directives/openacc/#challenges", 
            "text": "Relatively few compiler implementations at present (versus OpenMP)  Evolving standards  Descriptive approach sometimes impedes very high performance for a given kernel", 
            "title": "Challenges"
        }, 
        {
            "location": "/perfport/directives/openmp/", 
            "text": "OpenMP\n\n\nOpenMP is a specification for a set of compiler directives, library routines,\nand environment variables that can be used to specify high-level parallelism in\nFortran and C/C++ programs. The OpenMP API uses the fork-join model of parallel\nexecution. Multiple threads of execution perform tasks defined implicitly or\nexplicitly by OpenMP directives. (\nText taken from \nOpenMP\nFAQ\n and \nAPI\nspecification\n.\n)\n\n\nAlthough the directives in early versions of the OpenMP specification focused\non thread-level parallelism, more recent versions (especially 4.0 and 4.5) have\ngeneralized the specification to address more complex types (and multiple\ntypes) of parallelism, reflecting the increasing degree of on-node parallelism\nin HPC architectures. In particular, OpenMP 4.0 introduced the \nsimd\n and\n\ntarget\n constructs. We discuss each of these in detail below.\n\n\nomp simd\n\n\nDecorating a loop with the \nsimd\n construct informs the compiler that the loop\niterations are independent and can be executed with SIMD instructions (e.g.,\nAVX-512 on Intel Xeon Phi), e.g.,\n\n\n!$omp simd\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end simd\n\n\n\n\n\nExample output from a compiler optimization report for this loop is as follows:\n\nLOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15388: vectorization support: reference C(i) has aligned access   [ main.f90(10,19) ]\n   remark #15305: vectorization support: vector length 16\n   remark #15399: vectorization support: unroll factor set to 4\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 2\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.310\n   remark #15478: estimated potential speedup: 19.200\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\nThe \nsimd\n construct can be combined with the traditional \nparallel for\n (or\n\nparallel do\n in Fortran) constructs in order to execute the loop with both\nmulti-threading and with SIMD instructions, e.g.,\n\n\n!$omp parallel do simd\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end parallel do simd\n\n\n\n\n\nThe optimization report for the above snippet is as follows:\n\n\nBegin optimization report for: MAIN\n\n    Report from: OpenMP optimizations [openmp]\n\nmain.f90(8:9-8:9):OMP:MAIN__:  OpenMP DEFINED LOOP WAS PARALLELIZED\n\n    Report from: Vector optimizations [vec]\n\nLOOP BEGIN at main.f90(8,9)\n   remark #15388: vectorization support: reference a(i) has aligned access   [ main.f90(10,5) ]\n   remark #15389: vectorization support: reference b(i) has unaligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference c(i) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.667\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.370\n   remark #15478: estimated potential speedup: 15.670\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\n\nIt is important to note that compilers generally analyze loops (even those\nundecorated with \nomp simd\n) to determine if they can be executed with SIMD\ninstructions; applying this OpenMP construct usually allows the compiler to\nskip its loop dependency checks and immediately generate a SIMD version of the\nloop. Consequently, improper use of \nomp simd\n, e.g., on a loop which indeed\ncarries dependencies between iterations, can generate wrong code. This\nconstruct shifts the burden of correctness from the compiler to the user.\n\n\nFor example, consider the following loop, with a write-after-read dependency:\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \na\n(\ni\n-\n1\n)\n\n\nend do\n\n\n\n\nAttempting to compile it without the \nsimd\n construct yields the following\noptimization report:\n\n\nLOOP BEGIN at main.f90(8,3)\n   remark #15344: loop was not vectorized: vector dependence prevents vectorization\n   remark #15346: vector dependence: assumed FLOW dependence between a(i) (9:5) and a(i-1) (9:5)\nLOOP END\n\n\n\n\nThe compiler has determined that the loop iterations cannot be executed in\nSIMD. However, if we introduce the \nsimd\n construct, this assures the compiler\n(incorrectly) that the loop iterations can be executed in SIMD. Using the\nconstruct results in the following report:\n\n\nLOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference A(i-1) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 1\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.340\n   remark #15478: estimated potential speedup: 17.450\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\n\nThis example illustrates the prescriptive nature of OpenMP directives; they\nallow the user to instruct the compiler precisely how on-node parallelism\nshould be expressed, even if the compiler's own correctness-checking heuristics\nindicate that the desired approach will generate incorrect results.\n\n\nomp target\n\n\nThe OpenMP \ntarget\n device construct maps variables to a device data\nenvironment and executes the construct on that device. A region enclosed with\nthe \ntarget\n construct is assigned a target task to be executed on the device.\nThis construct supports several additional keywords which provide the user with\ncontrol of which data is moved to and from the device. Specifically, data\nmovement is achieved via the \nmap\n keyword, which accepts a list of variables\nto be copied between the host and device.\n\n\nConsider the following snippet:\n\n!$omp target map(to:b,c) map(from:a)\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end target\n\n\n\n\nThe compiler report from the following code offloaded to an Intel Xeon Phi\ncoprocessor is as follows:\n\n\n    Report from: Offload optimizations [offload]\n\nOFFLOAD:main(8,9):  Offload to target MIC 1\n Evaluate length/align/alloc_if/free_if/alloc/into expressions\n   Modifier expression assigned to __offload_free_if.19\n   Modifier expression assigned to __offload_alloc_if.20\n   Modifier expression assigned to __offload_free_if.21\n   Modifier expression assigned to __offload_alloc_if.22\n   Modifier expression assigned to __offload_free_if.23\n   Modifier expression assigned to __offload_alloc_if.24\n Data sent from host to target\n       i, scalar size 4 bytes\n       __offload_stack_ptr_main_$C_V$5.0, pointer to array reference expression with base\n       __offload_stack_ptr_main_$B_V$6.0, pointer to array reference expression with base\n Data received by host from target\n       __offload_stack_ptr_MAIN__.34, pointer to array reference expression with base \n\nLOOP BEGIN at main.f90(12,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(13,5) ]\n   remark #15389: vectorization support: reference B(i) has unaligned access   [ main.f90(13,12) ]\n   remark #15389: vectorization support: reference C(i) has unaligned access   [ main.f90(13,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.654\n   remark #15300: LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 7\n   remark #15477: vector cost: 0.400\n   remark #15478: estimated potential speedup: 17.180\n   remark #15488: --- end vector cost summary ---\n   remark #25015: Estimate of max trip count of loop=1024\nLOOP END\n\n\n\n\nThe same code offloaded to an NVIDIA Tesla GPU shows the following compiler\nreport (from a different compiler than the ones shown above):\n\n\n    1.           program main\n    2.             implicit none\n    3.\n    4.             integer, parameter :: array_size = 65536\n    5.             real, dimension(array_size) :: a, b, c\n    6.             integer :: i\n    7.\n    8.    fA--\n   b(:) = 1.0\n    9.    f---\n   c(:) = 2.0\n   10.\n   11.  + G----\n   !$omp target map(to:b,c) map(from:a)\n   12.    G g--\n   do i = 1, array_size\n   13.    G g        a(i) = b(i) * c(i)\n   14.    G g--\n   end do\n   15.    G----\n   !$omp end target\n   16.\n   17.             print *, a(1)\n   18.\n   19.           end program main\n\nftn-6230 ftn: VECTOR MAIN, File = main.f90, Line = 8\n  A loop starting at line 8 was replaced with multiple library calls.\n\nftn-6004 ftn: SCALAR MAIN, File = main.f90, Line = 9\n  A loop starting at line 9 was fused with the loop starting at line 8.\n\nftn-6405 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  A region starting at line 11 and ending at line 15 was placed on the accelerator.\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array \nc\n to accelerator, free at line 15 (acc_copyin).\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array \nb\n to accelerator, free at line 15 (acc_copyin).\n\nftn-6420 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory for whole array \na\n on accelerator, copy back at line 15 (acc_copyout).\n\nftn-6430 ftn: ACCEL MAIN, File = main.f90, Line = 12\n  A loop starting at line 12 was partitioned across the 128 threads within a threadblock.\n\n\n\n\nNote in the last compiler report that OpenMP automatically threads the loop and\npartitions the threads into threadblocks of the appropriate size for the device\nexecuting the loop.\n\n\nBenefits and Challenges\n\n\nBenefits\n\n\n\n\nAvailable for many different languages\n\n\nPrescriptive control of execution\n\n\nAllow performance optimization\n\n\nControlled by well-defined standards bodies\n\n\n\n\nChallenges\n\n\n\n\nSensitive to compiler support/maturity\n\n\nEvolving standards\n\n\n\n\nCompiler Support\n\n\nThe following table summarizes the OpenMP compiler support.\n\n\n\n\n\n\n\n\nCompiler\n\n\nLanguage Support\n\n\nArchitecture Support\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nGNU\n\n\nC/C++ (\n6.1), Fortran (\n7.2)\n\n\nRange of CPUs, nvidia GPUs\n\n\nperformance problems on CPU (\nbug #80859\n), compilation problems with \nenter data\n and \nexit data\n constructs (\nbug #81896\n)\n\n\n\n\n\n\nIntel\n\n\nC/C++, Fortran (\n17.0)\n\n\nx86-64 CPUs\n\n\ngenerated code in target region can be wrong, working on a testcase to demonstrate that problem\n\n\n\n\n\n\nCray\n\n\nC/C++, Fortran (\n8.5)\n\n\nx86-64 CPUs, nvidia GPUs\n\n\nfailure at link time when host-offloading is used (bug #189760)\n\n\n\n\n\n\nIBM\n\n\nC/C++ (\n13.1.5), Fortran (\n15.1.5)\n\n\nPower CPUs, nvidia GPUs\n\n\nmultiple-definitions link error when \ntarget\n region is contained in header file which is included by multiple compilation units (bug #147007)", 
            "title": "OpenMP"
        }, 
        {
            "location": "/perfport/directives/openmp/#openmp", 
            "text": "OpenMP is a specification for a set of compiler directives, library routines,\nand environment variables that can be used to specify high-level parallelism in\nFortran and C/C++ programs. The OpenMP API uses the fork-join model of parallel\nexecution. Multiple threads of execution perform tasks defined implicitly or\nexplicitly by OpenMP directives. ( Text taken from  OpenMP\nFAQ  and  API\nspecification . )  Although the directives in early versions of the OpenMP specification focused\non thread-level parallelism, more recent versions (especially 4.0 and 4.5) have\ngeneralized the specification to address more complex types (and multiple\ntypes) of parallelism, reflecting the increasing degree of on-node parallelism\nin HPC architectures. In particular, OpenMP 4.0 introduced the  simd  and target  constructs. We discuss each of these in detail below.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/perfport/directives/openmp/#omp-simd", 
            "text": "Decorating a loop with the  simd  construct informs the compiler that the loop\niterations are independent and can be executed with SIMD instructions (e.g.,\nAVX-512 on Intel Xeon Phi), e.g.,  !$omp simd  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end simd   Example output from a compiler optimization report for this loop is as follows: LOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15388: vectorization support: reference C(i) has aligned access   [ main.f90(10,19) ]\n   remark #15305: vectorization support: vector length 16\n   remark #15399: vectorization support: unroll factor set to 4\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 2\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.310\n   remark #15478: estimated potential speedup: 19.200\n   remark #15488: --- end vector cost summary ---\nLOOP END  The  simd  construct can be combined with the traditional  parallel for  (or parallel do  in Fortran) constructs in order to execute the loop with both\nmulti-threading and with SIMD instructions, e.g.,  !$omp parallel do simd  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end parallel do simd   The optimization report for the above snippet is as follows:  Begin optimization report for: MAIN\n\n    Report from: OpenMP optimizations [openmp]\n\nmain.f90(8:9-8:9):OMP:MAIN__:  OpenMP DEFINED LOOP WAS PARALLELIZED\n\n    Report from: Vector optimizations [vec]\n\nLOOP BEGIN at main.f90(8,9)\n   remark #15388: vectorization support: reference a(i) has aligned access   [ main.f90(10,5) ]\n   remark #15389: vectorization support: reference b(i) has unaligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference c(i) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.667\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.370\n   remark #15478: estimated potential speedup: 15.670\n   remark #15488: --- end vector cost summary ---\nLOOP END  It is important to note that compilers generally analyze loops (even those\nundecorated with  omp simd ) to determine if they can be executed with SIMD\ninstructions; applying this OpenMP construct usually allows the compiler to\nskip its loop dependency checks and immediately generate a SIMD version of the\nloop. Consequently, improper use of  omp simd , e.g., on a loop which indeed\ncarries dependencies between iterations, can generate wrong code. This\nconstruct shifts the burden of correctness from the compiler to the user.  For example, consider the following loop, with a write-after-read dependency: do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   a ( i - 1 )  end do   Attempting to compile it without the  simd  construct yields the following\noptimization report:  LOOP BEGIN at main.f90(8,3)\n   remark #15344: loop was not vectorized: vector dependence prevents vectorization\n   remark #15346: vector dependence: assumed FLOW dependence between a(i) (9:5) and a(i-1) (9:5)\nLOOP END  The compiler has determined that the loop iterations cannot be executed in\nSIMD. However, if we introduce the  simd  construct, this assures the compiler\n(incorrectly) that the loop iterations can be executed in SIMD. Using the\nconstruct results in the following report:  LOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference A(i-1) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 1\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.340\n   remark #15478: estimated potential speedup: 17.450\n   remark #15488: --- end vector cost summary ---\nLOOP END  This example illustrates the prescriptive nature of OpenMP directives; they\nallow the user to instruct the compiler precisely how on-node parallelism\nshould be expressed, even if the compiler's own correctness-checking heuristics\nindicate that the desired approach will generate incorrect results.", 
            "title": "omp simd"
        }, 
        {
            "location": "/perfport/directives/openmp/#omp-target", 
            "text": "The OpenMP  target  device construct maps variables to a device data\nenvironment and executes the construct on that device. A region enclosed with\nthe  target  construct is assigned a target task to be executed on the device.\nThis construct supports several additional keywords which provide the user with\ncontrol of which data is moved to and from the device. Specifically, data\nmovement is achieved via the  map  keyword, which accepts a list of variables\nto be copied between the host and device.  Consider the following snippet: !$omp target map(to:b,c) map(from:a)  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end target   The compiler report from the following code offloaded to an Intel Xeon Phi\ncoprocessor is as follows:      Report from: Offload optimizations [offload]\n\nOFFLOAD:main(8,9):  Offload to target MIC 1\n Evaluate length/align/alloc_if/free_if/alloc/into expressions\n   Modifier expression assigned to __offload_free_if.19\n   Modifier expression assigned to __offload_alloc_if.20\n   Modifier expression assigned to __offload_free_if.21\n   Modifier expression assigned to __offload_alloc_if.22\n   Modifier expression assigned to __offload_free_if.23\n   Modifier expression assigned to __offload_alloc_if.24\n Data sent from host to target\n       i, scalar size 4 bytes\n       __offload_stack_ptr_main_$C_V$5.0, pointer to array reference expression with base\n       __offload_stack_ptr_main_$B_V$6.0, pointer to array reference expression with base\n Data received by host from target\n       __offload_stack_ptr_MAIN__.34, pointer to array reference expression with base \n\nLOOP BEGIN at main.f90(12,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(13,5) ]\n   remark #15389: vectorization support: reference B(i) has unaligned access   [ main.f90(13,12) ]\n   remark #15389: vectorization support: reference C(i) has unaligned access   [ main.f90(13,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.654\n   remark #15300: LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 7\n   remark #15477: vector cost: 0.400\n   remark #15478: estimated potential speedup: 17.180\n   remark #15488: --- end vector cost summary ---\n   remark #25015: Estimate of max trip count of loop=1024\nLOOP END  The same code offloaded to an NVIDIA Tesla GPU shows the following compiler\nreport (from a different compiler than the ones shown above):      1.           program main\n    2.             implicit none\n    3.\n    4.             integer, parameter :: array_size = 65536\n    5.             real, dimension(array_size) :: a, b, c\n    6.             integer :: i\n    7.\n    8.    fA--    b(:) = 1.0\n    9.    f---    c(:) = 2.0\n   10.\n   11.  + G----    !$omp target map(to:b,c) map(from:a)\n   12.    G g--    do i = 1, array_size\n   13.    G g        a(i) = b(i) * c(i)\n   14.    G g--    end do\n   15.    G----    !$omp end target\n   16.\n   17.             print *, a(1)\n   18.\n   19.           end program main\n\nftn-6230 ftn: VECTOR MAIN, File = main.f90, Line = 8\n  A loop starting at line 8 was replaced with multiple library calls.\n\nftn-6004 ftn: SCALAR MAIN, File = main.f90, Line = 9\n  A loop starting at line 9 was fused with the loop starting at line 8.\n\nftn-6405 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  A region starting at line 11 and ending at line 15 was placed on the accelerator.\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array  c  to accelerator, free at line 15 (acc_copyin).\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array  b  to accelerator, free at line 15 (acc_copyin).\n\nftn-6420 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory for whole array  a  on accelerator, copy back at line 15 (acc_copyout).\n\nftn-6430 ftn: ACCEL MAIN, File = main.f90, Line = 12\n  A loop starting at line 12 was partitioned across the 128 threads within a threadblock.  Note in the last compiler report that OpenMP automatically threads the loop and\npartitions the threads into threadblocks of the appropriate size for the device\nexecuting the loop.", 
            "title": "omp target"
        }, 
        {
            "location": "/perfport/directives/openmp/#benefits-and-challenges", 
            "text": "", 
            "title": "Benefits and Challenges"
        }, 
        {
            "location": "/perfport/directives/openmp/#benefits", 
            "text": "Available for many different languages  Prescriptive control of execution  Allow performance optimization  Controlled by well-defined standards bodies", 
            "title": "Benefits"
        }, 
        {
            "location": "/perfport/directives/openmp/#challenges", 
            "text": "Sensitive to compiler support/maturity  Evolving standards", 
            "title": "Challenges"
        }, 
        {
            "location": "/perfport/directives/openmp/#compiler-support", 
            "text": "The following table summarizes the OpenMP compiler support.     Compiler  Language Support  Architecture Support  Notes      GNU  C/C++ ( 6.1), Fortran ( 7.2)  Range of CPUs, nvidia GPUs  performance problems on CPU ( bug #80859 ), compilation problems with  enter data  and  exit data  constructs ( bug #81896 )    Intel  C/C++, Fortran ( 17.0)  x86-64 CPUs  generated code in target region can be wrong, working on a testcase to demonstrate that problem    Cray  C/C++, Fortran ( 8.5)  x86-64 CPUs, nvidia GPUs  failure at link time when host-offloading is used (bug #189760)    IBM  C/C++ ( 13.1.5), Fortran ( 15.1.5)  Power CPUs, nvidia GPUs  multiple-definitions link error when  target  region is contained in header file which is included by multiple compilation units (bug #147007)", 
            "title": "Compiler Support"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/", 
            "text": "Kokkos\n\n\nKokkos\n implements a programming model in\nC++ for writing performance portable applications targeting all major HPC\nplatforms. For that purpose it provides abstractions for both parallel\nexecution of code and data management. Kokkos is designed to target complex\nnode architectures with N-level memory hierarchies and multiple types of\nexecution resources. It currently can use OpenMP, Pthreads and CUDA as backend\nprogramming models. (\nText provided by \nREADME\n in Kokkos source code repository\n).\n\n\nKokkos provides two types of abstraction which insulate the application\ndeveloper from the details of expressing parallelism on a particular\narchitecture. One is a \"memory space\", which characterizes where data resides\nin memory, e.g., in high-bandwidth memory, in DRAM, on GPU memory, etc. The\nother type is an \"execution space\", which describes how execution of a kernel\nis parallelized.\n\n\nIn terms of implementation, Kokkos expresses its memory and execution spaces\nvia templated C++ code. One constructs memory spaces through \"Views\", which are\ntemplated multi-dimensional arrays. One then issues an execution policy on the\ndata. The following snippet shows matrix-vector multiplication using Kokkos\nviews and a \"reduction\" execution policy. It is taken from the Kokkos \nGTC2017\ntutorial\n#2\n.\n\n\n  \nconst\n \nint\n \nN\n=\n128\n;\n \nconst\n \nint\n \nM\n=\n128\n;\n\n  \nKokkos\n::\nView\ndouble\n*\n  \nx\n(\n \nx\n,\n \nM\n \n);\n \n// a vector of length N\n\n  \nKokkos\n::\nView\ndouble\n*\n  \ny\n(\n \ny\n,\n \nN\n \n);\n \n// a vector of length M\n\n  \nKokkos\n::\nView\ndouble\n**\n \nA\n(\n \nA\n,\n \nN\n,\n \nM\n \n);\n \n// a matrix of size  NxM\n\n\n  \nKokkos\n::\nparallel_reduce\n(\n \nN\n,\n \nKOKKOS_LAMBDA\n \n(\n \nint\n \nj\n,\n \ndouble\n \nupdate\n \n)\n \n{\n\n    \ndouble\n \ntemp2\n \n=\n \n0\n;\n\n    \nfor\n \n(\n \nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nM\n;\n \n++\ni\n \n)\n \n{\n\n      \ntemp2\n \n+=\n \nA\n(\n \nj\n,\n \ni\n \n)\n \n*\n \nx\n(\n \ni\n \n);\n\n    \n}\n\n    \nupdate\n \n+=\n \ny\n(\n \nj\n \n)\n \n*\n \ntemp2\n;\n\n  \n},\n \nresult\n \n);\n\n\n\n\n\nNested parallelism\n\n\nModern CPU architectures exhibit a hierarchy of parallelism, and an application\nmust exploit the complete hierarchy in order to achieve good performance. Each\nlevel of the hierarchy is generally characterized by a group of execution\nresources which share a pool of memory.\n\n\nOn manycore CPUs such as Intel Xeon Phi, each processor contains ~70 cores,\neach of which supports 512 bit-wide SIMD instructions, and supports execution\nof 4 simultaneous hardware threads. On GPU-accelerated architectures, the host\nCPU has most of these same features, and the GPU often exhibits a very\ndifferent type of parallelism - a GPU may feature many streaming\nmultiprocessors, each of which executes a large number of threads, which are\ngrouped into clusters which execute synchronously.\n\n\nKokkos addresses this hierarchy via nested parallelism. In particular, at each\nlevel of a loop nest one can choose which execution policy to use. For example,\non Xeon Phi, one may wish to use multi-threading for the coarsest level of\nparallelism, and SIMD instructions for the finest level. On a GPU, one may wish\nto use multiple streaming multiprocessors as the coarsest level, and warps of\nthreads as the finest level. One can achieve this with the following example\ncode (taken from \nExercise 6 of the GTC2017\ntutorials\n):\n\n\nfor\n \n(\n \nint\n \nrepeat\n \n=\n \n0\n;\n \nrepeat\n \n \nnrepeat\n;\n \nrepeat\n++\n \n)\n \n{\n\n  \n// Application: \ny,Ax\n = y^T*A*x\n\n  \ndouble\n \nresult\n \n=\n \n0\n;\n\n\n  \nKokkos\n::\nparallel_reduce\n(\n \nteam_policy\n(\n \nE\n,\n \nKokkos\n::\nAUTO\n,\n \n32\n \n),\n \nKOKKOS_LAMBDA\n \n(\n\n  \nconst\n \nmember_type\n \nteamMember\n,\n \ndouble\n \nupdate\n \n)\n \n{\n\n    \nconst\n \nint\n \ne\n \n=\n \nteamMember\n.\nleague_rank\n();\n\n    \ndouble\n \ntempN\n \n=\n \n0\n;\n\n\n    \nKokkos\n::\nparallel_reduce\n(\n \nKokkos\n::\nTeamThreadRange\n(\n \nteamMember\n,\n \nN\n \n),\n \n[\n]\n \n(\n\n    \nconst\n \nint\n \nj\n,\n \ndouble\n \ninnerUpdateN\n \n)\n \n{\n\n      \ndouble\n \ntempM\n \n=\n \n0\n;\n\n\n      \nKokkos\n::\nparallel_reduce\n(\n \nKokkos\n::\nThreadVectorRange\n(\n \nteamMember\n,\n \nM\n \n),\n \n[\n]\n\n      \n(\n \nconst\n \nint\n \ni\n,\n \ndouble\n \ninnerUpdateM\n \n)\n \n{\n\n\n        \ninnerUpdateM\n \n+=\n \nA\n(\n \ne\n,\n \nj\n,\n \ni\n \n)\n \n*\n \nx\n(\n \ne\n,\n \ni\n \n);\n\n      \n},\n \ntempM\n \n);\n\n\n      \ninnerUpdateN\n \n+=\n \ny\n(\n \ne\n,\n \nj\n \n)\n \n*\n \ntempM\n;\n\n    \n},\n \ntempN\n \n);\n\n\n    \nKokkos\n::\nsingle\n(\n \nKokkos\n::\nPerTeam\n(\n \nteamMember\n \n),\n \n[\n]\n \n()\n \n{\n\n      \nupdate\n \n+=\n \ntempN\n;\n\n    \n});\n\n  \n},\n \nresult\n \n);\n\n\n\n\n\nBenefits and Challenges\n\n\nBenefits\n\n\n\n\nProvide good portability through the use of platform-dependent back-ends\n\n\nPromote good programming practices\n\n\n\n\nChallenges\n\n\n\n\nGenerally C++ only (at present)\n\n\nDo not represent recognized standards(yet)\n\n\nEvolving quickly", 
            "title": "Kokkos"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#kokkos", 
            "text": "Kokkos  implements a programming model in\nC++ for writing performance portable applications targeting all major HPC\nplatforms. For that purpose it provides abstractions for both parallel\nexecution of code and data management. Kokkos is designed to target complex\nnode architectures with N-level memory hierarchies and multiple types of\nexecution resources. It currently can use OpenMP, Pthreads and CUDA as backend\nprogramming models. ( Text provided by  README  in Kokkos source code repository ).  Kokkos provides two types of abstraction which insulate the application\ndeveloper from the details of expressing parallelism on a particular\narchitecture. One is a \"memory space\", which characterizes where data resides\nin memory, e.g., in high-bandwidth memory, in DRAM, on GPU memory, etc. The\nother type is an \"execution space\", which describes how execution of a kernel\nis parallelized.  In terms of implementation, Kokkos expresses its memory and execution spaces\nvia templated C++ code. One constructs memory spaces through \"Views\", which are\ntemplated multi-dimensional arrays. One then issues an execution policy on the\ndata. The following snippet shows matrix-vector multiplication using Kokkos\nviews and a \"reduction\" execution policy. It is taken from the Kokkos  GTC2017\ntutorial\n#2 .     const   int   N = 128 ;   const   int   M = 128 ; \n   Kokkos :: View double *    x (   x ,   M   );   // a vector of length N \n   Kokkos :: View double *    y (   y ,   N   );   // a vector of length M \n   Kokkos :: View double **   A (   A ,   N ,   M   );   // a matrix of size  NxM \n\n   Kokkos :: parallel_reduce (   N ,   KOKKOS_LAMBDA   (   int   j ,   double   update   )   { \n     double   temp2   =   0 ; \n     for   (   int   i   =   0 ;   i     M ;   ++ i   )   { \n       temp2   +=   A (   j ,   i   )   *   x (   i   ); \n     } \n     update   +=   y (   j   )   *   temp2 ; \n   },   result   );", 
            "title": "Kokkos"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#nested-parallelism", 
            "text": "Modern CPU architectures exhibit a hierarchy of parallelism, and an application\nmust exploit the complete hierarchy in order to achieve good performance. Each\nlevel of the hierarchy is generally characterized by a group of execution\nresources which share a pool of memory.  On manycore CPUs such as Intel Xeon Phi, each processor contains ~70 cores,\neach of which supports 512 bit-wide SIMD instructions, and supports execution\nof 4 simultaneous hardware threads. On GPU-accelerated architectures, the host\nCPU has most of these same features, and the GPU often exhibits a very\ndifferent type of parallelism - a GPU may feature many streaming\nmultiprocessors, each of which executes a large number of threads, which are\ngrouped into clusters which execute synchronously.  Kokkos addresses this hierarchy via nested parallelism. In particular, at each\nlevel of a loop nest one can choose which execution policy to use. For example,\non Xeon Phi, one may wish to use multi-threading for the coarsest level of\nparallelism, and SIMD instructions for the finest level. On a GPU, one may wish\nto use multiple streaming multiprocessors as the coarsest level, and warps of\nthreads as the finest level. One can achieve this with the following example\ncode (taken from  Exercise 6 of the GTC2017\ntutorials ):  for   (   int   repeat   =   0 ;   repeat     nrepeat ;   repeat ++   )   { \n   // Application:  y,Ax  = y^T*A*x \n   double   result   =   0 ; \n\n   Kokkos :: parallel_reduce (   team_policy (   E ,   Kokkos :: AUTO ,   32   ),   KOKKOS_LAMBDA   ( \n   const   member_type   teamMember ,   double   update   )   { \n     const   int   e   =   teamMember . league_rank (); \n     double   tempN   =   0 ; \n\n     Kokkos :: parallel_reduce (   Kokkos :: TeamThreadRange (   teamMember ,   N   ),   [ ]   ( \n     const   int   j ,   double   innerUpdateN   )   { \n       double   tempM   =   0 ; \n\n       Kokkos :: parallel_reduce (   Kokkos :: ThreadVectorRange (   teamMember ,   M   ),   [ ] \n       (   const   int   i ,   double   innerUpdateM   )   { \n\n         innerUpdateM   +=   A (   e ,   j ,   i   )   *   x (   e ,   i   ); \n       },   tempM   ); \n\n       innerUpdateN   +=   y (   e ,   j   )   *   tempM ; \n     },   tempN   ); \n\n     Kokkos :: single (   Kokkos :: PerTeam (   teamMember   ),   [ ]   ()   { \n       update   +=   tempN ; \n     }); \n   },   result   );", 
            "title": "Nested parallelism"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#benefits-and-challenges", 
            "text": "", 
            "title": "Benefits and Challenges"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#benefits", 
            "text": "Provide good portability through the use of platform-dependent back-ends  Promote good programming practices", 
            "title": "Benefits"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#challenges", 
            "text": "Generally C++ only (at present)  Do not represent recognized standards(yet)  Evolving quickly", 
            "title": "Challenges"
        }, 
        {
            "location": "/perfport/frameworks/raja/", 
            "text": "RAJA\n\n\nRAJA is a collection of C++ software abstractions, being developed at\nLawrence Livermore National Laboratory (LLNL), that enable architecture\nportability for HPC applications. The overarching goals of RAJA are to:\n\n\n\n\nMake existing (production) applications \nportable with minimal disruption\n\n\nProvide a model for new applications so that they are portable from\n    inception.\n\n\n\n\n(Text taken from RAJA \nREADME\n.)\n\n\nThe main conceptual abstraction in RAJA is a loop. A typical large multiphysics\ncode may contain O(10K) loops and these are where most computational work is\nperformed and where most fine-grained parallelism is available. RAJA defines a\nsystematic loop encapsulation paradigm that helps insulate application\ndevelopers from implementation details associated with software and hardware\nplatform choices. Such details include: non-portable compiler and\nplatform-specific directives, parallel programming model usage and constraints,\nand hardware-specific data management. \n(Text taken from \nRAJA\nPrimer\n.)\n\n\nRAJA implements three primary encapsulations: \nexecution policies\n,\n\nIndexSets\n, and \ndata type encapsulation\n. The execution policy instructs the\ncompiler regarding how the loop should execute and/or parallelized. IndexSets\ndescribe how the loop iteration space is traversed, e.g., stride-1, stride-2,\ntiled, etc. Data type encapsulation describes where and how the data is located\nin memory, e.g., its alignment on cache line boundaries, and aliasing\nproperties.\n\n\nAn example loop which adds two vectors, ported to RAJA and parallelized with\nOpenMP, is shown below (taken from the \nRAJA\nexamples\n):\n\n\n/*\n\n\n  RAJA::omp_parallel_for_exec - executes the forall loop using the\n\n\n  #pragma omp parallel for directive\n\n\n*/\n\n\nRAJA\n::\nforall\nRAJA\n::\nomp_parallel_for_exec\n\n  \n(\nRAJA\n::\nRangeSegment\n(\n0\n,\n \nN\n),\n \n[\n=\n](\nRAJA\n::\nIndex_type\n \ni\n)\n \n{\n\n    \nC\n[\ni\n]\n \n=\n \nA\n[\ni\n]\n \n+\n \nB\n[\ni\n];\n\n  \n});\n\n\n\n\n\nwhere \nRangeSegment(0, N)\n generates a sequential list of numbers from 0 to\n\nN\n. The same loop parallelized and executed on a GPU with CUDA looks similar:\n\n\nRAJA\n::\nforall\nRAJA\n::\ncuda_exec\nCUDA_BLOCK_SIZE\n\n  \n(\nRAJA\n::\nRangeSegment\n(\n0\n,\n \nN\n),\n \n[\n=\n]\n \n__device__\n(\nRAJA\n::\nIndex_type\n \ni\n)\n \n{\n\n    \nC\n[\ni\n]\n \n=\n \nA\n[\ni\n]\n \n+\n \nB\n[\ni\n];\n\n  \n});\n\n\ncheckSolution\n(\nC\n,\n \nN\n);\n\n\n\n\n\nBenefits and Challenges\n\n\nBenefits\n\n\n\n\nProvide good portability through the use of platform-dependent back-ends\n\n\nPromote good programming practices\n\n\nAllows incremental changes/improvements to an applications\n\n\n\n\nChallenges\n\n\n\n\nGenerally C++ only (at present)\n\n\nDo not represent recognized standards(yet)\n\n\nEvolving quickly", 
            "title": "RAJA"
        }, 
        {
            "location": "/perfport/frameworks/raja/#raja", 
            "text": "RAJA is a collection of C++ software abstractions, being developed at\nLawrence Livermore National Laboratory (LLNL), that enable architecture\nportability for HPC applications. The overarching goals of RAJA are to:   Make existing (production) applications  portable with minimal disruption  Provide a model for new applications so that they are portable from\n    inception.   (Text taken from RAJA  README .)  The main conceptual abstraction in RAJA is a loop. A typical large multiphysics\ncode may contain O(10K) loops and these are where most computational work is\nperformed and where most fine-grained parallelism is available. RAJA defines a\nsystematic loop encapsulation paradigm that helps insulate application\ndevelopers from implementation details associated with software and hardware\nplatform choices. Such details include: non-portable compiler and\nplatform-specific directives, parallel programming model usage and constraints,\nand hardware-specific data management.  (Text taken from  RAJA\nPrimer .)  RAJA implements three primary encapsulations:  execution policies , IndexSets , and  data type encapsulation . The execution policy instructs the\ncompiler regarding how the loop should execute and/or parallelized. IndexSets\ndescribe how the loop iteration space is traversed, e.g., stride-1, stride-2,\ntiled, etc. Data type encapsulation describes where and how the data is located\nin memory, e.g., its alignment on cache line boundaries, and aliasing\nproperties.  An example loop which adds two vectors, ported to RAJA and parallelized with\nOpenMP, is shown below (taken from the  RAJA\nexamples ):  /*    RAJA::omp_parallel_for_exec - executes the forall loop using the    #pragma omp parallel for directive  */  RAJA :: forall RAJA :: omp_parallel_for_exec \n   ( RAJA :: RangeSegment ( 0 ,   N ),   [ = ]( RAJA :: Index_type   i )   { \n     C [ i ]   =   A [ i ]   +   B [ i ]; \n   });   where  RangeSegment(0, N)  generates a sequential list of numbers from 0 to N . The same loop parallelized and executed on a GPU with CUDA looks similar:  RAJA :: forall RAJA :: cuda_exec CUDA_BLOCK_SIZE \n   ( RAJA :: RangeSegment ( 0 ,   N ),   [ = ]   __device__ ( RAJA :: Index_type   i )   { \n     C [ i ]   =   A [ i ]   +   B [ i ]; \n   });  checkSolution ( C ,   N );", 
            "title": "RAJA"
        }, 
        {
            "location": "/perfport/frameworks/raja/#benefits-and-challenges", 
            "text": "", 
            "title": "Benefits and Challenges"
        }, 
        {
            "location": "/perfport/frameworks/raja/#benefits", 
            "text": "Provide good portability through the use of platform-dependent back-ends  Promote good programming practices  Allows incremental changes/improvements to an applications", 
            "title": "Benefits"
        }, 
        {
            "location": "/perfport/frameworks/raja/#challenges", 
            "text": "Generally C++ only (at present)  Do not represent recognized standards(yet)  Evolving quickly", 
            "title": "Challenges"
        }, 
        {
            "location": "/perfport/dsl/", 
            "text": "Domain-Specific Languages (DSLs)\n\n\nIntroduction\n\n\nDomain-Specific Languages (DSLs) offer the possibility of expressing computation \nat a very high level of abstraction, rendering the mechanics of running on complex \nmodern platforms much more tractable. Given the level of abstraction afforded by DSLs, they\ncan be used to produce very portable code. \nBut, the performance of a DSL relies on the ability of the\ncompilers and runtime to effectively exploit architectural details (like the hardware \nand software environments) to carry out the high-level operations specified by the DSL programmer. \n\n\nThe actual implementation of DSLs can include annotations that are used to extend a general purpose\nlanguage (e.g. C or Fortran) or DSLs that are embedded in higher-level languages like Lua, Python, or R.  \n\n\nGiven the current state-of-the-art for DSLs, they are seldom adopted for new code projects, except\nas proof-of-principle exercises. Nevertheless, several HPC DSLs do exist, and if their structure is\ncongruent to a particular problem or set of problems, experimentation with this programming model \ncould prove fruitful. \n\n\nExamples\n\n\nNMODL, A DSL for Computational Neuroscience\n\n\nNMODL\n,\nan evolution of the earlier MODL, is designed for neuroscientists to enter\nneural tissue models into the NEURON tissue simulation code. The large-scale\nHPC branch of the simulation code, CoreNEURON, is central to the \nBlue Brain\nProject\n at EPFL, which is itself key to the brain\nsimulation component of the larger European Brain Project.\n\n\nCoreNEURON is essentially solving a large set of coupled nonlinear ODEs\nmodeling electrochemistry and other aspects of neural tissue behavior. NMODL\nreflects this, having key abstractions for dependent/independent variables and\ntheir derivatives in the equations; specifying chemical reactions; and\nmaintaining consistency of units. Handling of units is important as\nexperimental neuroscience is an important driver of the models. Single lines\nof NMODL are translated potentially into many lines of C code---lines which\nthe neuroscientist does not have to write (and get correct). Here is an\nexample showing some NMODL syntax \n[3]\n:\n\n\nNEURON {\n  SUFFIX leak\n  NONSPECIFIC_CURRENT I\n  RANGE i, e, g\n}\n\nPARAMETER {\n  g = 0.001  (siemens/cm2)  \n 0, 1e9 \n\n  e = -65    (millivolt)\n}\n\nASSIGNED {\n  i  (milliamp/cm2)\n  v  (millivolt)\n}\n\n\n\n\nNMODL is part of the performance portability strategy for CoreNEURON. The code\ngenerator produces code targeting specified architecture, using\nOpenMP/CUDA/OpenMP/vector intrinsics/OpenCL as appropriate. This allows for\nhighly optimized compiled code. The generic components of the CoreNEURON\nframework are optimized by experts, independently of the models coming in from\nNMODEL. Here is a sketch of the overall pipeline; in green are example\nspecific hardware architectures targeted:\n\n\n\n\nReferences\n\n\n\n\n\n\nM. L. Hines and N. T. Carnevale, \n\"Expanding NEURON's Repertoire of\nMechanisms with\nNMODL,\"\n\nin Neural Computation, vol. 12, no. 5, pp. 995-1007, May 1 2000.  doi:\n10.1162/089976600300015475\n\n\n\n\n\n\nNMODEL Model Description Language\n\n\n\n\n\n\nNerd Food: Tooling in Computational Neuroscience - Part I: NEURON\n\n\n\n\n\n\nOther Example DSLs for HPC\n\n\n\n\n\n\nEbb\n is a DSL for the solution of partial differential equations on meshes. \n\n\n\n\n\n\nAMRStencil\n is a DSL to implement solvers on AMR meshes.\n\n\n\n\n\n\nQDP++\n is a data-parallel programming environment for Lattice QCD.\n\n\n\n\n\n\nThe Tensor Contraction Engine\n is a DSL that allows chemists to specify the computation of tensor contractions encountered in many-body quantum calculations.", 
            "title": "DSL"
        }, 
        {
            "location": "/perfport/dsl/#domain-specific-languages-dsls", 
            "text": "", 
            "title": "Domain-Specific Languages (DSLs)"
        }, 
        {
            "location": "/perfport/dsl/#introduction", 
            "text": "Domain-Specific Languages (DSLs) offer the possibility of expressing computation \nat a very high level of abstraction, rendering the mechanics of running on complex \nmodern platforms much more tractable. Given the level of abstraction afforded by DSLs, they\ncan be used to produce very portable code. \nBut, the performance of a DSL relies on the ability of the\ncompilers and runtime to effectively exploit architectural details (like the hardware \nand software environments) to carry out the high-level operations specified by the DSL programmer.   The actual implementation of DSLs can include annotations that are used to extend a general purpose\nlanguage (e.g. C or Fortran) or DSLs that are embedded in higher-level languages like Lua, Python, or R.    Given the current state-of-the-art for DSLs, they are seldom adopted for new code projects, except\nas proof-of-principle exercises. Nevertheless, several HPC DSLs do exist, and if their structure is\ncongruent to a particular problem or set of problems, experimentation with this programming model \ncould prove fruitful.", 
            "title": "Introduction"
        }, 
        {
            "location": "/perfport/dsl/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/perfport/dsl/#nmodl-a-dsl-for-computational-neuroscience", 
            "text": "NMODL ,\nan evolution of the earlier MODL, is designed for neuroscientists to enter\nneural tissue models into the NEURON tissue simulation code. The large-scale\nHPC branch of the simulation code, CoreNEURON, is central to the  Blue Brain\nProject  at EPFL, which is itself key to the brain\nsimulation component of the larger European Brain Project.  CoreNEURON is essentially solving a large set of coupled nonlinear ODEs\nmodeling electrochemistry and other aspects of neural tissue behavior. NMODL\nreflects this, having key abstractions for dependent/independent variables and\ntheir derivatives in the equations; specifying chemical reactions; and\nmaintaining consistency of units. Handling of units is important as\nexperimental neuroscience is an important driver of the models. Single lines\nof NMODL are translated potentially into many lines of C code---lines which\nthe neuroscientist does not have to write (and get correct). Here is an\nexample showing some NMODL syntax  [3] :  NEURON {\n  SUFFIX leak\n  NONSPECIFIC_CURRENT I\n  RANGE i, e, g\n}\n\nPARAMETER {\n  g = 0.001  (siemens/cm2)    0, 1e9  \n  e = -65    (millivolt)\n}\n\nASSIGNED {\n  i  (milliamp/cm2)\n  v  (millivolt)\n}  NMODL is part of the performance portability strategy for CoreNEURON. The code\ngenerator produces code targeting specified architecture, using\nOpenMP/CUDA/OpenMP/vector intrinsics/OpenCL as appropriate. This allows for\nhighly optimized compiled code. The generic components of the CoreNEURON\nframework are optimized by experts, independently of the models coming in from\nNMODEL. Here is a sketch of the overall pipeline; in green are example\nspecific hardware architectures targeted:", 
            "title": "NMODL, A DSL for Computational Neuroscience"
        }, 
        {
            "location": "/perfport/dsl/#references", 
            "text": "M. L. Hines and N. T. Carnevale,  \"Expanding NEURON's Repertoire of\nMechanisms with\nNMODL,\" \nin Neural Computation, vol. 12, no. 5, pp. 995-1007, May 1 2000.  doi:\n10.1162/089976600300015475    NMODEL Model Description Language    Nerd Food: Tooling in Computational Neuroscience - Part I: NEURON", 
            "title": "References"
        }, 
        {
            "location": "/perfport/dsl/#other-example-dsls-for-hpc", 
            "text": "Ebb  is a DSL for the solution of partial differential equations on meshes.     AMRStencil  is a DSL to implement solvers on AMR meshes.    QDP++  is a data-parallel programming environment for Lattice QCD.    The Tensor Contraction Engine  is a DSL that allows chemists to specify the computation of tensor contractions encountered in many-body quantum calculations.", 
            "title": "Other Example DSLs for HPC"
        }, 
        {
            "location": "/case_studies/amr/overview/", 
            "text": "Thorsten / Brian to Write\n\n\nOverview of BoxLib/AMReX\n\n\nBoxLib\n is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the \nExascale\nComputing Project\n's\n\nBlock Structured Adaptive Mesh Refinement Co-Design\nCenter\n,\nBoxLib has since been superseded by\n\nAMReX\n. Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.\n\n\nBoxLib contains a wide variety of functionality:\n\n\n\n\nboundary condition exchange among boxes\n\n\nload balancing through regridding boxes among MPI processes\n\n\nmetadata operations such as computing volume intersections among boxes\n\n\nmemory management through pool allocators\n\n\n\n\nIn addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/amr/overview/#overview-of-boxlibamrex", 
            "text": "BoxLib  is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the  Exascale\nComputing Project 's Block Structured Adaptive Mesh Refinement Co-Design\nCenter ,\nBoxLib has since been superseded by AMReX . Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.  BoxLib contains a wide variety of functionality:   boundary condition exchange among boxes  load balancing through regridding boxes among MPI processes  metadata operations such as computing volume intersections among boxes  memory management through pool allocators   In addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview of BoxLib/AMReX"
        }, 
        {
            "location": "/case_studies/amr/parallelism/", 
            "text": "Parallelization\n\n\nBoxLib implements parallelization through a hybrid MPI+OpenMP approach.\n\n\nMPI\n\n\nAt the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series. An example is shown in the figure below, where\nthe red and green boxes are assigned to the same MPI process.\n\n\n\n\nOpenMP\n\n\nBoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.\n\n\nThis OpenMP parallelism is illustrated in the figure below. The box\ndistribution is the same as in the figure above, except in this case each box\nis further decomposed into smaller tiles. BoxLib then builds a list of all\ntiles comprising all boxes owned by a given MPI process, and distributes the\nlist among the OpenMP threads in the process. The figure below illustrates this\nprocess by color-coding each tile, with unique threads assigned to each color,\nsuch that the same thread may operate on tiles spanning different boxes. This\napproach avoids unnecessary thread synchronization which would occur if threads\nwere distributed among tiles within each box.\n\n\n\n\nThe figures on this page are taken from the AMReX User's Guide.", 
            "title": "Parallelism"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#parallelization", 
            "text": "BoxLib implements parallelization through a hybrid MPI+OpenMP approach.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#mpi", 
            "text": "At the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series. An example is shown in the figure below, where\nthe red and green boxes are assigned to the same MPI process.", 
            "title": "MPI"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#openmp", 
            "text": "BoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.  This OpenMP parallelism is illustrated in the figure below. The box\ndistribution is the same as in the figure above, except in this case each box\nis further decomposed into smaller tiles. BoxLib then builds a list of all\ntiles comprising all boxes owned by a given MPI process, and distributes the\nlist among the OpenMP threads in the process. The figure below illustrates this\nprocess by color-coding each tile, with unique threads assigned to each color,\nsuch that the same thread may operate on tiles spanning different boxes. This\napproach avoids unnecessary thread synchronization which would occur if threads\nwere distributed among tiles within each box.   The figures on this page are taken from the AMReX User's Guide.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/case_studies/amr/code_structure/", 
            "text": "Code Structure\n\n\nThe typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:\n\n\n// Advance the solution one grid at a time\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \nbx\n \n=\n \nmfi\n.\nvalidbox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \nbx\n.\nloVect\n(),\n \nbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nHere the \nMFIter\n object is an iterator over boxes owned by an MPI process. The\n\nBox\n object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables \nold_phi\n, \nnew_phi\n,\nand \nflux\n contain pointers to the arrays which contain the floating point data\non the grid. The \nupdate_phi\n function is a Fortran function which uses the\ndata from the \nBox\n object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:\n\n\nsubroutine \nupdate_phi\n(\nphiold\n,\n \nphinew\n,\n \nng_p\n,\n \nfluxx\n,\n \nfluxy\n,\n \nfluxz\n,\n \nng_f\n,\n \nlo\n,\n \nhi\n,\n \ndx\n,\n \ndt\n)\n \nbind\n(\nC\n,\n \nname\n=\nupdate_phi\n)\n\n\n  \ninteger\n          \n::\n \nlo\n(\n3\n),\n \nhi\n(\n3\n),\n \nng_p\n,\n \nng_f\n\n  \ndouble precision\n \n::\n \nphiold\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n \nphinew\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n  \nfluxx\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n+\n1\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxy\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n+\n1\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxz\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n+\n1\n)\n\n  \ndouble precision\n \n::\n \ndx\n,\n \ndt\n\n\n  \ninteger \ni\n,\nj\n,\nk\n\n\n  \ndo \nk\n=\nlo\n(\n3\n),\nhi\n(\n3\n)\n\n     \ndo \nj\n=\nlo\n(\n2\n),\nhi\n(\n2\n)\n\n        \ndo \ni\n=\nlo\n(\n1\n),\nhi\n(\n1\n)\n\n\n           \nphinew\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphiold\n(\ni\n,\nj\n,\nk\n)\n \n+\n \ndt\n \n*\n \n\n                \n(\n \nfluxx\n(\ni\n+\n1\n,\nj\n,\nk\n)\n-\nfluxx\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxy\n(\ni\n,\nj\n+\n1\n,\nk\n)\n-\nfluxy\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxz\n(\ni\n,\nj\n,\nk\n+\n1\n)\n-\nfluxz\n(\ni\n,\nj\n,\nk\n)\n \n)\n \n/\n \ndx\n\n\n        \nend do\n\n\n     end do\n\n\n  end do\n\n\n\n\n\nThe Fortran function constructs the appropriate \"view\" into each box using the\ndata from the \nBox\n object from the C++ function, as well as from the number of\nghost zones (\nng_p\n for \nold_phi\n and \nnew_phi\n, and \nng_f\n for \nflux\n).\n\n\nThe above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:\n\n\n// Advance the solution one grid at a time\n\n\n#ifdef _OPENMP\n\n\n#pragma omp parallel\n\n\n#endif\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n,\ntrue\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \ntbx\n \n=\n \nmfi\n.\ntilebox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \ntbx\n.\nloVect\n(),\n \ntbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nThe OpenMP parallelism is coarse-grained; rather than constructing a large\n\nBox\n from \nmfi.validbox()\n, it constructs a smaller \nBox\n from\n\nmfi.tilebox()\n. The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.\n\n\nMemory Management\n\n\nBoxLib abstracts the memory management by using the abstract \nArena\n class.\n\n\n#ifndef BL_ARENA_H\n\n\n#define BL_ARENA_H\n\n\n\n#include\n \nwinstd.H\n\n\n#include\n \ncstddef\n\n\n\nclass\n \nArena\n;\n\n\n\nnamespace\n \nBoxLib\n\n\n{\n\n    \nArena\n*\n \nThe_Arena\n \n();\n\n\n}\n\n\n\n//\n\n\n// A Virtual Base Class for Dynamic Memory Management\n\n\n//\n\n\n// This is a virtual base class for objects that manage their own dynamic\n\n\n// memory allocation.  Since it is a virtual base class, you have to derive\n\n\n// something from it to use it.\n\n\n//\n\n\n\nclass\n \nArena\n\n\n{\n\n\npublic\n:\n\n\n    \nvirtual\n \n~\nArena\n \n();\n\n    \n//\n\n    \n// Allocate a dynamic memory arena of size sz.\n\n    \n// A pointer to this memory should be returned.\n\n    \n//\n\n    \nvirtual\n \nvoid\n*\n \nalloc\n \n(\nstd\n::\nsize_t\n \nsz\n)\n \n=\n \n0\n;\n\n    \n//\n\n    \n// A pure virtual function for deleting the arena pointed to by pt.\n\n    \n//\n\n    \nvirtual\n \nvoid\n \nfree\n \n(\nvoid\n*\n \npt\n)\n \n=\n \n0\n;\n\n    \n//\n\n    \n// Given a minimum required arena size of sz bytes, this returns\n\n    \n// the next largest arena size that will align to align_size bytes.\n\n    \n//\n\n    \nstatic\n \nstd\n::\nsize_t\n \nalign\n \n(\nstd\n::\nsize_t\n \nsz\n);\n\n\n\nprotected\n:\n\n\n    \nstatic\n \nconst\n \nunsigned\n \nint\n \nalign_size\n \n=\n \n16\n;\n\n\n};\n\n\n\n#endif \n/*BL_ARENA_H*/\n\n\n\nThe most general container class in BoxLib, the \nBaseFab\n, calls the \nArena\n data allocator to allocate its memory. Therefore, by providing a specialized Arena-descendant, the user can easily plug in his own data containers or decorate his allocations with alignment or memory placing directives.\n\n\nC++ Kernel Rewrites\n\n\nSome programming models do not support Fortran and thus for using those, we need to port our kernels to C++. Below we show the ported GSRB kernel. For the sake of simplicity, we work directly with the fabs and not with the data pointers as we do in Fortran, so that we can use the access operator to index into our data containers. \n\n\nvoid\n \nC_GSRB_3D\n(\n\n\nconst\n \nBox\n \nbx\n,\n\n\nconst\n \nBox\n \nbbx\n,\n\n\nconst\n \nint\n \nnc\n,\n\n\nconst\n \nint\n \nrb\n,\n\n\nconst\n \nReal\n \nalpha\n,\n\n\nconst\n \nReal\n \nbeta\n,\n\n\nFArrayBox\n \nphi\n,\n\n\nconst\n \nFArrayBox\n \nrhs\n,\n\n\nconst\n \nFArrayBox\n \na\n,\n\n\nconst\n \nFArrayBox\n \nbX\n,\n\n\nconst\n \nFArrayBox\n \nbY\n,\n\n\nconst\n \nFArrayBox\n \nbZ\n,\n\n\nconst\n \nFArrayBox\n \nf0\n,\n\n\nconst\n \nMask\n \nm0\n,\n\n\nconst\n \nFArrayBox\n \nf1\n,\n\n\nconst\n \nMask\n \nm1\n,\n\n\nconst\n \nFArrayBox\n \nf2\n,\n\n\nconst\n \nMask\n \nm2\n,\n\n\nconst\n \nFArrayBox\n \nf3\n,\n\n\nconst\n \nMask\n \nm3\n,\n\n\nconst\n \nFArrayBox\n \nf4\n,\n\n\nconst\n \nMask\n \nm4\n,\n\n\nconst\n \nFArrayBox\n \nf5\n,\n\n\nconst\n \nMask\n \nm5\n,\n\n\nconst\n \nReal\n*\n \nh\n)\n\n\n{\n\n    \n//box extends:\n\n    \nconst\n \nint\n \n*\nlo\n \n=\n \nbx\n.\nloVect\n();\n\n    \nconst\n \nint\n \n*\nhi\n \n=\n \nbx\n.\nhiVect\n();\n\n    \n//blo\n\n    \nconst\n \nint\n \n*\nblo\n \n=\n \nbbx\n.\nloVect\n();\n\n    \nconst\n \nint\n \n*\nbhi\n \n=\n \nbbx\n.\nhiVect\n();\n\n\n    \n//some parameters\n\n    \nReal\n \nomega\n=\n \n1.15\n;\n\n    \nReal\n \ndhx\n \n=\n \nbeta\n/\n(\nh\n[\n0\n]\n*\nh\n[\n0\n]);\n\n    \nReal\n \ndhy\n \n=\n \nbeta\n/\n(\nh\n[\n1\n]\n*\nh\n[\n1\n]);\n\n    \nReal\n \ndhz\n \n=\n \nbeta\n/\n(\nh\n[\n2\n]\n*\nh\n[\n2\n]);\n\n\n    \nfor\n \n(\nint\n \nn\n \n=\n \n0\n;\n \nn\nnc\n;\n \nn\n++\n){\n\n        \nfor\n \n(\nint\n \nk\n \n=\n \nlo\n[\n2\n];\n \nk\n \n=\n \nhi\n[\n2\n];\n \n++\nk\n)\n \n{\n\n            \nfor\n \n(\nint\n \nj\n \n=\n \nlo\n[\n1\n];\n \nj\n \n=\n \nhi\n[\n1\n];\n \n++\nj\n)\n \n{\n\n                \nint\n \nioff\n \n=\n \n(\nlo\n[\n0\n]\n \n+\n \nj\n \n+\n \nk\n \n+\n \nrb\n)\n%\n2\n;\n\n                \nfor\n \n(\nint\n \ni\n \n=\n \nlo\n[\n0\n]\n \n+\n \nioff\n;\n \ni\n \n=\n \nhi\n[\n0\n];\n \ni\n+=\n2\n)\n \n{\n\n\n                    \n//BC terms\n\n                    \nReal\n \ncf0\n \n=\n \n(\n \n(\ni\n==\nblo\n[\n0\n])\n \n \n(\nm0\n(\nIntVect\n(\nblo\n[\n0\n]\n-\n1\n,\nj\n,\nk\n))\n0\n)\n \n?\n \nf0\n(\nIntVect\n(\nblo\n[\n0\n],\nj\n,\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf1\n \n=\n \n(\n \n(\nj\n==\nblo\n[\n1\n])\n \n \n(\nm1\n(\nIntVect\n(\ni\n,\nblo\n[\n1\n]\n-\n1\n,\nk\n))\n0\n)\n \n?\n \nf1\n(\nIntVect\n(\ni\n,\nblo\n[\n1\n],\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf2\n \n=\n \n(\n \n(\nk\n==\nblo\n[\n2\n])\n \n \n(\nm2\n(\nIntVect\n(\ni\n,\nj\n,\nblo\n[\n2\n]\n-\n1\n))\n0\n)\n \n?\n \nf2\n(\nIntVect\n(\ni\n,\nj\n,\nblo\n[\n2\n]))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf3\n \n=\n \n(\n \n(\ni\n==\nbhi\n[\n0\n])\n \n \n(\nm3\n(\nIntVect\n(\nbhi\n[\n0\n]\n+\n1\n,\nj\n,\nk\n))\n0\n)\n \n?\n \nf3\n(\nIntVect\n(\nbhi\n[\n0\n],\nj\n,\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf4\n \n=\n \n(\n \n(\nj\n==\nbhi\n[\n1\n])\n \n \n(\nm4\n(\nIntVect\n(\ni\n,\nbhi\n[\n1\n]\n+\n1\n,\nk\n))\n0\n)\n \n?\n \nf4\n(\nIntVect\n(\ni\n,\nbhi\n[\n1\n],\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf5\n \n=\n \n(\n \n(\nk\n==\nbhi\n[\n2\n])\n \n \n(\nm5\n(\nIntVect\n(\ni\n,\nj\n,\nbhi\n[\n2\n]\n+\n1\n))\n0\n)\n \n?\n \nf5\n(\nIntVect\n(\ni\n,\nj\n,\nbhi\n[\n2\n]))\n \n:\n \n0.\n \n);\n\n\n                    \n//assign ORA constants\n\n                    \ndouble\n \ngamma\n \n=\n \nalpha\n \n*\n \na\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n\n                                    \n+\n \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n)))\n\n                                    \n+\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n)))\n\n                                    \n+\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n)));\n\n\n                    \ndouble\n \ng_m_d\n \n=\n \ngamma\n\n                                    \n-\n \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf0\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n))\n*\ncf3\n)\n\n                                    \n-\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf1\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n))\n*\ncf4\n)\n\n                                    \n-\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf2\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n))\n*\ncf5\n);\n\n\n                    \ndouble\n \nrho\n \n=\n  \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n-\n1\n,\nj\n,\nk\n),\nn\n)\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n),\nn\n))\n\n                                \n+\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n-\n1\n,\nk\n),\nn\n)\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n),\nn\n))\n\n                                \n+\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n-\n1\n),\nn\n)\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n),\nn\n));\n\n\n                    \ndouble\n \nres\n \n=\n \nrhs\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n-\n \ngamma\n \n*\n \nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n+\n \nrho\n;\n\n                    \nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n+=\n \nomega\n/\ng_m_d\n \n*\n \nres\n;\n\n                \n}\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\nWe try to avoid porting all Fortran kernels for our explorations but some of the frameworks would basically require that. We will make comments about this in appropriate places.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#code-structure", 
            "text": "The typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:  // Advance the solution one grid at a time  for   (   MFIter   mfi ( old_phi );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   bx   =   mfi . validbox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   bx . loVect (),   bx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   Here the  MFIter  object is an iterator over boxes owned by an MPI process. The Box  object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables  old_phi ,  new_phi ,\nand  flux  contain pointers to the arrays which contain the floating point data\non the grid. The  update_phi  function is a Fortran function which uses the\ndata from the  Box  object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:  subroutine  update_phi ( phiold ,   phinew ,   ng_p ,   fluxx ,   fluxy ,   fluxz ,   ng_f ,   lo ,   hi ,   dx ,   dt )   bind ( C ,   name = update_phi ) \n\n   integer            ::   lo ( 3 ),   hi ( 3 ),   ng_p ,   ng_f \n   double precision   ::   phiold ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::   phinew ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::    fluxx ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f + 1 , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxy ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f + 1 , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxz ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f + 1 ) \n   double precision   ::   dx ,   dt \n\n   integer  i , j , k \n\n   do  k = lo ( 3 ), hi ( 3 ) \n      do  j = lo ( 2 ), hi ( 2 ) \n         do  i = lo ( 1 ), hi ( 1 ) \n\n            phinew ( i , j , k )   =   phiold ( i , j , k )   +   dt   *   \n                 (   fluxx ( i + 1 , j , k ) - fluxx ( i , j , k )   \n                 + fluxy ( i , j + 1 , k ) - fluxy ( i , j , k )   \n                 + fluxz ( i , j , k + 1 ) - fluxz ( i , j , k )   )   /   dx \n\n         end do       end do    end do   The Fortran function constructs the appropriate \"view\" into each box using the\ndata from the  Box  object from the C++ function, as well as from the number of\nghost zones ( ng_p  for  old_phi  and  new_phi , and  ng_f  for  flux ).  The above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:  // Advance the solution one grid at a time  #ifdef _OPENMP  #pragma omp parallel  #endif  for   (   MFIter   mfi ( old_phi , true );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   tbx   =   mfi . tilebox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   tbx . loVect (),   tbx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   The OpenMP parallelism is coarse-grained; rather than constructing a large Box  from  mfi.validbox() , it constructs a smaller  Box  from mfi.tilebox() . The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#memory-management", 
            "text": "BoxLib abstracts the memory management by using the abstract  Arena  class.  #ifndef BL_ARENA_H  #define BL_ARENA_H  #include   winstd.H  #include   cstddef  class   Arena ;  namespace   BoxLib  { \n     Arena *   The_Arena   ();  }  //  // A Virtual Base Class for Dynamic Memory Management  //  // This is a virtual base class for objects that manage their own dynamic  // memory allocation.  Since it is a virtual base class, you have to derive  // something from it to use it.  //  class   Arena  {  public : \n\n     virtual   ~ Arena   (); \n     // \n     // Allocate a dynamic memory arena of size sz. \n     // A pointer to this memory should be returned. \n     // \n     virtual   void *   alloc   ( std :: size_t   sz )   =   0 ; \n     // \n     // A pure virtual function for deleting the arena pointed to by pt. \n     // \n     virtual   void   free   ( void *   pt )   =   0 ; \n     // \n     // Given a minimum required arena size of sz bytes, this returns \n     // the next largest arena size that will align to align_size bytes. \n     // \n     static   std :: size_t   align   ( std :: size_t   sz );  protected : \n\n     static   const   unsigned   int   align_size   =   16 ;  };  #endif  /*BL_ARENA_H*/  \nThe most general container class in BoxLib, the  BaseFab , calls the  Arena  data allocator to allocate its memory. Therefore, by providing a specialized Arena-descendant, the user can easily plug in his own data containers or decorate his allocations with alignment or memory placing directives.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#c-kernel-rewrites", 
            "text": "Some programming models do not support Fortran and thus for using those, we need to port our kernels to C++. Below we show the ported GSRB kernel. For the sake of simplicity, we work directly with the fabs and not with the data pointers as we do in Fortran, so that we can use the access operator to index into our data containers.   void   C_GSRB_3D (  const   Box   bx ,  const   Box   bbx ,  const   int   nc ,  const   int   rb ,  const   Real   alpha ,  const   Real   beta ,  FArrayBox   phi ,  const   FArrayBox   rhs ,  const   FArrayBox   a ,  const   FArrayBox   bX ,  const   FArrayBox   bY ,  const   FArrayBox   bZ ,  const   FArrayBox   f0 ,  const   Mask   m0 ,  const   FArrayBox   f1 ,  const   Mask   m1 ,  const   FArrayBox   f2 ,  const   Mask   m2 ,  const   FArrayBox   f3 ,  const   Mask   m3 ,  const   FArrayBox   f4 ,  const   Mask   m4 ,  const   FArrayBox   f5 ,  const   Mask   m5 ,  const   Real *   h )  { \n     //box extends: \n     const   int   * lo   =   bx . loVect (); \n     const   int   * hi   =   bx . hiVect (); \n     //blo \n     const   int   * blo   =   bbx . loVect (); \n     const   int   * bhi   =   bbx . hiVect (); \n\n     //some parameters \n     Real   omega =   1.15 ; \n     Real   dhx   =   beta / ( h [ 0 ] * h [ 0 ]); \n     Real   dhy   =   beta / ( h [ 1 ] * h [ 1 ]); \n     Real   dhz   =   beta / ( h [ 2 ] * h [ 2 ]); \n\n     for   ( int   n   =   0 ;   n nc ;   n ++ ){ \n         for   ( int   k   =   lo [ 2 ];   k   =   hi [ 2 ];   ++ k )   { \n             for   ( int   j   =   lo [ 1 ];   j   =   hi [ 1 ];   ++ j )   { \n                 int   ioff   =   ( lo [ 0 ]   +   j   +   k   +   rb ) % 2 ; \n                 for   ( int   i   =   lo [ 0 ]   +   ioff ;   i   =   hi [ 0 ];   i += 2 )   { \n\n                     //BC terms \n                     Real   cf0   =   (   ( i == blo [ 0 ])     ( m0 ( IntVect ( blo [ 0 ] - 1 , j , k )) 0 )   ?   f0 ( IntVect ( blo [ 0 ], j , k ))   :   0.   ); \n                     Real   cf1   =   (   ( j == blo [ 1 ])     ( m1 ( IntVect ( i , blo [ 1 ] - 1 , k )) 0 )   ?   f1 ( IntVect ( i , blo [ 1 ], k ))   :   0.   ); \n                     Real   cf2   =   (   ( k == blo [ 2 ])     ( m2 ( IntVect ( i , j , blo [ 2 ] - 1 )) 0 )   ?   f2 ( IntVect ( i , j , blo [ 2 ]))   :   0.   ); \n                     Real   cf3   =   (   ( i == bhi [ 0 ])     ( m3 ( IntVect ( bhi [ 0 ] + 1 , j , k )) 0 )   ?   f3 ( IntVect ( bhi [ 0 ], j , k ))   :   0.   ); \n                     Real   cf4   =   (   ( j == bhi [ 1 ])     ( m4 ( IntVect ( i , bhi [ 1 ] + 1 , k )) 0 )   ?   f4 ( IntVect ( i , bhi [ 1 ], k ))   :   0.   ); \n                     Real   cf5   =   (   ( k == bhi [ 2 ])     ( m5 ( IntVect ( i , j , bhi [ 2 ] + 1 )) 0 )   ?   f5 ( IntVect ( i , j , bhi [ 2 ]))   :   0.   ); \n\n                     //assign ORA constants \n                     double   gamma   =   alpha   *   a ( IntVect ( i , j , k )) \n                                     +   dhx   *   ( bX ( IntVect ( i , j , k ))   +   bX ( IntVect ( i + 1 , j , k ))) \n                                     +   dhy   *   ( bY ( IntVect ( i , j , k ))   +   bY ( IntVect ( i , j + 1 , k ))) \n                                     +   dhz   *   ( bZ ( IntVect ( i , j , k ))   +   bZ ( IntVect ( i , j , k + 1 ))); \n\n                     double   g_m_d   =   gamma \n                                     -   dhx   *   ( bX ( IntVect ( i , j , k )) * cf0   +   bX ( IntVect ( i + 1 , j , k )) * cf3 ) \n                                     -   dhy   *   ( bY ( IntVect ( i , j , k )) * cf1   +   bY ( IntVect ( i , j + 1 , k )) * cf4 ) \n                                     -   dhz   *   ( bZ ( IntVect ( i , j , k )) * cf2   +   bZ ( IntVect ( i , j , k + 1 )) * cf5 ); \n\n                     double   rho   =    dhx   *   ( bX ( IntVect ( i , j , k )) * phi ( IntVect ( i - 1 , j , k ), n )   +   bX ( IntVect ( i + 1 , j , k )) * phi ( IntVect ( i + 1 , j , k ), n )) \n                                 +   dhy   *   ( bY ( IntVect ( i , j , k )) * phi ( IntVect ( i , j - 1 , k ), n )   +   bY ( IntVect ( i , j + 1 , k )) * phi ( IntVect ( i , j + 1 , k ), n )) \n                                 +   dhz   *   ( bZ ( IntVect ( i , j , k )) * phi ( IntVect ( i , j , k - 1 ), n )   +   bZ ( IntVect ( i , j , k + 1 )) * phi ( IntVect ( i , j , k + 1 ), n )); \n\n                     double   res   =   rhs ( IntVect ( i , j , k ), n )   -   gamma   *   phi ( IntVect ( i , j , k ), n )   +   rho ; \n                     phi ( IntVect ( i , j , k ), n )   +=   omega / g_m_d   *   res ; \n                 } \n             } \n         } \n     }  }   We try to avoid porting all Fortran kernels for our explorations but some of the frameworks would basically require that. We will make comments about this in appropriate places.", 
            "title": "C++ Kernel Rewrites"
        }, 
        {
            "location": "/case_studies/amr/multigrid/", 
            "text": "Geometric Multigrid\n\n\nMany problems encountered in BoxLib applications require solutions to linear\nsystem, e.g., \nelliptic partial differential equations\n such as the \nPoisson\nequation\n for self-gravity, and the \ndiffusion equation\n. BoxLib therefore\nincludes \ngeometric multigrid solvers\n for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.\n\n\nGeometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:\n\n\n\n\nrelaxation\n\n\nrestriction\n\n\nprolongation\n\n\ncoarse-grid linear solve (either approximate or exact)\n\n\n\n\nAlthough here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms. Although these steps are algorithmically unique, we note that all\nof them feature low arithmetic intensity and are thus sensitive to cache and\nmemory bandwidth.\n\n\nRelaxation\n\n\nA relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original \nGauss-Seidel method\n by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n     \nioff\n \n=\n \nMOD\n(\nlo\n(\n1\n)\n \n+\n \nj\n \n+\n \nk\n \n+\n \nredblack\n,\n2\n)\n\n     \ndo \ni\n \n=\n \nlo\n(\n1\n)\n \n+\n \nioff\n,\nhi\n(\n1\n),\n2\n\n        \ngamma\n \n=\n \nalpha\n*\na\n(\ni\n,\nj\n,\nk\n)\n \n\n              \n+\n   \ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n+\nbX\n(\ni\n+\n1\n,\nj\n,\nk\n))\n \n\n              \n+\n   \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n+\nbY\n(\ni\n,\nj\n+\n1\n,\nk\n))\n \n\n              \n+\n   \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n+\nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n))\n\n\n        \ng_m_d\n \n=\n \ngamma\n \n\n              \n-\n \n(\ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n*\ncf0\n \n+\n \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\ncf3\n)\n \n\n              \n+\n  \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n*\ncf1\n \n+\n \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\ncf4\n)\n \n\n              \n+\n  \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n*\ncf2\n \n+\n \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\ncf5\n))\n \n\n\n        \nrho\n \n=\n \ndhx\n*\n(\n \nbX\n(\ni\n  \n,\nj\n,\nk\n)\n*\nphi\n(\ni\n-\n1\n,\nj\n,\nk\n)\n \n\n            \n+\n       \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\nphi\n(\ni\n+\n1\n,\nj\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhy\n*\n(\n \nbY\n(\ni\n,\nj\n  \n,\nk\n)\n*\nphi\n(\ni\n,\nj\n-\n1\n,\nk\n)\n \n\n            \n+\n       \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\nphi\n(\ni\n,\nj\n+\n1\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhz\n*\n(\n \nbZ\n(\ni\n,\nj\n,\nk\n  \n)\n*\nphi\n(\ni\n,\nj\n,\nk\n-\n1\n)\n \n\n            \n+\n       \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\nphi\n(\ni\n,\nj\n,\nk\n+\n1\n)\n \n)\n \n\n\n        \nres\n \n=\n  \nrhs\n(\ni\n,\nj\n,\nk\n)\n \n-\n \n(\ngamma\n*\nphi\n(\ni\n,\nj\n,\nk\n)\n \n-\n \nrho\n)\n\n        \nphi\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphi\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nomega\n/\ng_m_d\n \n*\n \nres\n\n     \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nThe algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.\n\n\nThe GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.\n\n\nThe relaxation step and the coarse grid solve (discussed below) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.\n\n\nRestriction\n\n\nDuring a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n      \nc\n(\ni\n,\nj\n,\nk\n)\n \n=\n  \n(\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2p1\n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2p1\n)\n\n\n$\n                 \n)\n*\neighth\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nwhere \nf\n is the field on the fine grid and \nc\n is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point\n\nc(i,j,k)\n also contribute to the point \nc(i+1,j,k)\n.\n\n\nProlongation\n\n\nProlongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows:\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\nIn 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)\n\n\nExact linear solve\n\n\nThe multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often \n\\(2^3\\)\n if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is \nBiCGSTAB\n, a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#geometric-multigrid", 
            "text": "Many problems encountered in BoxLib applications require solutions to linear\nsystem, e.g.,  elliptic partial differential equations  such as the  Poisson\nequation  for self-gravity, and the  diffusion equation . BoxLib therefore\nincludes  geometric multigrid solvers  for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.  Geometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:   relaxation  restriction  prolongation  coarse-grid linear solve (either approximate or exact)   Although here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms. Although these steps are algorithmically unique, we note that all\nof them feature low arithmetic intensity and are thus sensitive to cache and\nmemory bandwidth.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#relaxation", 
            "text": "A relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original  Gauss-Seidel method  by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).  do  k   =   lo ( 3 ),   hi ( 3 ) \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n      ioff   =   MOD ( lo ( 1 )   +   j   +   k   +   redblack , 2 ) \n      do  i   =   lo ( 1 )   +   ioff , hi ( 1 ), 2 \n         gamma   =   alpha * a ( i , j , k )   \n               +     dhx * ( bX ( i , j , k ) + bX ( i + 1 , j , k ))   \n               +     dhy * ( bY ( i , j , k ) + bY ( i , j + 1 , k ))   \n               +     dhz * ( bZ ( i , j , k ) + bZ ( i , j , k + 1 )) \n\n         g_m_d   =   gamma   \n               -   ( dhx * ( bX ( i , j , k ) * cf0   +   bX ( i + 1 , j , k ) * cf3 )   \n               +    dhy * ( bY ( i , j , k ) * cf1   +   bY ( i , j + 1 , k ) * cf4 )   \n               +    dhz * ( bZ ( i , j , k ) * cf2   +   bZ ( i , j , k + 1 ) * cf5 ))   \n\n         rho   =   dhx * (   bX ( i    , j , k ) * phi ( i - 1 , j , k )   \n             +         bX ( i + 1 , j , k ) * phi ( i + 1 , j , k )   )   \n             +   dhy * (   bY ( i , j    , k ) * phi ( i , j - 1 , k )   \n             +         bY ( i , j + 1 , k ) * phi ( i , j + 1 , k )   )   \n             +   dhz * (   bZ ( i , j , k    ) * phi ( i , j , k - 1 )   \n             +         bZ ( i , j , k + 1 ) * phi ( i , j , k + 1 )   )   \n\n         res   =    rhs ( i , j , k )   -   ( gamma * phi ( i , j , k )   -   rho ) \n         phi ( i , j , k )   =   phi ( i , j , k )   +   omega / g_m_d   *   res \n      end do    end do  end do   The algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.  The GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.  The relaxation step and the coarse grid solve (discussed below) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.", 
            "title": "Relaxation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#restriction", 
            "text": "During a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:  do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n       c ( i , j , k )   =    (  $                   +   f ( i2p1 , j2p1 , k2    )   +   f ( i2 , j2p1 , k2    )  $                   +   f ( i2p1 , j2    , k2    )   +   f ( i2 , j2    , k2    )  $                   +   f ( i2p1 , j2p1 , k2p1 )   +   f ( i2 , j2p1 , k2p1 )  $                   +   f ( i2p1 , j2    , k2p1 )   +   f ( i2 , j2    , k2p1 )  $                   ) * eighth \n     end do    end do  end do   where  f  is the field on the fine grid and  c  is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point c(i,j,k)  also contribute to the point  c(i+1,j,k) .", 
            "title": "Restriction"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#prolongation", 
            "text": "Prolongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows: do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n\n       f ( i2p1 , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2    ) \n       f ( i2    , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2    ) \n       f ( i2p1 , j2    , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2    ) \n       f ( i2    , j2    , k2    )   =   c ( i , j , k )   +   f ( i2    , j2    , k2    ) \n       f ( i2p1 , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2p1 ) \n       f ( i2    , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2p1 ) \n       f ( i2p1 , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2p1 ) \n       f ( i2    , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2    , k2p1 ) \n\n     end do    end do  end do   In 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)", 
            "title": "Prolongation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#exact-linear-solve", 
            "text": "The multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often  \\(2^3\\)  if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is  BiCGSTAB , a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Exact linear solve"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/", 
            "text": "Kokkos Implementation\n\n\nThis section is written as some kind of lab report, since we think that it illustrates best what challenges users might face when making complicated frameworks such as BoxLib performance portable.\n\n\nFirst Attempt\n\n\nThis section will describe our first, unfinished attempt, to port BoxLib over to Kokkos. The approach was designed to be the cleanest but also the most difficult one. We learned some lessons in the process which we want to share with the reader of this case study.\n\n\nMemory Management\n\n\nIn theory, one would like to make the data resident on the \ndevice\n all the time to avoid unnecessary data transfer. We thus implemented a \nKArenaND\n class which allows us to use Kokkos views instead of plain arrays for storing the data. This is the abstract, N-dimensional class which provides a part of the interface:\n\n\n//generic ND template\n\n\ntemplate\n \ntypename\n \nT\n,\n \nint\n \nD\n\n\nclass\n \nKArenaND\n \n{\n\n\npublic\n:\n\n  \n//\n\n  \n// Allocates a dynamic memory arena of size sz.\n\n  \n// Returns a pointer to this memory.\n\n  \n//\n\n  \nvirtual\n \nvoid\n*\n \nalloc\n \n(\nconst\n \nstd\n::\nvector\nstd\n::\nsize_t\n \n_sz\n)\n \n=\n \n0\n;\n\n\nprotected\n:\n\n  \nvirtual\n \nvoid\n \nfree\n()\n \n=\n \n0\n;\n\n\n};\n\n\n\n\n\nWe then (partially) specialize this class in order generate a \nKArena3D\n-variant, as this is what we mostly use in our code. This class is defined as\n\n\ntemplate\n \ntypename\n \nT\n\n\nclass\n \nKArenaND\nT\n,\n3\n \n{\n\n\npublic\n:\n\n  \n//\n\n  \n// Allocates a dynamic memory arena of size sz.\n\n  \n// Returns a pointer to this memory.\n\n  \n//\n\n  \nvoid\n*\n \nalloc\n \n(\nconst\n \nstd\n::\nvector\nsize_t\n \nsz_vec\n);\n\n\n  \n// pass access operator through to simplify things\n\n  \nT\n \noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n);\n\n  \nconst\n \nT\n \noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n)\n \nconst\n;\n\n  \nT\n \noperator\n()(\nconst\n \nIntVect\n \na\n);\n\n  \nconst\n \nT\n \noperator\n()(\nconst\n \nIntVect\n \na\n)\n \nconst\n;\n\n\n  \n// and return the view\n\n  \nKokkos\n::\nView\nT\n***\n \nviewData\n(){\n \nreturn\n \nview\n;\n \n}\n\n\nprivate\n:\n\n  \nvoid\n \nfree\n();\n\n  \nKokkos\n::\nView\nT\n***\n \nview\n;\n\n\n};\n\n\n\n\n\nThe corresponding allocator looks like then:\n\n\ntemplate\ntypename\n \nT\n\n\nvoid\n*\n \nKArenaND\nT\n,\n3\n::\nalloc\n \n(\nconst\n \nstd\n::\nvector\nsize_t\n \n_sz_vec\n)\n\n\n{\n\n  \nif\n(\n_sz_vec\n.\nsize\n()\n!=\n3\n){\n\n    \nBoxLib\n::\nAbort\n(\nError, the vector size passed to KArenaND has to be equal to its dimension!\n);\n\n  \n}\n\n\n  \n// important: reverse dimensions for optimal access\n\n  \nview\n \n=\n \nKokkos\n::\nView\nT\n***\n(\nKArena_view3D\n,\n_sz_vec\n[\n2\n],\n_sz_vec\n[\n1\n],\n_sz_vec\n[\n0\n]);\n\n\n  \n// provide interface compatibility with the rest of boxlib, but never use that pointer.\n\n  \nreturn\n \nreinterpret_cast\nvoid\n*\n(\nview\n.\nptr_on_device\n());\n\n\n}\n\n\n\n\n\n\n\nWarning\n\n\nWhen packing Views into classes, avoid using pointers and the \nnew\n operator to instantiate a new View. \nThis pointer will be a host-only pointer and will have NULL value when accessed from a device which uses a different address space.\n The reference counting is only guaranteed to work properly if the View is stored and passed by value. In that sense, also avoid passing references to views. Note that Kokkos::Views are lightweight objects so there is no performance reason to use pointers/references instead of values in this case.\n\n\n\n\nWe further implemented operators to access the memory, which basically pass the View access operator to the outside. For example:\n\n\ntemplate\ntypename\n \nT\n\n\nT\n \nKArenaND\nT\n,\n3\n::\noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n)\n\n\n{\n\n  \n// indices reversed compared to BoxLib\n\n  \n// in roder to ensure interface compatibility\n\n  \nreturn\n \nview\n(\na2\n,\na1\n,\na0\n);\n\n\n}\n\n\n\n\n\nNote that we reverse the order of indices here. This is because BoxLib uses the indexing \n(x,y,z)\n whereas for Kokkos views it is more convenient if the order \n(z,y,x)\n is used so that one can use Kokkos default layouts, i.e. \nLayout::Left\n on GPU and \nLayout::Right\n on CPU. If one would use the BoxLib indexing on the view level, this logic would need to be inverted whenever a Kokkos parallel dispatch is used. Instead, we invert it on the access operator level so that we neither need to explicitly specify an iteration policy nor break the BoxLib indexing order in the rest of the code.\n\n\nThere are advantages and disadvantages to burying the Kokkos data containers deep into the framework. The obvious advantage is that once it works, basically the majority of the framework will already be Kokkos compatible. The disadvantage is that incremental porting is not possible, it is an all-or-nothing approach.\n\n\nRewriting Fortran Kernels\n\n\nA big difficulty with porting the BoxLib GMG to Kokkos is that most of the kernels are written in Fortran. For Kokkos, we need those kernels in C++ so we have started rewriting those kernels accordingly. As it turns out, the Kokkos GMG tutorial touches many of these Fortran kernels, and so we stopped after altering almost 10K lines of code in about 50 files. Below you find the comparison between the BoxLib master branch from which we started to the current state of the Kokkos port branch.\n\n\n~/BoxLib\n git diff --stat cpp_kernels_kokkos-views \n\n Src/C_BaseLib/FArrayBox.H                            |    2 -\n Src/C_BaseLib/FabArray.H                             |    8 +-\n Src/C_BaseLib/IArrayBox.H                            |    1 -\n Src/C_BaseLib/KArena.H                               |  265 ---------\n Src/C_BaseLib/KBaseFab.H                             | 3615 -----------------------------------------------------------------------------------------------------------------\n Src/C_BaseLib/Looping.H                              |  770 +-----------------------\n Src/C_BaseLib/Make.package                           |    6 +-\n Src/C_BaseLib/MultiFabUtil.cpp                       |  284 +++++----\n Src/C_BaseLib/MultiFabUtil_3d.cpp                    |  247 --------\n Src/C_BaseLib/MultiFabUtil_F.H                       |    8 -\n Src/C_BoundaryLib/Mask.H                             |    1 -\n Src/C_BoundaryLib/Mask.cpp                           |    2 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.H           |    9 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.cpp         | 1119 ++++++++++++++++-------------------\n Src/LinearSolvers/C_CellMG/ABec_3D.F                 |    4 +-\n Src/LinearSolvers/C_CellMG/CGSolver.H                |    6 +-\n Src/LinearSolvers/C_CellMG/CGSolver.cpp              |   13 +-\n Src/LinearSolvers/C_CellMG/LO_3D_cpp.cpp             |  235 --------\n Src/LinearSolvers/C_CellMG/LO_F.H                    |    5 -\n Src/LinearSolvers/C_CellMG/Laplacian.H               |    3 +-\n Src/LinearSolvers/C_CellMG/Laplacian.cpp             |  343 ++++++-----\n Src/LinearSolvers/C_CellMG/LinOp.H                   |   12 +-\n Src/LinearSolvers/C_CellMG/LinOp.cpp                 |   85 +--\n Src/LinearSolvers/C_CellMG/MG_3D_cpp.cpp             |  464 ---------------\n Src/LinearSolvers/C_CellMG/MG_3D_fortran.F           |   96 ---\n Src/LinearSolvers/C_CellMG/MG_3D_old.cpp             |  222 -------\n Src/LinearSolvers/C_CellMG/MG_F.H                    |   81 ---\n Src/LinearSolvers/C_CellMG/Make.package              |    6 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.H               |    4 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.cpp             | 1463 +++++++++++++++++++++++-----------------------\n Src/LinearSolvers/C_CellMG/old/MG_3D_cpp.cpp-average |   39 --\n Src/LinearSolvers/C_CellMG4/ABec2.H                  |    5 +-\n Src/LinearSolvers/C_CellMG4/ABec4.H                  |    3 +-\n Src/LinearSolvers/C_CellMG4/ABec4.cpp                |    7 +-\n Tools/C_mk/Make.rules                                |    4 +-\n Tools/Postprocessing/F_Src/GNUmakefile               |    2 +-\n Tutorials/MultiGrid_C/COEF_3D.F90                    |   14 +-\n Tutorials/MultiGrid_C/COEF_F.H                       |   10 +-\n Tutorials/MultiGrid_C/GNUmakefile                    |   28 +-\n Tutorials/MultiGrid_C/KokkosCore_config.h            |   11 -\n Tutorials/MultiGrid_C/KokkosCore_config.tmp          |   11 -\n Tutorials/MultiGrid_C/MG_helpers_cpp.cpp             |  162 ------\n Tutorials/MultiGrid_C/Make.package                   |    2 +-\n Tutorials/MultiGrid_C/RHS_3D.F90                     |  143 ++---\n Tutorials/MultiGrid_C/RHS_F.H                        |    3 +-\n Tutorials/MultiGrid_C/fcompare                       |  Bin 3475616 -\n 0 bytes\n Tutorials/MultiGrid_C/inputs                         |    6 +-\n Tutorials/MultiGrid_C/main.cpp                       | 1530 ++++++++++++++++++++++++------------------------\n Tutorials/MultiGrid_C/out-F                          |  522 -----------------\n Tutorials/MultiGrid_C/out-cpp                        |  522 -----------------\n 55 files changed, 2455 insertions(+), 10764 deletions(-)\n\n\n\n\nClearly, this is a major endeavor and we stopped our explorations for the moment at this point. \nFurthermore, it is not clear what performance Kokkos can deliver for the tasks at hand. To assess that, we abandoned the full port and continued with a partial port described below.\n\n\nSecond Attempt\n\n\nSince porting the full application is a major effort but we still want to assess Kokkos' potential for BoxLib, we followed a different strategy in this attempt: we will port all performance relevant GMG kernels to Kokkos, copying data into a suitable view before calling the kernel, then using Kokkos' parallel dispatcher to launch the kernels, and then fill the results back into BoxLib's own \nBaseFab\n datatype. \nIt is important to note that BoxLib uses offsets in the array-indexing and those can be different for different fields (e.g. some fields have ghost zones and some do not).\n\n\nWe thus decided to encapsulate this complexity into a new class. Below we show the declaration of the one specialized for \nFArraBox\n datatypes, i.e. \nBaseFab\n instances with types \nReal\n.\n\n\ntemplate\n\n\nclass\n \nViewFab\nReal\n \n{\n\n\npublic\n:\n\n\n  \n// swap indices here to get kokkos\n-canonical layout\n\n  \nKOKKOS_INLINE_FUNCTION\n\n  \nReal\n \noperator\n()(\nconst\n \nint\n \ni\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nn\n \n=\n \n0\n){\n\n    \nreturn\n \ndata\n(\nn\n,\n \nk\n-\nsmallend\n[\n2\n],\n \nj\n-\nsmallend\n[\n1\n],\n \ni\n-\nsmallend\n[\n0\n]);\n\n  \n}\n\n\n  \nKOKKOS_INLINE_FUNCTION\n\n  \nReal\n \noperator\n()(\nconst\n \nint\n \ni\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nn\n \n=\n \n0\n)\n \nconst\n \n{\n\n      \nreturn\n \ndata\n(\nn\n,\n \nk\n-\nsmallend\n[\n2\n],\n \nj\n-\nsmallend\n[\n1\n],\n \ni\n-\nsmallend\n[\n0\n]);\n\n  \n}\n\n\n  \nvoid\n \ninit\n(\nconst\n \nFArrayBox\n \nrhs_\n,\n \nconst\n \nstd\n::\nstring\n \nname_\n);\n\n\n  \nViewFab\n(){}\n\n\n  \nViewFab\n(\nconst\n \nFArrayBox\n \nrhs_\n,\n \nconst\n \nstd\n::\nstring\n \nname_\n){\n\n      \ninit\n(\nrhs_\n,\nname_\n);\n\n  \n}\n\n\n  \nViewFab\nReal\n \noperator\n=\n(\nconst\n \nViewFab\nReal\n \nrhs_\n);\n\n\n  \n// write the view data into a FArrayBox\n\n  \nvoid\n \nfill\n(\nFArrayBox\n \nlhs_\n)\n \nconst\n;\n\n\nprivate\n:\n\n  \nstd\n::\nstring\n \nname\n;\n\n  \nint\n \nnumvars\n;\n\n  \nIntVect\n \nsmallend\n,\n \nbigend\n,\n \nlength\n;\n\n  \nKokkos\n::\nView\nReal\n****\n \ndata\n;\n\n\n};\n\n\n\n\n\nThe important aspect is that the access operator hides the offset indexing and thus keeps the kernels clean.\nNote that this class is similar to what we try to use in our first attempt, but this time we do not bury it deep into the Framework but rather only use it for making the individual kernels performance portable. The access operators need to be decorated with \nKOKKOS_INLINE_FUNCTION\n macros because they will be called from the device. In order to copy relevant \nmetadata\n to the device, we use the functor approach. That means we pack all relevant parameters into a functor object and then provide an access operator to it. For example, the average (restriction) functor is\n\n\nstruct\n \nC_AVERAGE_FUNCTOR\n{\n\n\npublic\n:\n\n  \nC_AVERAGE_FUNCTOR\n(\nconst\n \nFArrayBox\n \nc_\n,\n \nconst\n \nFArrayBox\n \nf_\n)\n \n:\n \ncv\n(\nc_\n,\ncv\n),\n \nfv\n(\nf_\n,\nfv\n){\n\n    \ncv\n.\nsyncH2D\n();\n\n    \nfv\n.\nsyncH2D\n();\n\n  \n}\n\n\n  \nKOKKOS_INLINE_FUNCTION\n\n  \nvoid\n \noperator\n()(\nconst\n \nint\n \nn\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \ni\n)\n \nconst\n{\n\n    \ncv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n=\n  \n(\nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n,\n2\n*\nk\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n,\n2\n*\nk\n,\nn\n))\n*\n0.125\n;\n\n    \ncv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n+=\n \n(\nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n,\n2\n*\nk\n+\n1\n,\nn\n))\n*\n0.125\n;\n\n  \n}\n\n\n  \nvoid\n \nfill\n(\nFArrayBox\n \ncfab\n){\n\n    \ncv\n.\nsyncD2H\n();\n\n    \ncv\n.\nfill\n(\ncfab\n);\n\n  \n}\n\n\nprivate\n:\n\n  \nViewFab\nReal\n \ncv\n,\n \nfv\n;\n\n\n};\n\n\n\n\n\nIt contains the two viewfabs needed and makes sure that data is uploaded to and downloaded from the device when needed. The average-kernel then simply becomes:\n\n\nvoid\n \nC_AVERAGE\n(\n\n\nconst\n \nBox\n \nbx\n,\n\n\nconst\n \nint\n \nnc\n,\n\n\nFArrayBox\n \nc\n,\n\n\nconst\n \nFArrayBox\n \nf\n){\n\n\n  \nconst\n \nint\n \n*\nlo\n \n=\n \nbx\n.\nloVect\n();\n\n  \nconst\n \nint\n \n*\nhi\n \n=\n \nbx\n.\nhiVect\n();\n\n  \nconst\n \nint\n*\n \ncb\n \n=\n \nbx\n.\ncbVect\n();\n\n\n  \n// create functor\n\n  \nC_AVERAGE_FUNCTOR\n \ncavfunc\n(\nc\n,\nf\n);\n\n\n  \n// define policy\n\n  \ntypedef\n \nKokkos\n::\nExperimental\n::\nMDRangePolicy\nKokkos\n::\nExperimental\n::\nRank\n4\n \n \nt_policy\n;\n\n\n  \n// execute\n\n  \nKokkos\n::\nExperimental\n::\nmd_parallel_for\n(\nt_policy\n({\n0\n,\n \nlo\n[\n2\n],\n \nlo\n[\n1\n],\n \nlo\n[\n0\n]},{\nnc\n,\n \nhi\n[\n2\n]\n+\n1\n,\n \nhi\n[\n1\n]\n+\n1\n,\n \nhi\n[\n0\n]\n+\n1\n},{\nnc\n,\n \ncb\n[\n2\n],\n \ncb\n[\n1\n],\n \ncb\n[\n0\n]}),\ncavfunc\n);\n\n\n  \n// write back\n\n  \ncavfunc\n.\nfill\n(\nc\n);\n\n\n}\n\n\n\n\n\nIn order to employ loop-collapsing and additional cache blocking, we use the experimental multi-dimensional iteration policy feature. We added a cache-block-sizes vector \ncb\n which can be specified in the input file passed to the application. Most kernels can be ported like the one in the above example. The GSRB kernel though has non-unit stride access in the \ni\n loop, because of the red-black iteration pattern. For this case, we pass half of the actual range to the iteration policy and expand the index inside the loop. The access operator of the corresponding functor becomes\n\n\nstruct\n \nC_GSRB_FUNCTOR\n{\n\n\npublic\n:\n\n  \n...\n\n\n  \nKOKKOS_INLINE_FUNCTION\n\n  \nvoid\n \noperator\n()(\nconst\n \nint\n \nn\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \nii\n)\n \nconst\n{\n\n\n    \nint\n \nioff\n \n=\n \n(\nlo0\n \n+\n \nj\n \n+\n \nk\n \n+\n \nrb\n)\n \n%\n \n2\n;\n\n    \nint\n \ni\n \n=\n \n2\n \n*\n \n(\nii\n-\nlo0\n)\n \n+\n \nlo0\n \n+\n \nioff\n;\n\n\n    \n//be careful to not run over\n\n    \nif\n(\ni\n=\nhi0\n){\n\n\n      \n// boundary condition terms\n\n      \nReal\n \ncf0\n \n=\n \n(\n \n(\ni\n==\nblo\n[\n0\n])\n \n \n(\nm0v\n(\nblo\n[\n0\n]\n-\n1\n,\nj\n,\nk\n)\n0\n)\n \n?\n \nf0v\n(\nblo\n[\n0\n],\nj\n,\nk\n)\n \n:\n \n0.\n \n);\n\n      \nReal\n \ncf1\n \n=\n \n(\n \n(\nj\n==\nblo\n[\n1\n])\n \n \n(\nm1v\n(\ni\n,\nblo\n[\n1\n]\n-\n1\n,\nk\n)\n0\n)\n \n?\n \nf1v\n(\ni\n,\nblo\n[\n1\n],\nk\n)\n \n:\n \n0.\n \n);\n\n      \nReal\n \ncf2\n \n=\n \n(\n \n(\nk\n==\nblo\n[\n2\n])\n \n \n(\nm2v\n(\ni\n,\nj\n,\nblo\n[\n2\n]\n-\n1\n)\n0\n)\n \n?\n \nf2v\n(\ni\n,\nj\n,\nblo\n[\n2\n])\n \n:\n \n0.\n \n);\n\n      \nReal\n \ncf3\n \n=\n \n(\n \n(\ni\n==\nbhi\n[\n0\n])\n \n \n(\nm3v\n(\nbhi\n[\n0\n]\n+\n1\n,\nj\n,\nk\n)\n0\n)\n \n?\n \nf3v\n(\nbhi\n[\n0\n],\nj\n,\nk\n)\n \n:\n \n0.\n \n);\n\n      \nReal\n \ncf4\n \n=\n \n(\n \n(\nj\n==\nbhi\n[\n1\n])\n \n \n(\nm4v\n(\ni\n,\nbhi\n[\n1\n]\n+\n1\n,\nk\n)\n0\n)\n \n?\n \nf4v\n(\ni\n,\nbhi\n[\n1\n],\nk\n)\n \n:\n \n0.\n \n);\n\n      \nReal\n \ncf5\n \n=\n \n(\n \n(\nk\n==\nbhi\n[\n2\n])\n \n \n(\nm5v\n(\ni\n,\nj\n,\nbhi\n[\n2\n]\n+\n1\n)\n0\n)\n \n?\n \nf5v\n(\ni\n,\nj\n,\nbhi\n[\n2\n])\n \n:\n \n0.\n \n);\n\n\n      \n// assign overrelaxation constants\n\n      \ndouble\n \ngamma\n \n=\n  \nalpha\n \n*\n \nav\n(\ni\n,\nj\n,\nk\n)\n\n                    \n+\n \ndhx\n \n*\n \n(\nbXv\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nbXv\n(\ni\n+\n1\n,\nj\n,\nk\n))\n\n                    \n+\n \ndhy\n \n*\n \n(\nbYv\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nbYv\n(\ni\n,\nj\n+\n1\n,\nk\n))\n\n                    \n+\n \ndhz\n \n*\n \n(\nbZv\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nbZv\n(\ni\n,\nj\n,\nk\n+\n1\n));\n\n\n      \ndouble\n \ng_m_d\n \n=\n  \ngamma\n\n                    \n-\n \ndhx\n \n*\n \n(\nbXv\n(\ni\n,\nj\n,\nk\n)\n*\ncf0\n \n+\n \nbXv\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\ncf3\n)\n\n                    \n-\n \ndhy\n \n*\n \n(\nbYv\n(\ni\n,\nj\n,\nk\n)\n*\ncf1\n \n+\n \nbYv\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\ncf4\n)\n\n                    \n-\n \ndhz\n \n*\n \n(\nbZv\n(\ni\n,\nj\n,\nk\n)\n*\ncf2\n \n+\n \nbZv\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\ncf5\n);\n\n\n      \ndouble\n \nrho\n \n=\n  \ndhx\n \n*\n \n(\nbXv\n(\ni\n,\nj\n,\nk\n)\n*\nphiv\n(\ni\n-\n1\n,\nj\n,\nk\n,\nn\n)\n \n+\n \nbXv\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\nphiv\n(\ni\n+\n1\n,\nj\n,\nk\n,\nn\n))\n\n                  \n+\n \ndhy\n \n*\n \n(\nbYv\n(\ni\n,\nj\n,\nk\n)\n*\nphiv\n(\ni\n,\nj\n-\n1\n,\nk\n,\nn\n)\n \n+\n \nbYv\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\nphiv\n(\ni\n,\nj\n+\n1\n,\nk\n,\nn\n))\n\n                  \n+\n \ndhz\n \n*\n \n(\nbZv\n(\ni\n,\nj\n,\nk\n)\n*\nphiv\n(\ni\n,\nj\n,\nk\n-\n1\n,\nn\n)\n \n+\n \nbZv\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\nphiv\n(\ni\n,\nj\n,\nk\n+\n1\n,\nn\n));\n\n\n      \ndouble\n \nres\n \n=\n \nrhsv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n-\n \ngamma\n \n*\n \nphiv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n+\n \nrho\n;\n\n      \nphiv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n+=\n \nomega\n/\ng_m_d\n \n*\n \nres\n;\n\n    \n}\n\n  \n}\n\n\n  \n...\n\n\n};\n\n\n\n\n\nNote that we still have to avoid running over array bounds. That can occur when the grid extent in \ni\n-direction is odd. The iteration policy now becomes\n\n\n// instantiate functor\n\n\nC_GSRB_FUNCTOR\n \ncgsrbfunc\n(\nbx\n,\n \nbbx\n,\n \nrb\n,\n \nalpha\n,\n \nbeta\n,\n \nphi\n,\n \nrhs\n,\n \na\n,\n \nbX\n,\n \nbY\n,\n \nbZ\n,\n \nf0\n,\n \nm0\n,\n \nf1\n,\n \nm1\n,\n \nf2\n,\n \nm2\n,\n \nf3\n,\n \nm3\n,\n \nf4\n,\n \nm4\n,\n \nf5\n,\n \nm5\n,\n \nh\n);\n\n\n\n// compute bounds used in i-iteration\n\n\nint\n \nlength0\n \n=\n \nstd\n::\nfloor\n(\n \n(\nhi\n[\n0\n]\n-\nlo\n[\n0\n]\n+\n1\n)\n \n/\n \n2\n \n);\n\n\nint\n \nup0\n \n=\n \nlo\n[\n0\n]\n \n+\n \nlength0\n;\n\n\n\n// dispatch kernel\n\n\nKokkos\n::\nExperimental\n::\nmd_parallel_for\n(\nt_policy\n({\n0\n,\n \nlo\n[\n2\n],\n \nlo\n[\n1\n],\n \nlo\n[\n0\n]},\n \n{\nnc\n,\n \nhi\n[\n2\n]\n+\n1\n,\n \nhi\n[\n1\n]\n+\n1\n,\n \nup0\n+\n1\n},\n \n{\nnc\n,\n \ncb\n[\n2\n],\n \ncb\n[\n1\n],\n \nlength0\n}),\n \ncgsrbfunc\n);\n\n\n\n\n\nWe experimented with including the full loop over \ni\n into our iteration policy and then skipping the iteration if \n(i + j + k) % 2 != 0\n, but that decreased performance on the CPU by more than 50%. In general, it would be good if Kokkos' range policies allow for strided iterations.\n\n\nThe above approach clearly comes with data transfer overhead which we would have avoided if we would have followed through our first attempt. However, in the performance timings we will report later on, we explicitly exclude that overhead and just assess the run time of the kernels themselves. That way, we can determine if Kokkos is a viable framework for ensuring performance portability of BoxLib across architectures.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#kokkos-implementation", 
            "text": "This section is written as some kind of lab report, since we think that it illustrates best what challenges users might face when making complicated frameworks such as BoxLib performance portable.", 
            "title": "Kokkos Implementation"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#first-attempt", 
            "text": "This section will describe our first, unfinished attempt, to port BoxLib over to Kokkos. The approach was designed to be the cleanest but also the most difficult one. We learned some lessons in the process which we want to share with the reader of this case study.", 
            "title": "First Attempt"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#memory-management", 
            "text": "In theory, one would like to make the data resident on the  device  all the time to avoid unnecessary data transfer. We thus implemented a  KArenaND  class which allows us to use Kokkos views instead of plain arrays for storing the data. This is the abstract, N-dimensional class which provides a part of the interface:  //generic ND template  template   typename   T ,   int   D  class   KArenaND   {  public : \n   // \n   // Allocates a dynamic memory arena of size sz. \n   // Returns a pointer to this memory. \n   // \n   virtual   void *   alloc   ( const   std :: vector std :: size_t   _sz )   =   0 ;  protected : \n   virtual   void   free ()   =   0 ;  };   We then (partially) specialize this class in order generate a  KArena3D -variant, as this is what we mostly use in our code. This class is defined as  template   typename   T  class   KArenaND T , 3   {  public : \n   // \n   // Allocates a dynamic memory arena of size sz. \n   // Returns a pointer to this memory. \n   // \n   void *   alloc   ( const   std :: vector size_t   sz_vec ); \n\n   // pass access operator through to simplify things \n   T   operator ()( int   a0 ,   int   a1 ,   int   a2 ); \n   const   T   operator ()( int   a0 ,   int   a1 ,   int   a2 )   const ; \n   T   operator ()( const   IntVect   a ); \n   const   T   operator ()( const   IntVect   a )   const ; \n\n   // and return the view \n   Kokkos :: View T ***   viewData (){   return   view ;   }  private : \n   void   free (); \n   Kokkos :: View T ***   view ;  };   The corresponding allocator looks like then:  template typename   T  void *   KArenaND T , 3 :: alloc   ( const   std :: vector size_t   _sz_vec )  { \n   if ( _sz_vec . size () != 3 ){ \n     BoxLib :: Abort ( Error, the vector size passed to KArenaND has to be equal to its dimension! ); \n   } \n\n   // important: reverse dimensions for optimal access \n   view   =   Kokkos :: View T *** ( KArena_view3D , _sz_vec [ 2 ], _sz_vec [ 1 ], _sz_vec [ 0 ]); \n\n   // provide interface compatibility with the rest of boxlib, but never use that pointer. \n   return   reinterpret_cast void * ( view . ptr_on_device ());  }    Warning  When packing Views into classes, avoid using pointers and the  new  operator to instantiate a new View.  This pointer will be a host-only pointer and will have NULL value when accessed from a device which uses a different address space.  The reference counting is only guaranteed to work properly if the View is stored and passed by value. In that sense, also avoid passing references to views. Note that Kokkos::Views are lightweight objects so there is no performance reason to use pointers/references instead of values in this case.   We further implemented operators to access the memory, which basically pass the View access operator to the outside. For example:  template typename   T  T   KArenaND T , 3 :: operator ()( int   a0 ,   int   a1 ,   int   a2 )  { \n   // indices reversed compared to BoxLib \n   // in roder to ensure interface compatibility \n   return   view ( a2 , a1 , a0 );  }   Note that we reverse the order of indices here. This is because BoxLib uses the indexing  (x,y,z)  whereas for Kokkos views it is more convenient if the order  (z,y,x)  is used so that one can use Kokkos default layouts, i.e.  Layout::Left  on GPU and  Layout::Right  on CPU. If one would use the BoxLib indexing on the view level, this logic would need to be inverted whenever a Kokkos parallel dispatch is used. Instead, we invert it on the access operator level so that we neither need to explicitly specify an iteration policy nor break the BoxLib indexing order in the rest of the code.  There are advantages and disadvantages to burying the Kokkos data containers deep into the framework. The obvious advantage is that once it works, basically the majority of the framework will already be Kokkos compatible. The disadvantage is that incremental porting is not possible, it is an all-or-nothing approach.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#rewriting-fortran-kernels", 
            "text": "A big difficulty with porting the BoxLib GMG to Kokkos is that most of the kernels are written in Fortran. For Kokkos, we need those kernels in C++ so we have started rewriting those kernels accordingly. As it turns out, the Kokkos GMG tutorial touches many of these Fortran kernels, and so we stopped after altering almost 10K lines of code in about 50 files. Below you find the comparison between the BoxLib master branch from which we started to the current state of the Kokkos port branch.  ~/BoxLib  git diff --stat cpp_kernels_kokkos-views \n\n Src/C_BaseLib/FArrayBox.H                            |    2 -\n Src/C_BaseLib/FabArray.H                             |    8 +-\n Src/C_BaseLib/IArrayBox.H                            |    1 -\n Src/C_BaseLib/KArena.H                               |  265 ---------\n Src/C_BaseLib/KBaseFab.H                             | 3615 -----------------------------------------------------------------------------------------------------------------\n Src/C_BaseLib/Looping.H                              |  770 +-----------------------\n Src/C_BaseLib/Make.package                           |    6 +-\n Src/C_BaseLib/MultiFabUtil.cpp                       |  284 +++++----\n Src/C_BaseLib/MultiFabUtil_3d.cpp                    |  247 --------\n Src/C_BaseLib/MultiFabUtil_F.H                       |    8 -\n Src/C_BoundaryLib/Mask.H                             |    1 -\n Src/C_BoundaryLib/Mask.cpp                           |    2 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.H           |    9 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.cpp         | 1119 ++++++++++++++++-------------------\n Src/LinearSolvers/C_CellMG/ABec_3D.F                 |    4 +-\n Src/LinearSolvers/C_CellMG/CGSolver.H                |    6 +-\n Src/LinearSolvers/C_CellMG/CGSolver.cpp              |   13 +-\n Src/LinearSolvers/C_CellMG/LO_3D_cpp.cpp             |  235 --------\n Src/LinearSolvers/C_CellMG/LO_F.H                    |    5 -\n Src/LinearSolvers/C_CellMG/Laplacian.H               |    3 +-\n Src/LinearSolvers/C_CellMG/Laplacian.cpp             |  343 ++++++-----\n Src/LinearSolvers/C_CellMG/LinOp.H                   |   12 +-\n Src/LinearSolvers/C_CellMG/LinOp.cpp                 |   85 +--\n Src/LinearSolvers/C_CellMG/MG_3D_cpp.cpp             |  464 ---------------\n Src/LinearSolvers/C_CellMG/MG_3D_fortran.F           |   96 ---\n Src/LinearSolvers/C_CellMG/MG_3D_old.cpp             |  222 -------\n Src/LinearSolvers/C_CellMG/MG_F.H                    |   81 ---\n Src/LinearSolvers/C_CellMG/Make.package              |    6 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.H               |    4 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.cpp             | 1463 +++++++++++++++++++++++-----------------------\n Src/LinearSolvers/C_CellMG/old/MG_3D_cpp.cpp-average |   39 --\n Src/LinearSolvers/C_CellMG4/ABec2.H                  |    5 +-\n Src/LinearSolvers/C_CellMG4/ABec4.H                  |    3 +-\n Src/LinearSolvers/C_CellMG4/ABec4.cpp                |    7 +-\n Tools/C_mk/Make.rules                                |    4 +-\n Tools/Postprocessing/F_Src/GNUmakefile               |    2 +-\n Tutorials/MultiGrid_C/COEF_3D.F90                    |   14 +-\n Tutorials/MultiGrid_C/COEF_F.H                       |   10 +-\n Tutorials/MultiGrid_C/GNUmakefile                    |   28 +-\n Tutorials/MultiGrid_C/KokkosCore_config.h            |   11 -\n Tutorials/MultiGrid_C/KokkosCore_config.tmp          |   11 -\n Tutorials/MultiGrid_C/MG_helpers_cpp.cpp             |  162 ------\n Tutorials/MultiGrid_C/Make.package                   |    2 +-\n Tutorials/MultiGrid_C/RHS_3D.F90                     |  143 ++---\n Tutorials/MultiGrid_C/RHS_F.H                        |    3 +-\n Tutorials/MultiGrid_C/fcompare                       |  Bin 3475616 -  0 bytes\n Tutorials/MultiGrid_C/inputs                         |    6 +-\n Tutorials/MultiGrid_C/main.cpp                       | 1530 ++++++++++++++++++++++++------------------------\n Tutorials/MultiGrid_C/out-F                          |  522 -----------------\n Tutorials/MultiGrid_C/out-cpp                        |  522 -----------------\n 55 files changed, 2455 insertions(+), 10764 deletions(-)  Clearly, this is a major endeavor and we stopped our explorations for the moment at this point. \nFurthermore, it is not clear what performance Kokkos can deliver for the tasks at hand. To assess that, we abandoned the full port and continued with a partial port described below.", 
            "title": "Rewriting Fortran Kernels"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#second-attempt", 
            "text": "Since porting the full application is a major effort but we still want to assess Kokkos' potential for BoxLib, we followed a different strategy in this attempt: we will port all performance relevant GMG kernels to Kokkos, copying data into a suitable view before calling the kernel, then using Kokkos' parallel dispatcher to launch the kernels, and then fill the results back into BoxLib's own  BaseFab  datatype. \nIt is important to note that BoxLib uses offsets in the array-indexing and those can be different for different fields (e.g. some fields have ghost zones and some do not).  We thus decided to encapsulate this complexity into a new class. Below we show the declaration of the one specialized for  FArraBox  datatypes, i.e.  BaseFab  instances with types  Real .  template  class   ViewFab Real   {  public : \n\n   // swap indices here to get kokkos -canonical layout \n   KOKKOS_INLINE_FUNCTION \n   Real   operator ()( const   int   i ,   const   int   j ,   const   int   k ,   const   int   n   =   0 ){ \n     return   data ( n ,   k - smallend [ 2 ],   j - smallend [ 1 ],   i - smallend [ 0 ]); \n   } \n\n   KOKKOS_INLINE_FUNCTION \n   Real   operator ()( const   int   i ,   const   int   j ,   const   int   k ,   const   int   n   =   0 )   const   { \n       return   data ( n ,   k - smallend [ 2 ],   j - smallend [ 1 ],   i - smallend [ 0 ]); \n   } \n\n   void   init ( const   FArrayBox   rhs_ ,   const   std :: string   name_ ); \n\n   ViewFab (){} \n\n   ViewFab ( const   FArrayBox   rhs_ ,   const   std :: string   name_ ){ \n       init ( rhs_ , name_ ); \n   } \n\n   ViewFab Real   operator = ( const   ViewFab Real   rhs_ ); \n\n   // write the view data into a FArrayBox \n   void   fill ( FArrayBox   lhs_ )   const ;  private : \n   std :: string   name ; \n   int   numvars ; \n   IntVect   smallend ,   bigend ,   length ; \n   Kokkos :: View Real ****   data ;  };   The important aspect is that the access operator hides the offset indexing and thus keeps the kernels clean.\nNote that this class is similar to what we try to use in our first attempt, but this time we do not bury it deep into the Framework but rather only use it for making the individual kernels performance portable. The access operators need to be decorated with  KOKKOS_INLINE_FUNCTION  macros because they will be called from the device. In order to copy relevant  metadata  to the device, we use the functor approach. That means we pack all relevant parameters into a functor object and then provide an access operator to it. For example, the average (restriction) functor is  struct   C_AVERAGE_FUNCTOR {  public : \n   C_AVERAGE_FUNCTOR ( const   FArrayBox   c_ ,   const   FArrayBox   f_ )   :   cv ( c_ , cv ),   fv ( f_ , fv ){ \n     cv . syncH2D (); \n     fv . syncH2D (); \n   } \n\n   KOKKOS_INLINE_FUNCTION \n   void   operator ()( const   int   n ,   const   int   k ,   const   int   j ,   const   int   i )   const { \n     cv ( i , j , k , n )   =    ( fv ( 2 * i + 1 , 2 * j + 1 , 2 * k , n )   +   fv ( 2 * i , 2 * j + 1 , 2 * k , n )   +   fv ( 2 * i + 1 , 2 * j , 2 * k , n )   +   fv ( 2 * i , 2 * j , 2 * k , n )) * 0.125 ; \n     cv ( i , j , k , n )   +=   ( fv ( 2 * i + 1 , 2 * j + 1 , 2 * k + 1 , n )   +   fv ( 2 * i , 2 * j + 1 , 2 * k + 1 , n )   +   fv ( 2 * i + 1 , 2 * j , 2 * k + 1 , n )   +   fv ( 2 * i , 2 * j , 2 * k + 1 , n )) * 0.125 ; \n   } \n\n   void   fill ( FArrayBox   cfab ){ \n     cv . syncD2H (); \n     cv . fill ( cfab ); \n   }  private : \n   ViewFab Real   cv ,   fv ;  };   It contains the two viewfabs needed and makes sure that data is uploaded to and downloaded from the device when needed. The average-kernel then simply becomes:  void   C_AVERAGE (  const   Box   bx ,  const   int   nc ,  FArrayBox   c ,  const   FArrayBox   f ){ \n\n   const   int   * lo   =   bx . loVect (); \n   const   int   * hi   =   bx . hiVect (); \n   const   int *   cb   =   bx . cbVect (); \n\n   // create functor \n   C_AVERAGE_FUNCTOR   cavfunc ( c , f ); \n\n   // define policy \n   typedef   Kokkos :: Experimental :: MDRangePolicy Kokkos :: Experimental :: Rank 4     t_policy ; \n\n   // execute \n   Kokkos :: Experimental :: md_parallel_for ( t_policy ({ 0 ,   lo [ 2 ],   lo [ 1 ],   lo [ 0 ]},{ nc ,   hi [ 2 ] + 1 ,   hi [ 1 ] + 1 ,   hi [ 0 ] + 1 },{ nc ,   cb [ 2 ],   cb [ 1 ],   cb [ 0 ]}), cavfunc ); \n\n   // write back \n   cavfunc . fill ( c );  }   In order to employ loop-collapsing and additional cache blocking, we use the experimental multi-dimensional iteration policy feature. We added a cache-block-sizes vector  cb  which can be specified in the input file passed to the application. Most kernels can be ported like the one in the above example. The GSRB kernel though has non-unit stride access in the  i  loop, because of the red-black iteration pattern. For this case, we pass half of the actual range to the iteration policy and expand the index inside the loop. The access operator of the corresponding functor becomes  struct   C_GSRB_FUNCTOR {  public : \n   ... \n\n   KOKKOS_INLINE_FUNCTION \n   void   operator ()( const   int   n ,   const   int   k ,   const   int   j ,   const   int   ii )   const { \n\n     int   ioff   =   ( lo0   +   j   +   k   +   rb )   %   2 ; \n     int   i   =   2   *   ( ii - lo0 )   +   lo0   +   ioff ; \n\n     //be careful to not run over \n     if ( i = hi0 ){ \n\n       // boundary condition terms \n       Real   cf0   =   (   ( i == blo [ 0 ])     ( m0v ( blo [ 0 ] - 1 , j , k ) 0 )   ?   f0v ( blo [ 0 ], j , k )   :   0.   ); \n       Real   cf1   =   (   ( j == blo [ 1 ])     ( m1v ( i , blo [ 1 ] - 1 , k ) 0 )   ?   f1v ( i , blo [ 1 ], k )   :   0.   ); \n       Real   cf2   =   (   ( k == blo [ 2 ])     ( m2v ( i , j , blo [ 2 ] - 1 ) 0 )   ?   f2v ( i , j , blo [ 2 ])   :   0.   ); \n       Real   cf3   =   (   ( i == bhi [ 0 ])     ( m3v ( bhi [ 0 ] + 1 , j , k ) 0 )   ?   f3v ( bhi [ 0 ], j , k )   :   0.   ); \n       Real   cf4   =   (   ( j == bhi [ 1 ])     ( m4v ( i , bhi [ 1 ] + 1 , k ) 0 )   ?   f4v ( i , bhi [ 1 ], k )   :   0.   ); \n       Real   cf5   =   (   ( k == bhi [ 2 ])     ( m5v ( i , j , bhi [ 2 ] + 1 ) 0 )   ?   f5v ( i , j , bhi [ 2 ])   :   0.   ); \n\n       // assign overrelaxation constants \n       double   gamma   =    alpha   *   av ( i , j , k ) \n                     +   dhx   *   ( bXv ( i , j , k )   +   bXv ( i + 1 , j , k )) \n                     +   dhy   *   ( bYv ( i , j , k )   +   bYv ( i , j + 1 , k )) \n                     +   dhz   *   ( bZv ( i , j , k )   +   bZv ( i , j , k + 1 )); \n\n       double   g_m_d   =    gamma \n                     -   dhx   *   ( bXv ( i , j , k ) * cf0   +   bXv ( i + 1 , j , k ) * cf3 ) \n                     -   dhy   *   ( bYv ( i , j , k ) * cf1   +   bYv ( i , j + 1 , k ) * cf4 ) \n                     -   dhz   *   ( bZv ( i , j , k ) * cf2   +   bZv ( i , j , k + 1 ) * cf5 ); \n\n       double   rho   =    dhx   *   ( bXv ( i , j , k ) * phiv ( i - 1 , j , k , n )   +   bXv ( i + 1 , j , k ) * phiv ( i + 1 , j , k , n )) \n                   +   dhy   *   ( bYv ( i , j , k ) * phiv ( i , j - 1 , k , n )   +   bYv ( i , j + 1 , k ) * phiv ( i , j + 1 , k , n )) \n                   +   dhz   *   ( bZv ( i , j , k ) * phiv ( i , j , k - 1 , n )   +   bZv ( i , j , k + 1 ) * phiv ( i , j , k + 1 , n )); \n\n       double   res   =   rhsv ( i , j , k , n )   -   gamma   *   phiv ( i , j , k , n )   +   rho ; \n       phiv ( i , j , k , n )   +=   omega / g_m_d   *   res ; \n     } \n   } \n\n   ...  };   Note that we still have to avoid running over array bounds. That can occur when the grid extent in  i -direction is odd. The iteration policy now becomes  // instantiate functor  C_GSRB_FUNCTOR   cgsrbfunc ( bx ,   bbx ,   rb ,   alpha ,   beta ,   phi ,   rhs ,   a ,   bX ,   bY ,   bZ ,   f0 ,   m0 ,   f1 ,   m1 ,   f2 ,   m2 ,   f3 ,   m3 ,   f4 ,   m4 ,   f5 ,   m5 ,   h );  // compute bounds used in i-iteration  int   length0   =   std :: floor (   ( hi [ 0 ] - lo [ 0 ] + 1 )   /   2   );  int   up0   =   lo [ 0 ]   +   length0 ;  // dispatch kernel  Kokkos :: Experimental :: md_parallel_for ( t_policy ({ 0 ,   lo [ 2 ],   lo [ 1 ],   lo [ 0 ]},   { nc ,   hi [ 2 ] + 1 ,   hi [ 1 ] + 1 ,   up0 + 1 },   { nc ,   cb [ 2 ],   cb [ 1 ],   length0 }),   cgsrbfunc );   We experimented with including the full loop over  i  into our iteration policy and then skipping the iteration if  (i + j + k) % 2 != 0 , but that decreased performance on the CPU by more than 50%. In general, it would be good if Kokkos' range policies allow for strided iterations.  The above approach clearly comes with data transfer overhead which we would have avoided if we would have followed through our first attempt. However, in the performance timings we will report later on, we explicitly exclude that overhead and just assess the run time of the kernels themselves. That way, we can determine if Kokkos is a viable framework for ensuring performance portability of BoxLib across architectures.", 
            "title": "Second Attempt"
        }, 
        {
            "location": "/case_studies/amr/openmp_implementation/", 
            "text": "Porting BoxLib to OpenMP 4.x\n\n\nSince version 4.0, OpenMP has supported accelerator devices through data\noffloading and kernel execution semantics. OpenMP presents an appealing\nopportunity to achieve performance portability, as it requires fairly\nnon-invasive code modifications through directives which are ignored as\ncomments if OpenMP is not activated during compilation. However, as we discuss\nbelow, it is currently challenging to achieve a portable implementation of any\nkernel (to say nothing of one which has high performance).\n\n\nBoxLib already contains a large amount of OpenMP in the C++ framework to\nimplement thread parallelization and loop tiling (see \nhere\n\nand \nhere\n for more details). However, these directives are\nlimited to version 3.0 and older, and consist primarily of multi-threading of\nloops, such that the Fortran kernel execution happens entirely within a\nthread-private region. This approach yields high performance on self-hosted\nsystems such as Intel Xeon and Xeon Phi, but provides no support for\narchitectures featuring a discrete accelerator such as a GPU.\n\n\nWe implemented the OpenMP \ntarget\n construct in several of the geometric\nmultigrid kernels in BoxLib in order to support kernel execution on GPUs. The\nmost minimal approach to the \ntarget\n directive is simply to decorate a loop\nwith \ntarget\n and \nmap\n to move the data back and forth between host and device\nduring execution of the loop (see, e.g.,\n\nhere\n). However, this will\noften lead to slow code execution, especially if the loop is encountered\nmultiple times, as the data must migrate back and forth between host and device\neach time the loop is executed.\n\n\nA more optimized approach is to allocate the data on the device prior to look\nexecution, such that it need not re-allocate it each time the loop executes\n(any updated values of the data will still need to be updated on the device).\nThis can be done with the \ntarget enter data\n and \ntarget exit data\n\nconstructs, introduced in OpenMP 4.5. In BoxLib, we accomplished this by\noverloading the default data container \nArena\n (see\n\nhere\n) with a new \nOMPArena\n class which\ninvokes \nomp target enter data\n as soon as the memory is allocated. This\nensures that all FABs will be resident in device memory, obviating the need to\nmigrate all data in a loop back and forth between host and device:\n\n\nvoid\n*\n\n\nOMPArena\n::\nalloc\n \n(\nstd\n::\nsize_t\n \n_sz\n)\n\n\n{\n\n  \nvoid\n*\n \npt\n=::\noperator\n \nnew\n(\n_sz\n);\n\n  \nchar\n*\n \nptr\n=\nreinterpret_cast\nchar\n*\n(\npt\n);\n\n\n#pragma omp target enter data map(alloc:ptr[0:_sz])\n\n  \nreturn\n \npt\n;\n\n\n}\n\n\n\nvoid\n\n\nOMPArena\n::\nfree\n \n(\nvoid\n*\n \npt\n)\n\n\n{\n\n  \nchar\n*\n \nptr\n=\nreinterpret_cast\nchar\n*\n(\npt\n);\n\n\n#pragma omp target exit data map(release:ptr[:0])\n\n    \n::\noperator\n \ndelete\n(\npt\n);\n\n\n}\n\n\n\n\n\nWe also modified some existing macros in BoxLib which characterize loop-level\nparallelism. In these directives we implemented the \ntarget\n construct, e.g.,:\n\n\n#define ForAllThisCPencilAdd(T,b,ns,nc,red)  \\\n\n\n{                                                                     \\\n\n\n  BL_ASSERT(contains(b));                                             \\\n\n\n  BL_ASSERT((ns) \n= 0 \n (ns) + (nc) \n= nComp());                     \\\n\n\n  const int *_th_plo = loVect();                                      \\\n\n\n  const int *_th_plen = length();                                     \\\n\n\n  const int *_b_lo = (b).loVect();                                    \\\n\n\n  IntVect b_length = (b).size();                                      \\\n\n\n  const int *_b_len = b_length.getVect();                             \\\n\n\n  const T* _th_p = dptr;                                              \\\n\n\n  const int _ns = (ns);                                               \\\n\n\n  const int _nc = (nc);                                               \\\n\n\n  T redR = (red);                                                     \\\n\n\n  _Pragma(\nomp target update to(_th_p[_ns*_th_plen[2]:(_ns+_nc)*_th_plen[2]])\n) \\\n\n\n  _Pragma(\nomp target data map(tofrom: redR) map(to: _nc, _ns, _th_plo[0:3], _th_plen[0:3], _b_len[0:3], _b_lo[0:3])\n) \\\n\n\n  _Pragma(\nomp target if(1)\n)                                         \\\n\n\n  {                                                                   \\\n\n\n  _Pragma(\nomp teams distribute parallel for collapse(3) reduction(+:redR)\n) \\\n\n\n  for(int _n = _ns; _n \n _ns+_nc; ++_n) {                             \\\n\n\n    for(int _k = 0; _k \n _b_len[2]; ++_k) {                           \\\n\n\n      for(int _j = 0; _j \n _b_len[1]; ++_j) {                         \\\n\n\n        int nR = _n; nR += 0;                                         \\\n\n\n        const int jR = _j + _b_lo[1];                                 \\\n\n\n        const int kR = _k + _b_lo[2];                                 \\\n\n\n        const T *_th_pp =  _th_p                                      \\\n\n\n                + ((_b_lo[0] - _th_plo[0])                            \\\n\n\n                   + _th_plen[0]*(                                    \\\n\n\n                       (jR - _th_plo[1])                              \\\n\n\n                       + _th_plen[1]*(                                \\\n\n\n                           (kR - _th_plo[2])                          \\\n\n\n                           + _n * _th_plen[2])));                     \\\n\n\n        for(int _i = 0; _i \n _b_len[0]; ++_i){                        \\\n\n\n          const int iR = _i + _b_lo[0];                               \\\n\n\n          const T \nthisR = _th_pp[_i];\n\n\n\n\n\nNote that we are using the \"_Pragma\" construct which allows using \n#pragma\n statements in C-macros.\n\n\nAfter this, one can add the \ntarget teams distribute parallel for\n construct to\nmany loops, moving to the device only the data which has changed since the\nprevious time the loop was executed. This can be done with the \nupdate\n\nconstruct. For example, the restriction kernel in the multigrid solver becomes:\n\n\n \nsubroutine\n \nFORT_AVERAGE\n \n(\n\n\n$\n     \nc\n,\n \nDIMS\n(\nc\n),\n\n\n$\n     \nf\n,\n \nDIMS\n(\nf\n),\n\n\n$\n     \nlo\n,\n \nhi\n,\n \nnc\n)\n\n     \nimplicit\n \nnone\n\n     \ninteger\n \nnc\n\n     \ninteger\n \nDIMDEC\n(\nc\n)\n\n     \ninteger\n \nDIMDEC\n(\nf\n)\n\n     \ninteger\n \nlo\n(\nBL_SPACEDIM\n)\n\n     \ninteger\n \nhi\n(\nBL_SPACEDIM\n)\n\n     \nREAL_T\n \nf\n(\nDIMV\n(\nf\n),\nnc\n)\n\n     \nREAL_T\n \nc\n(\nDIMV\n(\nc\n),\nnc\n)\n\n\n     \ninteger\n \ni\n,\n \ni2\n,\n \ni2p1\n,\n \nj\n,\n \nj2\n,\n \nj2p1\n,\n \nk\n,\n \nk2\n,\n \nk2p1\n,\n \nn\n\n\n     \n!\n$\nomp\n \ntarget\n \nupdate\n \nto\n(\nf\n)\n\n\n     \n!\n$\nomp\n \ntarget\n \nmap\n(\nc\n,\n \nf\n)\n \nmap\n(\nto\n:\n \nhi\n,\n \nlo\n)\n\n     \n!\n$\nomp\n \nteams\n \ndistribute\n \nparallel\n \ndo\n \nsimd\n \ncollapse\n(\n4\n)\n\n     \n!\nomp\n \nprivate\n(\nn\n,\nk\n,\nj\n,\ni\n,\n \nk2\n,\nj2\n,\ni2\n,\n \nk2p1\n,\n \nj2p1\n,\ni2p1\n)\n\n     \ndo\n \nn\n \n=\n \n1\n,\n \nnc\n\n        \ndo\n \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n           \ndo\n \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n               \ndo\n \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n                  \nk2\n \n=\n \n2\n*\nk\n\n                  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n                  \nj2\n \n=\n \n2\n*\nj\n\n                  \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n                  \ni2\n \n=\n \n2\n*\ni\n\n                  \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n                  \nc\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n=\n  \n(\n\n\n$\n                     \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n,\nn\n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2\n  \n,\nn\n)\n\n\n$\n                     \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n,\nn\n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2\n  \n,\nn\n)\n\n\n$\n                     \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n,\nn\n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2p1\n,\nn\n)\n\n\n$\n                     \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n,\nn\n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2p1\n,\nn\n)\n\n\n$\n                     \n)\n*\neighth\n\n               \nend\n \ndo\n\n           \nend\n \ndo\n\n        \nend\n \ndo\n\n     \nend\n \ndo\n\n     \n!\n$\nomp\n \nend\n \nteams\n \ndistribute\n \nparallel\n \ndo\n \nsimd\n\n     \n!\n$\nomp\n \nend\n \ntarget\n\n \nend\n\n\n\n\n\nNote that only \nf\n, which contains the fine grid data, needs to be updated on\nthe GPU before the loop begins. (This is because a few auxiliary functions\nmodify the finest grids which were not ported to the device, and so the finest\ngrid was updated on the host.) After the loop finishes, none of the data moves\noff the device, since the fine grid \nf\n and the coarse grid \nc\n are not changed\non the host before the next kernel which requires this data executes on the\ndevice. The only data which must be mapped to the device (but not mapped back)\nare the \nlo\n and \nhi\n bounds of the loop indices.\n\n\nChallenges\n\n\nWe encountered several significant barriers to achieving performance\nportability using OpenMP.\n\n\nUndefined behavior of \ntarget\n construct in absence of a device\n\n\nOpenMP 4.0 introduced the \ntarget\n construct, allowing the use to move data\namong a host and its attached devices. The traditional \nparallel\n construct\nfrom earlier versions of the OpenMP specification do not specify a mechanism\nfor executing code on a device, or how to move data to or from a device.\nTherefore, we explored the possibility of executing loops decorated with the\n\ntarget\n construct on a host, in order to compare the behavior of the code with\nthe original loops which were annotated with the \nparallel\n construct.\n\n\nUnfortunately, the OpenMP 4.5 API specification does not specify the behavior\nof code regions decorated with a \ntarget\n construct in the absence of a device.\nWe have found that this has resulted in a wide range in behavior of OpenMP\nimplementations in different compilers when executing \ntarget\n regions on the\nhost:\n\n\n\n\n\n\nGCC\n: supports \ntarget\n regions on host CPU, but exhibits significant\n  performance degradation compared to traditional \nparallel do\n construct, due\nto a bug in the way threads are cached in libgomp.\n\n\n\n\n\n\nIntel\n: by default looks for an x100-series Xeon Phi (\"Knights Corner\")\n  co-processor, and compilation will fail at link time if the KNC libraries are\nunavailable. If the libraries are available but the co-processor is not,\nhowever, it will fail at execution time because the KNC ISA is incompatible\nwith KNL and Xeon.\nFortunately, Intel does support host execution of the \ntarget\n construct via\nthe \n-qopenmp-offload=host\n compiler flag. However, the product documentation\ndoes not specify the behavior of the OpenMP run time when \ntarget\n regions\nexecute on the host.\n\n\n\n\n\n\nCray\n: supports execution of \ntarget\n construct on both CPU host and\n  device, through the \ncraype-accel-*\n modules. To compile \ntarget\n regions for\nthe host, one must load the \ncraype-accel-host\n module; for devices, one must\nload the appropriate accelerator module, e.g., \ncraype-accel-nvidia60\n for\nNVIDIA \"Pascal\" GPUs.\n\n\n\n\n\n\nCompiler bugs\n\n\nOur progress in implementing the OpenMP \ntarget\n construct has also been\nhindered by compiler problems.\n\n\n\n\n\n\nAs noted above, GCC encounters a performance regression when executing\n  \ntarget\n regions on the host CPU. This has been reported in GCC Bugzilla as\nbug \n#80859\n.\n\n\n\n\n\n\nCCE 8.6.0 and 8.6.1 (the latest available as of August 2017) encounter a\n  segmentation fault on one of the source files in BoxLib. This has been\nreported to Cray as bug #189702. CCE 8.6.1 was\n\n\n\n\n\n\nCCE 8.6.1 fails to link BoxLib at all (without \ntarget\n constructs), with g++\n  tuple errors. This has been reported to Cray as bug #189760.\n\n\n\n\n\n\nIBM XLC 13.1 fails to link multiple compilation units together when more than one include the same header file and that header file contains a \ntarget\n region. Fixed in 14.0 but that\n  triggered an internal compilation error. Reported and acknowledged as bug #147007.", 
            "title": "OpenMP 4.x"
        }, 
        {
            "location": "/case_studies/amr/openmp_implementation/#porting-boxlib-to-openmp-4x", 
            "text": "Since version 4.0, OpenMP has supported accelerator devices through data\noffloading and kernel execution semantics. OpenMP presents an appealing\nopportunity to achieve performance portability, as it requires fairly\nnon-invasive code modifications through directives which are ignored as\ncomments if OpenMP is not activated during compilation. However, as we discuss\nbelow, it is currently challenging to achieve a portable implementation of any\nkernel (to say nothing of one which has high performance).  BoxLib already contains a large amount of OpenMP in the C++ framework to\nimplement thread parallelization and loop tiling (see  here \nand  here  for more details). However, these directives are\nlimited to version 3.0 and older, and consist primarily of multi-threading of\nloops, such that the Fortran kernel execution happens entirely within a\nthread-private region. This approach yields high performance on self-hosted\nsystems such as Intel Xeon and Xeon Phi, but provides no support for\narchitectures featuring a discrete accelerator such as a GPU.  We implemented the OpenMP  target  construct in several of the geometric\nmultigrid kernels in BoxLib in order to support kernel execution on GPUs. The\nmost minimal approach to the  target  directive is simply to decorate a loop\nwith  target  and  map  to move the data back and forth between host and device\nduring execution of the loop (see, e.g., here ). However, this will\noften lead to slow code execution, especially if the loop is encountered\nmultiple times, as the data must migrate back and forth between host and device\neach time the loop is executed.  A more optimized approach is to allocate the data on the device prior to look\nexecution, such that it need not re-allocate it each time the loop executes\n(any updated values of the data will still need to be updated on the device).\nThis can be done with the  target enter data  and  target exit data \nconstructs, introduced in OpenMP 4.5. In BoxLib, we accomplished this by\noverloading the default data container  Arena  (see here ) with a new  OMPArena  class which\ninvokes  omp target enter data  as soon as the memory is allocated. This\nensures that all FABs will be resident in device memory, obviating the need to\nmigrate all data in a loop back and forth between host and device:  void *  OMPArena :: alloc   ( std :: size_t   _sz )  { \n   void *   pt =:: operator   new ( _sz ); \n   char *   ptr = reinterpret_cast char * ( pt );  #pragma omp target enter data map(alloc:ptr[0:_sz]) \n   return   pt ;  }  void  OMPArena :: free   ( void *   pt )  { \n   char *   ptr = reinterpret_cast char * ( pt );  #pragma omp target exit data map(release:ptr[:0]) \n     :: operator   delete ( pt );  }   We also modified some existing macros in BoxLib which characterize loop-level\nparallelism. In these directives we implemented the  target  construct, e.g.,:  #define ForAllThisCPencilAdd(T,b,ns,nc,red)  \\  {                                                                     \\    BL_ASSERT(contains(b));                                             \\    BL_ASSERT((ns)  = 0   (ns) + (nc)  = nComp());                     \\    const int *_th_plo = loVect();                                      \\    const int *_th_plen = length();                                     \\    const int *_b_lo = (b).loVect();                                    \\    IntVect b_length = (b).size();                                      \\    const int *_b_len = b_length.getVect();                             \\    const T* _th_p = dptr;                                              \\    const int _ns = (ns);                                               \\    const int _nc = (nc);                                               \\    T redR = (red);                                                     \\    _Pragma( omp target update to(_th_p[_ns*_th_plen[2]:(_ns+_nc)*_th_plen[2]]) ) \\    _Pragma( omp target data map(tofrom: redR) map(to: _nc, _ns, _th_plo[0:3], _th_plen[0:3], _b_len[0:3], _b_lo[0:3]) ) \\    _Pragma( omp target if(1) )                                         \\    {                                                                   \\    _Pragma( omp teams distribute parallel for collapse(3) reduction(+:redR) ) \\    for(int _n = _ns; _n   _ns+_nc; ++_n) {                             \\      for(int _k = 0; _k   _b_len[2]; ++_k) {                           \\        for(int _j = 0; _j   _b_len[1]; ++_j) {                         \\          int nR = _n; nR += 0;                                         \\          const int jR = _j + _b_lo[1];                                 \\          const int kR = _k + _b_lo[2];                                 \\          const T *_th_pp =  _th_p                                      \\                  + ((_b_lo[0] - _th_plo[0])                            \\                     + _th_plen[0]*(                                    \\                         (jR - _th_plo[1])                              \\                         + _th_plen[1]*(                                \\                             (kR - _th_plo[2])                          \\                             + _n * _th_plen[2])));                     \\          for(int _i = 0; _i   _b_len[0]; ++_i){                        \\            const int iR = _i + _b_lo[0];                               \\            const T  thisR = _th_pp[_i];   Note that we are using the \"_Pragma\" construct which allows using  #pragma  statements in C-macros.  After this, one can add the  target teams distribute parallel for  construct to\nmany loops, moving to the device only the data which has changed since the\nprevious time the loop was executed. This can be done with the  update \nconstruct. For example, the restriction kernel in the multigrid solver becomes:    subroutine   FORT_AVERAGE   (  $       c ,   DIMS ( c ),  $       f ,   DIMS ( f ),  $       lo ,   hi ,   nc ) \n      implicit   none \n      integer   nc \n      integer   DIMDEC ( c ) \n      integer   DIMDEC ( f ) \n      integer   lo ( BL_SPACEDIM ) \n      integer   hi ( BL_SPACEDIM ) \n      REAL_T   f ( DIMV ( f ), nc ) \n      REAL_T   c ( DIMV ( c ), nc ) \n\n      integer   i ,   i2 ,   i2p1 ,   j ,   j2 ,   j2p1 ,   k ,   k2 ,   k2p1 ,   n \n\n      ! $ omp   target   update   to ( f ) \n\n      ! $ omp   target   map ( c ,   f )   map ( to :   hi ,   lo ) \n      ! $ omp   teams   distribute   parallel   do   simd   collapse ( 4 ) \n      ! omp   private ( n , k , j , i ,   k2 , j2 , i2 ,   k2p1 ,   j2p1 , i2p1 ) \n      do   n   =   1 ,   nc \n         do   k   =   lo ( 3 ),   hi ( 3 ) \n            do   j   =   lo ( 2 ),   hi ( 2 ) \n                do   i   =   lo ( 1 ),   hi ( 1 ) \n                   k2   =   2 * k \n                   k2p1   =   k2   +   1 \n                   j2   =   2 * j \n                   j2p1   =   j2   +   1 \n                   i2   =   2 * i \n                   i2p1   =   i2   +   1 \n                   c ( i , j , k , n )   =    (  $                       +   f ( i2p1 , j2p1 , k2    , n )   +   f ( i2 , j2p1 , k2    , n )  $                       +   f ( i2p1 , j2    , k2    , n )   +   f ( i2 , j2    , k2    , n )  $                       +   f ( i2p1 , j2p1 , k2p1 , n )   +   f ( i2 , j2p1 , k2p1 , n )  $                       +   f ( i2p1 , j2    , k2p1 , n )   +   f ( i2 , j2    , k2p1 , n )  $                       ) * eighth \n                end   do \n            end   do \n         end   do \n      end   do \n      ! $ omp   end   teams   distribute   parallel   do   simd \n      ! $ omp   end   target \n  end   Note that only  f , which contains the fine grid data, needs to be updated on\nthe GPU before the loop begins. (This is because a few auxiliary functions\nmodify the finest grids which were not ported to the device, and so the finest\ngrid was updated on the host.) After the loop finishes, none of the data moves\noff the device, since the fine grid  f  and the coarse grid  c  are not changed\non the host before the next kernel which requires this data executes on the\ndevice. The only data which must be mapped to the device (but not mapped back)\nare the  lo  and  hi  bounds of the loop indices.", 
            "title": "Porting BoxLib to OpenMP 4.x"
        }, 
        {
            "location": "/case_studies/amr/openmp_implementation/#challenges", 
            "text": "We encountered several significant barriers to achieving performance\nportability using OpenMP.", 
            "title": "Challenges"
        }, 
        {
            "location": "/case_studies/amr/openmp_implementation/#undefined-behavior-of-target-construct-in-absence-of-a-device", 
            "text": "OpenMP 4.0 introduced the  target  construct, allowing the use to move data\namong a host and its attached devices. The traditional  parallel  construct\nfrom earlier versions of the OpenMP specification do not specify a mechanism\nfor executing code on a device, or how to move data to or from a device.\nTherefore, we explored the possibility of executing loops decorated with the target  construct on a host, in order to compare the behavior of the code with\nthe original loops which were annotated with the  parallel  construct.  Unfortunately, the OpenMP 4.5 API specification does not specify the behavior\nof code regions decorated with a  target  construct in the absence of a device.\nWe have found that this has resulted in a wide range in behavior of OpenMP\nimplementations in different compilers when executing  target  regions on the\nhost:    GCC : supports  target  regions on host CPU, but exhibits significant\n  performance degradation compared to traditional  parallel do  construct, due\nto a bug in the way threads are cached in libgomp.    Intel : by default looks for an x100-series Xeon Phi (\"Knights Corner\")\n  co-processor, and compilation will fail at link time if the KNC libraries are\nunavailable. If the libraries are available but the co-processor is not,\nhowever, it will fail at execution time because the KNC ISA is incompatible\nwith KNL and Xeon.\nFortunately, Intel does support host execution of the  target  construct via\nthe  -qopenmp-offload=host  compiler flag. However, the product documentation\ndoes not specify the behavior of the OpenMP run time when  target  regions\nexecute on the host.    Cray : supports execution of  target  construct on both CPU host and\n  device, through the  craype-accel-*  modules. To compile  target  regions for\nthe host, one must load the  craype-accel-host  module; for devices, one must\nload the appropriate accelerator module, e.g.,  craype-accel-nvidia60  for\nNVIDIA \"Pascal\" GPUs.", 
            "title": "Undefined behavior of target construct in absence of a device"
        }, 
        {
            "location": "/case_studies/amr/openmp_implementation/#compiler-bugs", 
            "text": "Our progress in implementing the OpenMP  target  construct has also been\nhindered by compiler problems.    As noted above, GCC encounters a performance regression when executing\n   target  regions on the host CPU. This has been reported in GCC Bugzilla as\nbug  #80859 .    CCE 8.6.0 and 8.6.1 (the latest available as of August 2017) encounter a\n  segmentation fault on one of the source files in BoxLib. This has been\nreported to Cray as bug #189702. CCE 8.6.1 was    CCE 8.6.1 fails to link BoxLib at all (without  target  constructs), with g++\n  tuple errors. This has been reported to Cray as bug #189760.    IBM XLC 13.1 fails to link multiple compilation units together when more than one include the same header file and that header file contains a  target  region. Fixed in 14.0 but that\n  triggered an internal compilation error. Reported and acknowledged as bug #147007.", 
            "title": "Compiler bugs"
        }, 
        {
            "location": "/case_studies/gw/", 
            "text": "Overview of BerkeleyGW Case Study\n\n\nDescription\n\n\nBerkeleyGW is a material science application that predicts the excited-state properties of a wide range of materials from molecules and nanostuctures to\ncrystals - including systems with defects and complex interfaces. The excited-state properties of materials (properties associated with electrons in states\nabove the lowest energy configuration) are important for a number of important energy applications including the material design of batteries,\nsemiconductors, quantum computing devices, photovoltaics and emitting devices among others. The BerkeleyGW application is commonly used in\nconjunction with Density Functional Theory (DFT) applications like Quantum ESPRESSO, PARATEC, ABINIT which compute accurately the ground-state properties of\nmaterials. In BerkeleyGW, the electronic energies are computed as a solution to the so-called Dyson equation:\n\n\n\\[\n\\left[ -\\frac{1}{2}\\nabla^2+V_{\\rm loc}+\\Sigma(E_{n}) \\right] \\phi_{n}=E_{n}\\phi_{n},\n\\]\nwhich is similar in form to the DFT Kohn-Sham equations with the addition of the energy-dependent Self-Energy operator \n\\(\\Sigma\\)\n.\n\n\nBerkeleyGW contains many of the computational bottlenecks of DFT applications including a significant amount of time spent in FFTs and dense linear algebra.\nAdditionally, similarly to quantum chemistry packages, there are a number of tensor-contraction operations that cannot be performed in library calls. One such\noccurrence is the evaluation of the electron \"Self-Energy\" within the common General Plasmon Pole (GPP) approximation:\n\n\n\\[\n\\Sigma_{n}=\n\\sum_{n'}\\sum_{{\\bf GG}'}\nM^{*}_{n'n}(-{\\bf G})M_{n'n}(-{\\bf G}')\\frac{\\Omega^2_{{\\bf GG}'}}\n{\\tilde{\\omega}_{{\\bf GG}'}\n\\left(E\\,{-}\\,E_{n'}{-}\n\\tilde{\\omega}_{{\\bf GG}'}\\right)}\nv{\\left({\\bf G}'\\right)}\n\\]\nwhere \n\\(M\\)\n, \n\\(\\Omega\\)\n and \n\\(\\tilde{\\omega}\\)\n are pre-computed complex double-precision arrays. In this report, we focus on the evaluation of this expression (the GPP kernel), which is a\nmajor bottleneck in the application.\n\n\nImplementation\n\n\nThe fact that the denominator in the above equation depends on \n\\(n'\\)\n, \n\\(G\\)\n and \n\\(G'\\)\n means it is difficult to write the matrix-reduction using standard\nmath-libraries. The baseline code is implemented in FORTRAN-90 utilizing an MPI+OpenMP parallel approach, with care given to ensure a vectorizable inner loop. MPI parallelism is\ngenerally used to parallelize over \n\\(n\\)\n and \n\\(n'\\)\n, while OpenMP parallelizes the \n\\(G'\\)\n loop and the \n\\(G\\)\n loop is left for vectorization. Significant data re-use of\nthe arrays is possible if many values of \n\\(E\\)\n are required. At minimum, we require three \n\\(E\\)\n values; which leads to an arithmetic intensity of \n 1. An initial\nroofline plot for KNL Xeon processors are shown below.\n\n\n\n\nThe gap between the code performance and the ceiling can be explained via two factors: 1. the code lacks multiply-add symmetry and 2. the divide instruction has a\nmulti-cycle latency.\n\n\nShown below are the roofline plots of the application kernel with fortran and C++ implementations respectively.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/gw/#overview-of-berkeleygw-case-study", 
            "text": "", 
            "title": "Overview of BerkeleyGW Case Study"
        }, 
        {
            "location": "/case_studies/gw/#description", 
            "text": "BerkeleyGW is a material science application that predicts the excited-state properties of a wide range of materials from molecules and nanostuctures to\ncrystals - including systems with defects and complex interfaces. The excited-state properties of materials (properties associated with electrons in states\nabove the lowest energy configuration) are important for a number of important energy applications including the material design of batteries,\nsemiconductors, quantum computing devices, photovoltaics and emitting devices among others. The BerkeleyGW application is commonly used in\nconjunction with Density Functional Theory (DFT) applications like Quantum ESPRESSO, PARATEC, ABINIT which compute accurately the ground-state properties of\nmaterials. In BerkeleyGW, the electronic energies are computed as a solution to the so-called Dyson equation:  \\[\n\\left[ -\\frac{1}{2}\\nabla^2+V_{\\rm loc}+\\Sigma(E_{n}) \\right] \\phi_{n}=E_{n}\\phi_{n},\n\\] which is similar in form to the DFT Kohn-Sham equations with the addition of the energy-dependent Self-Energy operator  \\(\\Sigma\\) .  BerkeleyGW contains many of the computational bottlenecks of DFT applications including a significant amount of time spent in FFTs and dense linear algebra.\nAdditionally, similarly to quantum chemistry packages, there are a number of tensor-contraction operations that cannot be performed in library calls. One such\noccurrence is the evaluation of the electron \"Self-Energy\" within the common General Plasmon Pole (GPP) approximation:  \\[\n\\Sigma_{n}=\n\\sum_{n'}\\sum_{{\\bf GG}'}\nM^{*}_{n'n}(-{\\bf G})M_{n'n}(-{\\bf G}')\\frac{\\Omega^2_{{\\bf GG}'}}\n{\\tilde{\\omega}_{{\\bf GG}'}\n\\left(E\\,{-}\\,E_{n'}{-}\n\\tilde{\\omega}_{{\\bf GG}'}\\right)}\nv{\\left({\\bf G}'\\right)}\n\\] where  \\(M\\) ,  \\(\\Omega\\)  and  \\(\\tilde{\\omega}\\)  are pre-computed complex double-precision arrays. In this report, we focus on the evaluation of this expression (the GPP kernel), which is a\nmajor bottleneck in the application.", 
            "title": "Description"
        }, 
        {
            "location": "/case_studies/gw/#implementation", 
            "text": "The fact that the denominator in the above equation depends on  \\(n'\\) ,  \\(G\\)  and  \\(G'\\)  means it is difficult to write the matrix-reduction using standard\nmath-libraries. The baseline code is implemented in FORTRAN-90 utilizing an MPI+OpenMP parallel approach, with care given to ensure a vectorizable inner loop. MPI parallelism is\ngenerally used to parallelize over  \\(n\\)  and  \\(n'\\) , while OpenMP parallelizes the  \\(G'\\)  loop and the  \\(G\\)  loop is left for vectorization. Significant data re-use of\nthe arrays is possible if many values of  \\(E\\)  are required. At minimum, we require three  \\(E\\)  values; which leads to an arithmetic intensity of   1. An initial\nroofline plot for KNL Xeon processors are shown below.   The gap between the code performance and the ceiling can be explained via two factors: 1. the code lacks multiply-add symmetry and 2. the divide instruction has a\nmulti-cycle latency.  Shown below are the roofline plots of the application kernel with fortran and C++ implementations respectively.", 
            "title": "Implementation"
        }, 
        {
            "location": "/case_studies/gw/code_structure/", 
            "text": "Code structure\n\n\nOriginal code structure\n\n\nThe inner loops of the FORTRAN-90 code have the following form. \n\n\n!$OMP DO reduction(+:achtemp)\n\n\ndo \nigp\n \n=\n \n1\n,\n \nngpown\n\n  \n...\n\n\n  \ndo \niw\n=\n1\n,\n3\n \n! Original Inner Loop Bad for Vectorization\n\n\n    \n...\n\n\n    \ndo \nig\n \n=\n \n1\n,\n \nncouls\n\n\n      \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n      \n...\n\n      \nscht\n \n=\n \nscht\n \n+\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n \n\n    \nenddo\n \n! loop over g\n\n\n    \nachtemp\n(\niw\n)\n \n=\n \nachtemp\n(\niw\n)\n \n+\n \n0.5D0\n*\nscht\n*\nvcoul\n(\nigp\n)\n\n\n  \nenddo\n   \n\n\nenddo\n \n\n\n\n\nwhere, in the production code, we block the \nig\n loop around the \niw\n loop in order to gain data reuse. However, the inner 'ig' loop is left appropriately \nlong to \nget efficient vector performance - typically block sizes of around 256 are used, which is many vector lengths on a KNL processor.\n\n\nPortability considerations\n\n\nThere are essentially three hot-arrays in this code, for convenience named array1, array2, array3 corresponding to the three complex-double precision arrays \non the equation in the previous page: \n\\(M\\)\n, \n\\(\\Omega\\)\n and \n\\(\\tilde{\\omega}\\)\n. It will be important to place these in the fastest memory tier. \n\n\nThe data-structures are generally double-precision complex. This is a native FORTRAN type, but is less standard in C/C++. Performance additionally requires \na fast vectorizable-divide instruction for complex-numbers or a suitable work-around. This was an issue only the earlier generation Xeon-Phi, Knights \nCorner, for example.", 
            "title": "Code structure"
        }, 
        {
            "location": "/case_studies/gw/code_structure/#code-structure", 
            "text": "", 
            "title": "Code structure"
        }, 
        {
            "location": "/case_studies/gw/code_structure/#original-code-structure", 
            "text": "The inner loops of the FORTRAN-90 code have the following form.   !$OMP DO reduction(+:achtemp)  do  igp   =   1 ,   ngpown \n   ... \n\n   do  iw = 1 , 3   ! Original Inner Loop Bad for Vectorization \n\n     ... \n\n     do  ig   =   1 ,   ncouls \n\n       delw   =   array2 ( ig , np )   /   ( wxt ( iw , np )   -   array1 ( ig , igp )) \n       ... \n       scht   =   scht   +   ...   *   delw   *   array3 ( ig , igp )  \n\n     enddo   ! loop over g \n\n     achtemp ( iw )   =   achtemp ( iw )   +   0.5D0 * scht * vcoul ( igp ) \n\n   enddo     enddo    where, in the production code, we block the  ig  loop around the  iw  loop in order to gain data reuse. However, the inner 'ig' loop is left appropriately \nlong to \nget efficient vector performance - typically block sizes of around 256 are used, which is many vector lengths on a KNL processor.", 
            "title": "Original code structure"
        }, 
        {
            "location": "/case_studies/gw/code_structure/#portability-considerations", 
            "text": "There are essentially three hot-arrays in this code, for convenience named array1, array2, array3 corresponding to the three complex-double precision arrays \non the equation in the previous page:  \\(M\\) ,  \\(\\Omega\\)  and  \\(\\tilde{\\omega}\\) . It will be important to place these in the fastest memory tier.   The data-structures are generally double-precision complex. This is a native FORTRAN type, but is less standard in C/C++. Performance additionally requires \na fast vectorizable-divide instruction for complex-numbers or a suitable work-around. This was an issue only the earlier generation Xeon-Phi, Knights \nCorner, for example.", 
            "title": "Portability considerations"
        }, 
        {
            "location": "/case_studies/gw/kokkos_implementation/", 
            "text": "Complex Numbers in Kokkos\n\n\nKokkos has its own implementation of complex numbers.\nThe framework supports most of the operations involving complex numbers but there are a few that are not yet available.\nFor example the power of a complex number is not defined and a few others such as multiplying, adding or subtracting a double to/from a complex number are yet to be implemented.\nWe can however define our own implementation and annotate the function with \nKOKKOS_INLINE_FUNCTION\n to achieve the result.\n\n\nKOKKOS_INLINE_FUNCTION\n\n\nKokkos\n::\ncomplex\ndouble\n \nkokkos_square\n(\nKokkos\n::\ncomplex\ndouble\n \ncompl_num\n)\n\n\n{\n\n    \ndouble\n \nre\n \n=\n \nKokkos\n::\nreal\n(\ncompl_num\n);\n\n    \ndouble\n \nim\n \n=\n \nKokkos\n::\nimag\n(\ncompl_num\n);\n\n\n    \nKokkos\n::\ncomplex\ndouble\n \nresult\n(\nre\n*\nre\n \n-\n \nim\n*\nim\n,\n \n2\n*\nre\n*\nim\n);\n\n    \nreturn\n \nresult\n;\n\n\n}\n\n\n\n\n\nIn Kokkos it is advised to use a \nKokkos::View\n datatype inorder to store modify the data inside a Kokkos construct.\nWe follow the below shown technique to create a vector and matrix type of\nviews\n with the appropriate execution and memory spaces.\n\n\n#define CUDASPACE 0\n\n\n#define OPENMPSPACE 0\n\n\n#define CUDAUVM 1\n\n\n#define SERIAL 0\n\n\n#define THREADS 0\n\n\n\n#if OPENMPSPACE\n\n        \ntypedef\n \nKokkos\n::\nOpenMP\n   \nExecSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nOpenMP\n        \nMemSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nLayoutRight\n  \nLayout\n;\n\n\n#endif\n\n\n\n#if CUDASPACE\n\n        \ntypedef\n \nKokkos\n::\nCuda\n     \nExecSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nCudaSpace\n     \nMemSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nLayoutLeft\n   \nLayout\n;\n\n\n#endif\n\n\n\n#if SERIAL\n\n        \ntypedef\n \nKokkos\n::\nSerial\n   \nExecSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nHostSpace\n     \nMemSpace\n;\n\n\n#endif\n\n\n\n#if THREADS\n\n        \ntypedef\n \nKokkos\n::\nThreads\n  \nExecSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nHostSpace\n     \nMemSpace\n;\n\n\n#endif\n\n\n\n#if CUDAUVM\n\n        \ntypedef\n \nKokkos\n::\nCuda\n     \nExecSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nCudaUVMSpace\n  \nMemSpace\n;\n\n        \ntypedef\n \nKokkos\n::\nLayoutLeft\n   \nLayout\n;\n\n\n#endif\n\n\n\ntypedef\n \nKokkos\n::\nRangePolicy\nExecSpace\n  \nrange_policy\n;\n\n\n\ntypedef\n \nKokkos\n::\nView\nKokkos\n::\ncomplex\ndouble\n,\n \nLayout\n,\n \nMemSpace\n   \nViewScalarTypeComplex\n;\n\n\ntypedef\n \nKokkos\n::\nView\nKokkos\n::\ncomplex\ndouble\n*\n,\n \nLayout\n,\n \nMemSpace\n   \nViewVectorTypeComplex\n;\n\n\ntypedef\n \nKokkos\n::\nView\nKokkos\n::\ncomplex\ndouble\n**\n,\n \nLayout\n,\n \nMemSpace\n  \nViewMatrixTypeComplex\n;\n\n\n\n};\n\n\n\nViewMatrixTypeComplex\n \narray1\n(\narray1\n,\n \nN\n,\nM\n);\n\n\nViewMatrixTypeComplex\n \narray2\n(\narray2\n,\n \nN\n,\nM\n);\n\n\nViewVectorTypeComplex\n \nvcoul\n(\nvcoul\n,\n \nngpown\n)\n\n\n\nAs shown in the code structure section, there is a reduction being performed over a vector of complex numbers \nachtemp\n\nIn the OpenMP version of the code we reduce these values inside individual thread-local arrays and then later accumulate the results.\nBut since Kokkos allows user defined reduction, we create a structure with an array of 3 complex numbers and overload the += operator for this structure.\nWe then perform a \nKokkos::parallel_reduce\n on the structure.\n\n\nKokkos+OpenMP implementation\n\n\nstruct\n \nachtempStruct\n\n\n{\n\n    \nKokkos\n::\ncomplex\ndouble\n \nvalue\n[\n3\n];\n\n\nKOKKOS_INLINE_FUNCTION\n\n    \nvoid\n \noperator\n+=\n(\nachtempStruct\n \nconst\n \nother\n)\n\n    \n{\n\n        \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \n3\n;\n \n++\ni\n)\n\n            \nvalue\n[\ni\n]\n \n+=\n \nother\n.\nvalue\n[\ni\n];\n\n    \n}\n\n\nKOKKOS_INLINE_FUNCTION\n\n    \nvoid\n \noperator\n+=\n(\nachtempStruct\n \nconst\n \nvolatile\n \nother\n)\n \nvolatile\n\n    \n{\n\n        \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \n3\n;\n \n++\ni\n)\n\n            \nvalue\n[\ni\n]\n \n+=\n \nother\n.\nvalue\n[\ni\n];\n\n    \n}\n\n\n};\n\n\n\nKokkos\n::\ncomplex\ndouble\n  \nachtemp\n[\n3\n];\n\n\nachStruct\n \nachtempVar\n \n=\n \n{{\nachtemp\n[\n0\n],\nachtemp\n[\n1\n],\nachtemp\n[\n2\n]}};\n\n\n\nKokkos\n::\nparallel_reduce\n(\nngpown\n,\n \nKOKKOS_LAMBDA\n \n(\nint\n \nmy_igp\n,\n \nachtempStruct\n \nachUpdate\n)\n\n\n{\n\n    \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n    \n{\n\n        \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n        \n{\n\n            \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n            \n...\n\n            \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n        \n}\n\n        \nachUpdate\n.\nvalue\n[\niw\n]\n \n+=\n \nvcoul\n(\nigp\n)\n \n*\n \nsch_array\n[\niw\n];\n\n    \n}\n\n\n\n},\nachtempVar\n);\n\n\n\n\n\nINITIAL OBSERVATION\n\n\nIn this section, we will describe some of the initial hurdles faced while porting the gw-kernel using Kokkos.\nNot shown in the code structure is the outermost loop of the computation (shown below), which iterates over the number of bands.\nIf we update the achtemp as a view inside the Kokkos construct, we loose its value when the outer loop starts the new iteration.\nhence we have to store it in a separate structure in order to save its value for the next iteration.\n\n\nfor\n(\nint\n \nn1\n \n=\n \n0\n;\n \nn1\nnumber_bands\n;\n \n++\nn1\n)\n\n\n{\n\n    \nKokkos\n::\nparallel_reduce\n(\nngpown\n,\n \nKOKKOS_LAMBDA\n \n(\nint\n \nmy_igp\n,\n \nachtempStruct\n \nachUpdate\n)\n\n    \n{\n\n        \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n        \n{\n\n            \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n            \n{\n\n                \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n                \n...\n\n                \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n            \n}\n\n            \nachUpdate\n.\nvalue\n[\niw\n]\n \n+=\n \nvcoul\n(\nigp\n)\n \n*\n \nsch_array\n[\niw\n];\n\n        \n}\n\n\n    \n},\nachtempVar\n);\n\n\n    \nfor\n(\nint\n \niw\n=\nnstart\n;\n \niw\nnend\n;\n \n++\niw\n)\n\n        \nachtemp\n[\niw\n]\n \n+=\n \nachtempVar\n.\nvalue\n[\niw\n];\n\n\n}\n\n\n\nAs shown in the above code snippet, we save the value of achtempVar in achtemp, else we loose it's value of  from iteration i to i+1.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/gw/kokkos_implementation/#complex-numbers-in-kokkos", 
            "text": "Kokkos has its own implementation of complex numbers.\nThe framework supports most of the operations involving complex numbers but there are a few that are not yet available.\nFor example the power of a complex number is not defined and a few others such as multiplying, adding or subtracting a double to/from a complex number are yet to be implemented.\nWe can however define our own implementation and annotate the function with  KOKKOS_INLINE_FUNCTION  to achieve the result.  KOKKOS_INLINE_FUNCTION  Kokkos :: complex double   kokkos_square ( Kokkos :: complex double   compl_num )  { \n     double   re   =   Kokkos :: real ( compl_num ); \n     double   im   =   Kokkos :: imag ( compl_num ); \n\n     Kokkos :: complex double   result ( re * re   -   im * im ,   2 * re * im ); \n     return   result ;  }   In Kokkos it is advised to use a  Kokkos::View  datatype inorder to store modify the data inside a Kokkos construct.\nWe follow the below shown technique to create a vector and matrix type of views  with the appropriate execution and memory spaces.  #define CUDASPACE 0  #define OPENMPSPACE 0  #define CUDAUVM 1  #define SERIAL 0  #define THREADS 0  #if OPENMPSPACE \n         typedef   Kokkos :: OpenMP     ExecSpace ; \n         typedef   Kokkos :: OpenMP          MemSpace ; \n         typedef   Kokkos :: LayoutRight    Layout ;  #endif  #if CUDASPACE \n         typedef   Kokkos :: Cuda       ExecSpace ; \n         typedef   Kokkos :: CudaSpace       MemSpace ; \n         typedef   Kokkos :: LayoutLeft     Layout ;  #endif  #if SERIAL \n         typedef   Kokkos :: Serial     ExecSpace ; \n         typedef   Kokkos :: HostSpace       MemSpace ;  #endif  #if THREADS \n         typedef   Kokkos :: Threads    ExecSpace ; \n         typedef   Kokkos :: HostSpace       MemSpace ;  #endif  #if CUDAUVM \n         typedef   Kokkos :: Cuda       ExecSpace ; \n         typedef   Kokkos :: CudaUVMSpace    MemSpace ; \n         typedef   Kokkos :: LayoutLeft     Layout ;  #endif  typedef   Kokkos :: RangePolicy ExecSpace    range_policy ;  typedef   Kokkos :: View Kokkos :: complex double ,   Layout ,   MemSpace     ViewScalarTypeComplex ;  typedef   Kokkos :: View Kokkos :: complex double * ,   Layout ,   MemSpace     ViewVectorTypeComplex ;  typedef   Kokkos :: View Kokkos :: complex double ** ,   Layout ,   MemSpace    ViewMatrixTypeComplex ;  };  ViewMatrixTypeComplex   array1 ( array1 ,   N , M );  ViewMatrixTypeComplex   array2 ( array2 ,   N , M );  ViewVectorTypeComplex   vcoul ( vcoul ,   ngpown )  \nAs shown in the code structure section, there is a reduction being performed over a vector of complex numbers  achtemp \nIn the OpenMP version of the code we reduce these values inside individual thread-local arrays and then later accumulate the results.\nBut since Kokkos allows user defined reduction, we create a structure with an array of 3 complex numbers and overload the += operator for this structure.\nWe then perform a  Kokkos::parallel_reduce  on the structure.", 
            "title": "Complex Numbers in Kokkos"
        }, 
        {
            "location": "/case_studies/gw/kokkos_implementation/#kokkosopenmp-implementation", 
            "text": "struct   achtempStruct  { \n     Kokkos :: complex double   value [ 3 ];  KOKKOS_INLINE_FUNCTION \n     void   operator += ( achtempStruct   const   other ) \n     { \n         for   ( int   i   =   0 ;   i     3 ;   ++ i ) \n             value [ i ]   +=   other . value [ i ]; \n     }  KOKKOS_INLINE_FUNCTION \n     void   operator += ( achtempStruct   const   volatile   other )   volatile \n     { \n         for   ( int   i   =   0 ;   i     3 ;   ++ i ) \n             value [ i ]   +=   other . value [ i ]; \n     }  };  Kokkos :: complex double    achtemp [ 3 ];  achStruct   achtempVar   =   {{ achtemp [ 0 ], achtemp [ 1 ], achtemp [ 2 ]}};  Kokkos :: parallel_reduce ( ngpown ,   KOKKOS_LAMBDA   ( int   my_igp ,   achtempStruct   achUpdate )  { \n     for ( int   iw   =   0 ;   iw     3 ;   iw ++   ) \n     { \n         for ( int   ig   =   0 ;   ig     ncouls ;   ig ++ ) \n         { \n             delw   =   array2 ( ig , np )   /   ( wxt ( iw , np )   -   array1 ( ig , igp )) \n             ... \n             scht   +=   ...   *   delw   *   array3 ( ig , igp ) \n         } \n         achUpdate . value [ iw ]   +=   vcoul ( igp )   *   sch_array [ iw ]; \n     }  }, achtempVar );", 
            "title": "Kokkos+OpenMP implementation"
        }, 
        {
            "location": "/case_studies/gw/kokkos_implementation/#initial-observation", 
            "text": "In this section, we will describe some of the initial hurdles faced while porting the gw-kernel using Kokkos.\nNot shown in the code structure is the outermost loop of the computation (shown below), which iterates over the number of bands.\nIf we update the achtemp as a view inside the Kokkos construct, we loose its value when the outer loop starts the new iteration.\nhence we have to store it in a separate structure in order to save its value for the next iteration.  for ( int   n1   =   0 ;   n1 number_bands ;   ++ n1 )  { \n     Kokkos :: parallel_reduce ( ngpown ,   KOKKOS_LAMBDA   ( int   my_igp ,   achtempStruct   achUpdate ) \n     { \n         for ( int   iw   =   0 ;   iw     3 ;   iw ++   ) \n         { \n             for ( int   ig   =   0 ;   ig     ncouls ;   ig ++ ) \n             { \n                 delw   =   array2 ( ig , np )   /   ( wxt ( iw , np )   -   array1 ( ig , igp )) \n                 ... \n                 scht   +=   ...   *   delw   *   array3 ( ig , igp ) \n             } \n             achUpdate . value [ iw ]   +=   vcoul ( igp )   *   sch_array [ iw ]; \n         } \n\n     }, achtempVar ); \n\n     for ( int   iw = nstart ;   iw nend ;   ++ iw ) \n         achtemp [ iw ]   +=   achtempVar . value [ iw ];  }  \nAs shown in the above code snippet, we save the value of achtempVar in achtemp, else we loose it's value of  from iteration i to i+1.", 
            "title": "INITIAL OBSERVATION"
        }, 
        {
            "location": "/case_studies/gw/openmp_implementation/", 
            "text": "The original code C++ code structure is as follows :\n\n\nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n\n{\n\n    \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n    \n{\n\n        \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n        \n{\n\n            \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n            \n...\n\n            \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n        \n}\n\n        \nachtemp\n[\niw\n]\n \n+=\n \nsch_array\n[\niw\n]\n \n*\n \nvcoul\n[\nigp\n];\n\n    \n}\n\n\n}\n\n\n\nThere are 3 nested loops and nearly 97% of the computation happens in the innermost loop.\nThe outermost loop is distributed among the threads for parallel execution and involves a reduction.\nThe innermost loop is vectorized.\nSince OpenMP does not support reduction over complex variables, we update them inside individual threads and then accumulate the final result at the end.\nThe OpenMP 3.0 follows the following structure\n\n\n#pragma omp parallel for shared(wxt, array1, vcoul[0:ngpown])  firstprivate(...) schedule(dynamic) private(tid)\n\n\nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n\n{\n\n    \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n    \n{\n\n        \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n        \n{\n\n            \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n            \n...\n\n            \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n        \n}\n\n        \n(\n*\nach_threadArr_vla\n)[\ntid\n][\niw\n]\n \n+=\n \nsch_array\n[\niw\n]\n \n*\n \nvcoul\n[\nigp\n];\n\n    \n}\n\n\n}\n\n\n#pragma omp simd\n\n    \nfor\n(\nint\n \niw\n=\nnstart\n;\n \niw\nnend\n;\n \n++\niw\n)\n\n        \nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nnumThreads\n;\n \ni\n++\n)\n\n            \nachtemp\n[\niw\n]\n \n+=\n \n(\n*\nachtemp_threadArr_vla\n)[\ni\n][\niw\n];\n\n\n\n\n\nTo port the application for an accelerator using OpenMP4.5x we use the target clause to offload the computation.\nThe target implementation has following structure:\n\n\n#pragma omp target map(to:array2[0:ncouls], vcoul[0:ngpown]) map(from: achtemp_threadArr_vla[0:numberThreads*3])\n\n\n{\n\n    \n...\n\n\n#pragma omp teams distribute parallel for shared(wxt, array1, vcoul)  firstprivate(...) schedule(dynamic) private(tid)\n\n    \nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n    \n{\n\n        \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n        \n{\n\n            \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n            \n{\n\n                \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n                \n...\n\n                \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n            \n}\n\n            \n(\n*\nach_threadArr_vla\n)[\ntid\n][\niw\n]\n \n+=\n \nscht\n \n*\n \nvcoul\n[\nigp\n];\n\n        \n}\n\n    \n}\n\n\n}\n\n\n#pragma omp simd\n\n    \nfor\n(\nint\n \niw\n=\nnstart\n;\n \niw\nnend\n;\n \n++\niw\n)\n\n        \nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nnumThreads\n;\n \ni\n++\n)\n\n            \nach\n[\niw\n]\n \n+=\n \n(\n*\nach_threadArr_vla\n)[\ni\n][\niw\n];\n\n\n\nFor Intel compilers we need the flag \n-qopenmp-offload=host\n and in case of GCC with for the NVIDIA GPUs we need the \n-foffload=nvptx-none\n flag during compilation.\n\n\n\n\n#pragma omp target\n - offload the code block on the device\n\n\nmap(to:var, arr[0:N])\n - copy the data on to the device\n\n\nmap(from:var, arr[0:N])\n- copy data from the device\n\n\n#pragma omp teams\n - Create thread teams\n\n\ndistribute\n - distribute the iterations of the loop over thread teams\n\n\nparallel for\n - parallelize the iterations over the threads inside thread teams\n\n\n\n\nThe original code C++ code structure is as follows :\n\n\nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n\n{\n\n    \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n    \n{\n\n        \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n        \n{\n\n            \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n            \n...\n\n            \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n        \n}\n\n        \nachtemp\n[\niw\n]\n \n+=\n \nsch_array\n[\niw\n]\n \n*\n \nvcoul\n[\nigp\n];\n\n    \n}\n\n\n}\n\n\n\nThere are 3 nested loops and nearly 97% of the computation happens in the innermost loop.\nThe outermost loop is distributed among the threads for parallel execution and involves a reduction.\nThe innermost loop is vectorized.\nSince OpenMP does not support reduction over complex variables, we update them inside individual threads and then accumulate the final result at the end.\nThe OpenMP 3.0 follows the following structure\n\n\n#pragma omp parallel for shared(wxt, array1, vcoul[0:ngpown])  firstprivate(...) schedule(dynamic) private(tid)\n\n\nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n\n{\n\n    \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n    \n{\n\n        \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n        \n{\n\n            \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n            \n...\n\n            \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n        \n}\n\n        \n(\n*\nach_threadArr_vla\n)[\ntid\n][\niw\n]\n \n+=\n \nsch_array\n[\niw\n]\n \n*\n \nvcoul\n[\nigp\n];\n\n    \n}\n\n\n}\n\n\n#pragma omp simd\n\n    \nfor\n(\nint\n \niw\n=\nnstart\n;\n \niw\nnend\n;\n \n++\niw\n)\n\n        \nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nnumThreads\n;\n \ni\n++\n)\n\n            \nachtemp\n[\niw\n]\n \n+=\n \n(\n*\nachtemp_threadArr_vla\n)[\ni\n][\niw\n];\n\n\n\n\n\nTo port the application for an accelerator using OpenMP4.5x we use the target clause to offload the computation.\nThe target implementation has following structure:\n\n\n#pragma omp declare target\n\n\nvoid\n \nflagOCC_solver\n(\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nint\n \n,\n \nint\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nint\n \n,\n \nint\n \n,\n \nint\n \n,\n \nint\n \n,\n \nint\n \n);\n\n\n\nvoid\n \nreduce_achstemp\n(\nint\n \n,\n \nint\n \n,\n \nint\n*\n,\n \nint\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n*\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n  \nint\n*\n \n,\n \nint\n \n,\n \ndouble\n*\n \n);\n\n\n\nvoid\n \nssxt_scht_solver\n(\ndouble\n \n,\n \nint\n \n,\n \nint\n \n,\n \nint\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n,\n \nstd\n::\ncomplex\ndouble\n \n);\n\n\n#pragma omp end declare target\n\n\n\n\n#pragma omp target map(to:array2[0:ncouls], vcoul[0:ngpown]) map(from: achtemp_threadArr_vla[0:numberThreads*3])\n\n\n{\n\n    \n...\n\n\n#pragma omp teams distribute parallel for shared(wxt, array1, vcoul)  firstprivate(...) schedule(dynamic) private(tid)\n\n    \nfor\n(\nint\n \nigp\n \n=\n \n0\n;\n \nigp\nngpown\n;\n \nig\n++\n)\n\n    \n{\n\n        \nfor\n(\nint\n \niw\n \n=\n \n0\n;\n \niw\n \n \n3\n;\n \niw\n++\n \n)\n\n        \n{\n\n            \nfor\n(\nint\n \nig\n \n=\n \n0\n;\n \nig\n \n \nncouls\n;\n \nig\n++\n)\n\n            \n{\n\n                \ndelw\n \n=\n \narray2\n(\nig\n,\nnp\n)\n \n/\n \n(\nwxt\n(\niw\n,\nnp\n)\n \n-\n \narray1\n(\nig\n,\nigp\n))\n\n                \n...\n\n                \nscht\n \n+=\n \n...\n \n*\n \ndelw\n \n*\n \narray3\n(\nig\n,\nigp\n)\n\n            \n}\n\n            \n(\n*\nach_threadArr_vla\n)[\ntid\n][\niw\n]\n \n+=\n \nscht\n \n*\n \nvcoul\n[\nigp\n];\n\n        \n}\n\n    \n}\n\n\n}\n\n\n#pragma omp simd\n\n    \nfor\n(\nint\n \niw\n=\nnstart\n;\n \niw\nnend\n;\n \n++\niw\n)\n\n        \nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nnumThreads\n;\n \ni\n++\n)\n\n            \nach\n[\niw\n]\n \n+=\n \n(\n*\nach_threadArr_vla\n)[\ni\n][\niw\n];\n\n\n\nFunction accessed from inside the \ntarget\n directive need to be declared inside the \n#pragma omp declare target\n and \n#pragma omp end declare target\n.\nFor Intel compilers we need the flag \n-qopenmp-offload=host\n and in case of GCC with for the NVIDIA GPUs we need the \n-foffload=nvptx-none\n flag during compilation.\n\n\n\n\n#pragma omp target\n - offload the code block on the device\n\n\nmap(to:var, arr[0:N])\n - copy the data on to the device\n\n\nmap(from:var, arr[0:N])\n- copy data from the device\n\n\n#pragma omp teams\n - Create thread teams\n\n\ndistribute\n - distribute the iterations of the loop over thread teams\n\n\nparallel for\n - parallelize the iterations over the threads inside thread teams", 
            "title": "OpenMP 4.x"
        }, 
        {
            "location": "/case_studies/qcd/overview/", 
            "text": "Introduction to Lattice QCD\n\n\nLattice QCD\n is a numerical method to evaluate \nQuantum Chromodynamics (QCD)\n,\nthe theory of the strong interaction which binds quarks into nucleons and nucleons into nuclei,\nin a straightforward way with quantifiable uncertainties. It is non-perturbative and thus has access\nto energy regimes where common analytical methods fail.\nIn order to transform continuum QCD to Lattice QCD, one first rotates the time axis to imaginary times which \ntransforms the 4-dimensional \nMinkowski space\n into Euclidean \n\\(\\mathbb{R}^4\\)\n. Then,\neuclidean space-time is discretized by introducing a lattice spacing \n\\(a\\)\n as well as finite volume with side extents \n\\(L\\)\n.\n\n\nWilson Fermions\n\n\nThe most expensive part of Lattice QCD is the calculation of so-called quark propagators, i.e.\ncomputing the solution of the Dirac equation\n\n\\((m - /\\!\\!\\!\\!D)\\psi = \\eta\\)\n, where \n\\(m\\)\n is the mass of the quark, \n\\(\\eta\\)\n is a given vector (we will refer to this object as \nsource\n or \nright-hand-side spinor\n)\nand \n\\(/\\!\\!\\!\\!D\\)\n is a so-called gauge-covariant, Fermion derivative operator. There are many possibilities for discretizing the\ncontinuum version of the Fermion derivative operator and the most common one are the so-called \nWilson fermions\n. In this discretizaton,\nthe operator, also called \nWilson operator\n or \nDslash\n (inspired by the mathematical notation), is given by\n\n\n\\[\n/\\!\\!\\!\\!D(x,y) = \\sum\\limits_{\\mu=0}^3 U_{\\mu}(x)(1-\\gamma_{\\mu})\\delta_{y,x+\\hat{\\mu}}+U^{\\dagger}_{\\mu}(x-\\hat{\\mu})(1+\\gamma_{\\mu})\\delta_{y,x-\\hat{\\mu}}.\n\\]\nHere, \n\\(\\hat{\\mu}\\)\n denotes a displacement in \n\\(\\mu\\)\n-direction by one lattice site. \n\\(U_{\\mu}(x)\\)\n are the so-called links connecting the neighboring sites \n\\(x\\)\n and \n\\(x+\\hat{\\mu}\\)\n in a gauge-covariant way. They are elements of \n\\(SU(3)\\)\n, i.e. they can be described by 3x3 complex-valued, \nunitary matrices\n with unit \ndeterminant\n. The \n\\(\\gamma_{\\mu}\\)\n are sparse 4x4 matrices and are the generators of the so-called \nDirac algebra\n, a 4-dimensional spin \nClifford algebra\n. The indices of \n\\(U\\)\n and \n\\(\\gamma\\)\n are called color and spin indices respectively. \nNote that the Wilson operator couples only neighboring lattice sites and is thus ultra-local.\n\n\nIn modern lattice calculations, the majority of CPU time is spent on solving the Dirac equation. Therefore,\nmost optimization efforts focus on optimizing the Wilson operator as well as solvers which use this operator as their kernel.\nIt is thus important to find out whether the Wilson operator can be implemented in a performance portable way.\n\n\nImplementation\n\n\nIn this section we will briefly discuss architecture-independent implementation details of the Wilson operator. \n\n\nMultiple Right Hand Sides\n\n\nAn efficient way to increase the arithmetic intensity in sparse linear systems is to solve for multiple right hand side (MRHS) vectors simultaneously. Working on a number of right hand sides which fits SIMD registers, is also a quick and easy way to explore effects of vectorization in an implementation.\nFurther, this case is also relevant to many lattice QCD applications -- in some cases O(10\n5\n)-O(10\n6\n) systems may need to be solved with the same gauge configuration as input. For all these reasons, we have also implemented this version of the operator in our small test case. \n\n\nArithmetic Intensity\n\n\nThe arithmetic intensity per lattice site for the Wilson operator can be computed as follows:\n\n\n\\[\n\\frac{\\#\\mathrm{Flops}}{\\#\\mathrm{Bytes}} = \\frac{1320}{8G + (9-R+r)S},\n\\]\nwhere \n\\(G\\)\n is the size of a gauge link, \n\\(S\\)\n the size of a spinor, \n\\(R\\)\n the nearest neighbor spinor reuse factor (assuming that caches which are closer to the processor than the level of memory where the data resides are infinitely fast) and \n\\(r=0\\)\n if streaming stores are used and \n\\(r=1\\)\n otherwise (read-for-write). The constant factors account for the fact that in 4 dimensions, each lattice site has 8 neighbors and thus 8 links and spinors needs to be read from memory and one spinor needs to be written. If streaming stores are not used, the output spinor needs to be read into cache first and thus the total number of spinors transferred per computed site will be 10 in this case. Whereas the spinor always consists of 12 complex numbers (3 color and 4 spin components), the gauge links G can be in theory compressed to 8 real numbers by using properties of \nLie algebras\n along with the generators of \n\\(SU(3)\\)\n. However, this can require trigonometric functions whose performance may be strongly hardware dependent, so that usually a less aggressive form of compression is used by simply dropping one row or column of the gauge link and reconstruct it on the fly when needed. This format is called \n12-compression\n and widely used in modern Wilson operator implementations. In our simple test case however, we do not use this kind of compression and thus the expected arithmetic intensity is between \n\\(0.86\\)\n \n\\((R=0,\\,r=1,\\,G=18)\\)\n and \n\\(1.72\\)\n \n\\((R=7,\\,r=0,\\,G=18)\\)\n for single precision.\n\n\nWe have applied one additional common optimization to our code known as the spin-projection trick:\n\n\n\n\n\n\nThe terms \n\\(( 1 \\pm \\gamma_\\mu)\\)\n in the spin-indices act as a projector in spin, and applying them to an input vector reduces the number of independent spin-degrees of freedom in the result from 4 to 2 (with the remaining two being related to the 2 independent ones through trivial operations such as - sign, or multiplication by complex \n\\(i\\)\n, or similar). Hence, because multiplication in spin by the projectors and in color by the gauge-link matrices commute, one typically first projects an input 4-spinor to a 2-component object known as a \nhalf spinor\n. The 3x3 gauge link matrix is then multiplied to the 3-color vector object for each of the two spin components. Finally the remaining 2 spin components are \nreconstructed\n by applying the necessary trivial transformation. Spin projection depends on direction \n\\(\\mu\\)\n, but not on the lattice site indices. \n\n\n\n\n\n\nIn order to be able to utilize vector registers on architectures like Intel Xeon Phi Knight's Landing, we attempt to vectorize over the multiple-right sources in a 'multiple-right-hand side' application (MRHS) of the operator", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/qcd/overview/#introduction-to-lattice-qcd", 
            "text": "Lattice QCD  is a numerical method to evaluate  Quantum Chromodynamics (QCD) ,\nthe theory of the strong interaction which binds quarks into nucleons and nucleons into nuclei,\nin a straightforward way with quantifiable uncertainties. It is non-perturbative and thus has access\nto energy regimes where common analytical methods fail.\nIn order to transform continuum QCD to Lattice QCD, one first rotates the time axis to imaginary times which \ntransforms the 4-dimensional  Minkowski space  into Euclidean  \\(\\mathbb{R}^4\\) . Then,\neuclidean space-time is discretized by introducing a lattice spacing  \\(a\\)  as well as finite volume with side extents  \\(L\\) .", 
            "title": "Introduction to Lattice QCD"
        }, 
        {
            "location": "/case_studies/qcd/overview/#wilson-fermions", 
            "text": "The most expensive part of Lattice QCD is the calculation of so-called quark propagators, i.e.\ncomputing the solution of the Dirac equation \\((m - /\\!\\!\\!\\!D)\\psi = \\eta\\) , where  \\(m\\)  is the mass of the quark,  \\(\\eta\\)  is a given vector (we will refer to this object as  source  or  right-hand-side spinor )\nand  \\(/\\!\\!\\!\\!D\\)  is a so-called gauge-covariant, Fermion derivative operator. There are many possibilities for discretizing the\ncontinuum version of the Fermion derivative operator and the most common one are the so-called  Wilson fermions . In this discretizaton,\nthe operator, also called  Wilson operator  or  Dslash  (inspired by the mathematical notation), is given by  \\[\n/\\!\\!\\!\\!D(x,y) = \\sum\\limits_{\\mu=0}^3 U_{\\mu}(x)(1-\\gamma_{\\mu})\\delta_{y,x+\\hat{\\mu}}+U^{\\dagger}_{\\mu}(x-\\hat{\\mu})(1+\\gamma_{\\mu})\\delta_{y,x-\\hat{\\mu}}.\n\\] Here,  \\(\\hat{\\mu}\\)  denotes a displacement in  \\(\\mu\\) -direction by one lattice site.  \\(U_{\\mu}(x)\\)  are the so-called links connecting the neighboring sites  \\(x\\)  and  \\(x+\\hat{\\mu}\\)  in a gauge-covariant way. They are elements of  \\(SU(3)\\) , i.e. they can be described by 3x3 complex-valued,  unitary matrices  with unit  determinant . The  \\(\\gamma_{\\mu}\\)  are sparse 4x4 matrices and are the generators of the so-called  Dirac algebra , a 4-dimensional spin  Clifford algebra . The indices of  \\(U\\)  and  \\(\\gamma\\)  are called color and spin indices respectively. \nNote that the Wilson operator couples only neighboring lattice sites and is thus ultra-local.  In modern lattice calculations, the majority of CPU time is spent on solving the Dirac equation. Therefore,\nmost optimization efforts focus on optimizing the Wilson operator as well as solvers which use this operator as their kernel.\nIt is thus important to find out whether the Wilson operator can be implemented in a performance portable way.", 
            "title": "Wilson Fermions"
        }, 
        {
            "location": "/case_studies/qcd/overview/#implementation", 
            "text": "In this section we will briefly discuss architecture-independent implementation details of the Wilson operator.", 
            "title": "Implementation"
        }, 
        {
            "location": "/case_studies/qcd/overview/#multiple-right-hand-sides", 
            "text": "An efficient way to increase the arithmetic intensity in sparse linear systems is to solve for multiple right hand side (MRHS) vectors simultaneously. Working on a number of right hand sides which fits SIMD registers, is also a quick and easy way to explore effects of vectorization in an implementation.\nFurther, this case is also relevant to many lattice QCD applications -- in some cases O(10 5 )-O(10 6 ) systems may need to be solved with the same gauge configuration as input. For all these reasons, we have also implemented this version of the operator in our small test case.", 
            "title": "Multiple Right Hand Sides"
        }, 
        {
            "location": "/case_studies/qcd/overview/#arithmetic-intensity", 
            "text": "The arithmetic intensity per lattice site for the Wilson operator can be computed as follows:  \\[\n\\frac{\\#\\mathrm{Flops}}{\\#\\mathrm{Bytes}} = \\frac{1320}{8G + (9-R+r)S},\n\\] where  \\(G\\)  is the size of a gauge link,  \\(S\\)  the size of a spinor,  \\(R\\)  the nearest neighbor spinor reuse factor (assuming that caches which are closer to the processor than the level of memory where the data resides are infinitely fast) and  \\(r=0\\)  if streaming stores are used and  \\(r=1\\)  otherwise (read-for-write). The constant factors account for the fact that in 4 dimensions, each lattice site has 8 neighbors and thus 8 links and spinors needs to be read from memory and one spinor needs to be written. If streaming stores are not used, the output spinor needs to be read into cache first and thus the total number of spinors transferred per computed site will be 10 in this case. Whereas the spinor always consists of 12 complex numbers (3 color and 4 spin components), the gauge links G can be in theory compressed to 8 real numbers by using properties of  Lie algebras  along with the generators of  \\(SU(3)\\) . However, this can require trigonometric functions whose performance may be strongly hardware dependent, so that usually a less aggressive form of compression is used by simply dropping one row or column of the gauge link and reconstruct it on the fly when needed. This format is called  12-compression  and widely used in modern Wilson operator implementations. In our simple test case however, we do not use this kind of compression and thus the expected arithmetic intensity is between  \\(0.86\\)   \\((R=0,\\,r=1,\\,G=18)\\)  and  \\(1.72\\)   \\((R=7,\\,r=0,\\,G=18)\\)  for single precision.  We have applied one additional common optimization to our code known as the spin-projection trick:    The terms  \\(( 1 \\pm \\gamma_\\mu)\\)  in the spin-indices act as a projector in spin, and applying them to an input vector reduces the number of independent spin-degrees of freedom in the result from 4 to 2 (with the remaining two being related to the 2 independent ones through trivial operations such as - sign, or multiplication by complex  \\(i\\) , or similar). Hence, because multiplication in spin by the projectors and in color by the gauge-link matrices commute, one typically first projects an input 4-spinor to a 2-component object known as a  half spinor . The 3x3 gauge link matrix is then multiplied to the 3-color vector object for each of the two spin components. Finally the remaining 2 spin components are  reconstructed  by applying the necessary trivial transformation. Spin projection depends on direction  \\(\\mu\\) , but not on the lattice site indices.     In order to be able to utilize vector registers on architectures like Intel Xeon Phi Knight's Landing, we attempt to vectorize over the multiple-right sources in a 'multiple-right-hand side' application (MRHS) of the operator", 
            "title": "Arithmetic Intensity"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/", 
            "text": "Code Structure\n\n\nOur test code is written in C++ and designed completely from scratch. We use type definitions, templates, template-specialization, overloading and other C++ features to provide flexibility in changing precision, testing datatypes which help vectorization and also making it easier to hide architecture dependent code. The general idea is to decompose the problem into a loop over lattice site and then for each lattice site and each direction, we:\n\n\n\n\nstream-in the relevant neighboring spinor (or block of spinors in case of multiple right hand sides) from memory\n\n\nproject the 4-spinor to a 2-spinor\n\n\nread relevant gauge link and multiply it or or its hermitian adjoint with the projected spinor \n\n\nreconstruct the 4-spinor from the resulting 2-spinor and accumulate to the sum over directions\n\n\nstream-out the resulting summed vector to memory\n\n\n\n\nIn multi-node implementations the application step would be separated into bulk- and boundary application and the former interleaved with boundary communication.\n\n\nAs is common in Lattice codes we attribute a color (checkerboard) to each lattice site, depending on whether\nthe 4-dimensional coordinates sum to an even number or an odd number. The color is also referred to as a checkerboard\nindex (cb), or parity. In the traditional 2-coloring scheme (red-black or even-odd) each checkerboard of the lattice \ncontains half of the total number of lattice sites. In a nearest neighbor operator such as dslash, output spinors on sites of one\ncheckerboard color (target_cb) will need neighboring input spinors only from sites of the other checkerboard color (source_cb),\nand hence all the sites of a given checkerboard can be conveniently computed in parallel, with no write conflicts.\nGauge fields are usually stored as the forward pointing links in the 4 forward directions. The backward pointing links at a site\nare the hermitian conjugates of the forward pointing links of the site's back neighbors in each direction (which will have the\nopposite parity from the original site). Hence even for applying Dslash to sites of only one parity, the gauge fields from both\nparities need to be read.\n\n\nData Primitives\n\n\nFor facilitating this workflow, we define spinor and gauge link classes (in C++-like pseudocode):\n\n\n// num_sites is the number of lattice sites on a single checkerboard\n\n\n// color of the lattice (half the total number of lattice sites)\n\n\n//\n\n\ntemplate\ntypename\n \nST\n,\nint\n \nnspin\n \n\nclass\n \nCBSpinor\n \n{\n\n\npublic\n:\n \n  \n...\n\n\nprivate\n:\n\n  \n// dims are: site, color, spin. \n\n  \nspin_container\nST\n[\nnum_sites\n][\n3\n][\nnspin\n],\nnspin\n \ndata\n;\n\n\n};\n\n\n\ntemplate\ntypename\n \nGT\n \n\nclass\n \nCBGaugeField\n \n{\n\n\npublic\n:\n\n  \n...\n\n\nprivate\n:\n\n  \n// dims are: site, direction, color, color\n\n  \ngauge_container\nGT\n[\nnum_sites\n][\n4\n][\n3\n][\n3\n]\n;\n\n\n};\n\n\n\n\n\nhere, ST and GT refer to spinor-type and gauge-type respectively. Those types could be SIMD or scalar types and they do not necessarily need to be the same. The data containers can be plain arrays, e.g. for (non-portable) plain implementations, or arrays decorated with pragmas (e.g. for OpenMP 4.5 offloading) or more general data container classes such as Kokkos::Views, etc.. The member functions are adopted to the container classes used in the individual implementations. Note that this design allows us to test different performance portable frameworks/methods without having to restructure large parts of the code. The additional template parameter \nnspin\n allows us to easily define 2- and 4-spinor objects. \n\n\nWilson Operator\n\n\nAt this point in time, the dslash test code is not multi-node ready, so we will focus solely on on-node parallelism for the moment. Our goal is to achieve this by threading over lattice sites and applying SIMD/SIMT parallelism over multiple right hand sides. In theory, one could achieve vectorization for single right hand side vectors also by using an array or structure of array data layout but we will not consider this technique here. We will nevertheless compare our single right hand side performance we achieved with our performance portable implementations with those of optimized libraries which feature such improvements.\n\n\nOur dslash class is implemented as follow:\n\n\ntemplate\ntypename\n \nGT\n,\n \ntypename\n \nST\n,\n \ntypename\n \nTST\n,\n \nconst\n \nint\n \nisign\n,\n \nconst\n \nint\n \ntarget_cb\n\n\nclass\n \nDslash\n \n{\n\n\npublic\n:\n\n  \nvoid\n \noperator\n(\nconst\n \nCBSpinor\nST\n,\n4\n \ns_in\n,\n\n                \nconst\n \nCBGaugeField\nGT\n \ng_in_src_cb\n,\n\n                \nconst\n \nCBGaugeField\nGT\n \ng_in_target_cb\n,\n\n                \nCBSpinor\nST\n,\n4\n \ns_out\n)\n\n  \n{\n\n    \n// Threaded loop over sites\n\n    \nparallel_for\n(\nint\n \ni\n=\n0\n;\n \ni\nnum_sites\n;\n \ni\n++\n){\n\n\n      \nCBThreadSpinor\nTST\n,\n4\n \nres_sum\n;\n\n      \nCBThreadSpinor\nTST\n,\n2\n \nproj_res\n,\n \nmult_proj_res\n;\n\n\n      \nZero\n(\nres_sum\n);\n\n\n      \n// go for direction -T\n\n      \nProjectDir3\nST\n,\nTST\n,\nisign\n(\ns_in\n,\n \nproj_res\n,\nNeighborTMinus\n(\nsite\n,\ntarget_cb\n));\n\n      \nmult_adj_u_halfspinor\nGT\n,\nTST\n(\ng_in_src_cb\n,\nproj_res\n,\nmult_proj_res\n,\nNeighborTMinus\n(\nsite\n,\ntarget_cb\n),\n3\n);\n\n      \nRecons23Dir3\nTST\n,\nisign\n(\nmult_proj_res\n,\nres_sum\n);\n\n\n      \n// go for direction +T\n\n      \nProjectDir3\nST\n,\nTST\n,\n-\nisign\n(\ns_in\n,\nproj_res\n,\nNeighborTPlus\n(\nsite\n,\ntarget_cb\n));\n\n      \nmult_u_halfspinor\nGT\n,\nTST\n(\ng_in_target_cb\n,\nproj_res\n,\nmult_proj_res\n,\nsite\n,\nNeighborTPlus\n(\nsite\n,\ntarget_cb\n),\n3\n);\n\n      \nRecons23Dir3\nTST\n,\n-\nisign\n(\nmult_proj_res\n,\n \nres_sum\n);\n\n\n      \n// go for other directions: -Z, +Z, -Y, +Y, -X, +X\n\n      \n...\n\n    \n}\n\n  \n}\n\n\n};\n\n\n\n\n\nHere, the type \nTST\n denotes a thread-spinor-type which belongs to the \nCBThreadSpinor\n class. It is important to make the distinction between \nCBSpinor\n and \nCBThreadSpinor\n because, depending on the performance portability framework used, this type has to be different on CPU or GPU. What we would like to achieve ultimately is displayed in the picture below:\n\n\n\n\nIn case of the GPU (left), individual threads are each working on a single/scalar entry of the global spinor, i.e. on a single right hand side component. In case of the CPU (right), each thread is working on a chunk of right hand sites, ideally using its vector units. In both cases, the input and output spinor datatype is the same and the work spinor type is optimized for the targeted architecture. \n\n\nNote that, similar to the data classes discussed above, this skeleton-dslash allows us to specialize the Wilson operator for a variety of performance portable frameworks. Additionally, if we need more architectural specialization than the various frameworks could offer, this can be implemented cleanly by operator overloading and template specializations.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#code-structure", 
            "text": "Our test code is written in C++ and designed completely from scratch. We use type definitions, templates, template-specialization, overloading and other C++ features to provide flexibility in changing precision, testing datatypes which help vectorization and also making it easier to hide architecture dependent code. The general idea is to decompose the problem into a loop over lattice site and then for each lattice site and each direction, we:   stream-in the relevant neighboring spinor (or block of spinors in case of multiple right hand sides) from memory  project the 4-spinor to a 2-spinor  read relevant gauge link and multiply it or or its hermitian adjoint with the projected spinor   reconstruct the 4-spinor from the resulting 2-spinor and accumulate to the sum over directions  stream-out the resulting summed vector to memory   In multi-node implementations the application step would be separated into bulk- and boundary application and the former interleaved with boundary communication.  As is common in Lattice codes we attribute a color (checkerboard) to each lattice site, depending on whether\nthe 4-dimensional coordinates sum to an even number or an odd number. The color is also referred to as a checkerboard\nindex (cb), or parity. In the traditional 2-coloring scheme (red-black or even-odd) each checkerboard of the lattice \ncontains half of the total number of lattice sites. In a nearest neighbor operator such as dslash, output spinors on sites of one\ncheckerboard color (target_cb) will need neighboring input spinors only from sites of the other checkerboard color (source_cb),\nand hence all the sites of a given checkerboard can be conveniently computed in parallel, with no write conflicts.\nGauge fields are usually stored as the forward pointing links in the 4 forward directions. The backward pointing links at a site\nare the hermitian conjugates of the forward pointing links of the site's back neighbors in each direction (which will have the\nopposite parity from the original site). Hence even for applying Dslash to sites of only one parity, the gauge fields from both\nparities need to be read.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#data-primitives", 
            "text": "For facilitating this workflow, we define spinor and gauge link classes (in C++-like pseudocode):  // num_sites is the number of lattice sites on a single checkerboard  // color of the lattice (half the total number of lattice sites)  //  template typename   ST , int   nspin   class   CBSpinor   {  public :  \n   ...  private : \n   // dims are: site, color, spin.  \n   spin_container ST [ num_sites ][ 3 ][ nspin ], nspin   data ;  };  template typename   GT   class   CBGaugeField   {  public : \n   ...  private : \n   // dims are: site, direction, color, color \n   gauge_container GT [ num_sites ][ 4 ][ 3 ][ 3 ] ;  };   here, ST and GT refer to spinor-type and gauge-type respectively. Those types could be SIMD or scalar types and they do not necessarily need to be the same. The data containers can be plain arrays, e.g. for (non-portable) plain implementations, or arrays decorated with pragmas (e.g. for OpenMP 4.5 offloading) or more general data container classes such as Kokkos::Views, etc.. The member functions are adopted to the container classes used in the individual implementations. Note that this design allows us to test different performance portable frameworks/methods without having to restructure large parts of the code. The additional template parameter  nspin  allows us to easily define 2- and 4-spinor objects.", 
            "title": "Data Primitives"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#wilson-operator", 
            "text": "At this point in time, the dslash test code is not multi-node ready, so we will focus solely on on-node parallelism for the moment. Our goal is to achieve this by threading over lattice sites and applying SIMD/SIMT parallelism over multiple right hand sides. In theory, one could achieve vectorization for single right hand side vectors also by using an array or structure of array data layout but we will not consider this technique here. We will nevertheless compare our single right hand side performance we achieved with our performance portable implementations with those of optimized libraries which feature such improvements.  Our dslash class is implemented as follow:  template typename   GT ,   typename   ST ,   typename   TST ,   const   int   isign ,   const   int   target_cb  class   Dslash   {  public : \n   void   operator ( const   CBSpinor ST , 4   s_in , \n                 const   CBGaugeField GT   g_in_src_cb , \n                 const   CBGaugeField GT   g_in_target_cb , \n                 CBSpinor ST , 4   s_out ) \n   { \n     // Threaded loop over sites \n     parallel_for ( int   i = 0 ;   i num_sites ;   i ++ ){ \n\n       CBThreadSpinor TST , 4   res_sum ; \n       CBThreadSpinor TST , 2   proj_res ,   mult_proj_res ; \n\n       Zero ( res_sum ); \n\n       // go for direction -T \n       ProjectDir3 ST , TST , isign ( s_in ,   proj_res , NeighborTMinus ( site , target_cb )); \n       mult_adj_u_halfspinor GT , TST ( g_in_src_cb , proj_res , mult_proj_res , NeighborTMinus ( site , target_cb ), 3 ); \n       Recons23Dir3 TST , isign ( mult_proj_res , res_sum ); \n\n       // go for direction +T \n       ProjectDir3 ST , TST , - isign ( s_in , proj_res , NeighborTPlus ( site , target_cb )); \n       mult_u_halfspinor GT , TST ( g_in_target_cb , proj_res , mult_proj_res , site , NeighborTPlus ( site , target_cb ), 3 ); \n       Recons23Dir3 TST , - isign ( mult_proj_res ,   res_sum ); \n\n       // go for other directions: -Z, +Z, -Y, +Y, -X, +X \n       ... \n     } \n   }  };   Here, the type  TST  denotes a thread-spinor-type which belongs to the  CBThreadSpinor  class. It is important to make the distinction between  CBSpinor  and  CBThreadSpinor  because, depending on the performance portability framework used, this type has to be different on CPU or GPU. What we would like to achieve ultimately is displayed in the picture below:   In case of the GPU (left), individual threads are each working on a single/scalar entry of the global spinor, i.e. on a single right hand side component. In case of the CPU (right), each thread is working on a chunk of right hand sites, ideally using its vector units. In both cases, the input and output spinor datatype is the same and the work spinor type is optimized for the targeted architecture.   Note that, similar to the data classes discussed above, this skeleton-dslash allows us to specialize the Wilson operator for a variety of performance portable frameworks. Additionally, if we need more architectural specialization than the various frameworks could offer, this can be implemented cleanly by operator overloading and template specializations.", 
            "title": "Wilson Operator"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/", 
            "text": "Kokkos Implementation\n\n\nIn Kokkos, it is advised to use the \nKokkos::View\n datatype as our data container. Therefore, the \nspinor and gaugefield classes\n become\n\n\ntemplate\ntypename\n \nST\n,\n \nint\n \nnspin\n\n\nclass\n \nCBSpinor\n \n{\n\n\npublic\n:\n \n    \n// Define a type handle for the Kokkos View in this class (a Trait)\n\n    \n// dims are: site, color, spin\n\n    \nusing\n \nDataType\n \n=\n  \nKokkos\n::\nView\nST\n*\n[\n3\n][\nnspin\n],\nLayout\n,\nMemorySpace\n;\n\n\n    \n// Get the actual Kokkos views...\n\n    \nconst\n \nDataType\n \nGetData\n()\n \nconst\n \n{\n \nreturn\n \n_data\n;\n \n}\n\n    \nDataType\n \nGetData\n \n{\n \nreturn\n \n_data\n;\n \n}\n\n\nprivate\n:\n\n    \nDataType\n \n_data\n;\n\n\n};\n\n\n\n// Convenient Name for the View inside a 4 spinor\n\n\ntemplate\ntypename\n \nST\n\n\nusing\n \nSpinorView\n \n=\n \ntypename\n \nCBSpinor\nST\n,\n4\n::\nDataType\n;\n \n\n\ntemplate\ntypename\n \nGT\n \n\nclass\n \nCBGaugeField\n \n{\n\n\npublic\n:\n\n    \n// Define a type handle for the Kokkos View in this class (a Trait)\n\n    \n// dims are: site, direction, color, color  \n\n    \nusing\n \nDataType\n \n=\n  \nKokkos\n::\nView\nGT\n*\n[\n4\n][\n3\n][\n3\n],\nGaugeLayout\n,\nMemorySpace\n;\n\n\n    \n// Get the actual Kokkos views...\n\n    \nconst\n \nDataType\n \nGetData\n()\n \nconst\n \n{\n \nreturn\n \n_data\n;\n \n}\n\n    \nDataType\n \nGetData\n \n{\n \nreturn\n \n_data\n;\n \n}\n\n\nprivate\n:\n\n    \nDataType\n \n_data\n;\n      \n\n};\n\n\n\n// A Handle for the View part of the Gauge Field\n\n\ntemplate\ntypename\n \nGT\n\n\nusing\n \nGaugeView\n \n=\n \ntypename\n \nCBGaugeField\nGT\n::\nDataType\n;\n\n\n\n\n\nIn each container class, we defined a \nDataType\n to be an appropriate \nKokkos::View\n for that object.\nNote that in the Views the site index dimension is a runtime dimension (denoted by \n*\n) whereas the other dimensions - color and spin - are fixed (denoted by \n[const]\n). Explicitly stating this is recommended by the kokkos developers because it should help the compiler to optimize the code.\n\n\nHere \nLayout\n, \nGaugeLayout\n and \nMemorySpace\n are policies we set at compile time in \na single header file, based on architecture e.g. for KNL one would have:\n\n\nusing\n \nExecSpace\n \n=\n \nKokkos\n::\nOpenMP\n::\nexecution_space\n;\n\n\nusing\n \nMemorySpace\n \n=\n \nKokkos\n::\nOpenMP\n::\nmemory_space\n;\n\n\nusing\n \nLayout\n \n=\n \nKokkos\n::\nOpenMP\n::\narray_layout\n;\n \n\nusing\n \nGaugeLayout\n \n=\n \nKokkos\n::\nOpenMP\n::\narray_layout\n;\n\n\nusing\n \nNeighLayout\n \n=\n \nKokkos\n::\nOpenMP\n::\narray_layout\n;\n\n\nusing\n \nThreadExecPolicy\n \n=\n \nKokkos\n::\nTeamPolicy\nExecSpace\n;\n\n\n\n\n\nwhereas the appropriate settings for Single Right Hand Side Dslash for GPUs were:\n\nusing\n \nExecSpace\n \n=\n \nKokkos\n::\nCuda\n::\nexecution_space\n;\n\n\nusing\n \nMemorySpace\n \n=\n \nKokkos\n::\nCuda\n::\nmemory_space\n;\n\n\nusing\n \nLayout\n \n=\n \nKokkos\n::\nCuda\n::\narray_layout\n;\n\n\nusing\n \nGaugeLayout\n \n=\n \nKokkos\n::\nCuda\n::\narray_layout\n;\n\n\nusing\n \nNeighLayout\n \n=\n \nKokkos\n::\nCuda\n::\narray_layout\n;\n\n\nusing\n \nThreadExecPolicy\n \n=\n  \nKokkos\n::\nTeamPolicy\nExecSpace\n,\nKokkos\n::\nLaunchBounds\n128\n,\n1\n;\n\n\n\nFinally we note that when a VectorType is used (see below) the setting for \nLayout\n, \nGaugeLayout\n, and \nNeighLayout\n\nwas \nKokkos::LayoutRight\n for both CPU and GPU.\n\n\nCare needs to be taken whether one passes by value or reference, as in an environment involving GPUs\none cannot pass references to data on the host to the GPU (unless one uses Unified Virtual Memory -- UVM).\nAttempting to do so can lead to a run-time error. Also, we have found that one should not execute Kokkos\nparallel regions in the class constructor. Kokkos Views are small objects with a pointer potentially to memory\nin the appropriate \nMemorySpace\n. They are cheap to copy, and it help avoiding programming errors if we can\npass the views directly to our kernels, rather than passing the wrapper \nCBSpinor\n objects. Hence we define\naccessors: \nGetData()\n for both our wrapper classes, and we also define the shorthand type aliases \nSpinorView\n and\n\nGaugeView\n for the Kokkos views as used in spinors or gauge fields respectively.\n\n\nIt is common in lattice QCD to apply Dslash for both checkerboards and parities, and these are usually given\nas run-time parameters to a DSlash operator. In order to give the compiler maximum information, and to avoid\nunnecessary register spilling on GPUs we wanted to make these parameters essentially compile time. Hence we defined \na functor class \nDslashFunctor\n whose \noperator()\n implements the Dslash as discussed in \nWilson operator class\n.\n\n\ntemplate\ntypename\n \nGT\n,\n \ntypename\n \nST\n,\n \ntypename\n \nTST\n,\n \nconst\n \nint\n \nisign\n,\n \nconst\n \nint\n \ntarget_cb\n\n\nstruct\n \nDslashFunctor\n \n{\n \n  \nSpinorView\nST\n \ns_in\n;\n          \n// These are the Kokkos::Views\n\n  \nGaugeView\nGT\n \ng_in_src_cb\n;\n\n  \nGaugeView\nGT\n \ng_in_target_cb\n;\n\n  \nSpinorView\nST\n \ns_out\n;\n\n  \nint\n \nnum_sites\n;\n\n  \nint\n \nsites_per_team\n;\n\n  \nSiteTable\n \nneigh_table\n;\n\n\n  \nKOKKOS_FORCEINLINE_FUNCTION\n\n  \nvoid\n \noperator\n()(\nconst\n \nTeamHandle\n \nteam\n)\n \nconst\n \n{\n\n    \nconst\n \nint\n \nstart_idx\n \n=\n \nteam\n.\nleague_rank\n()\n*\nsites_per_team\n;\n\n    \nconst\n \nint\n \nend_idx\n \n=\n \nstart_idx\n \n+\n \nsites_per_team\n  \n \nnum_sites\n \n?\n \nstart_idx\n \n+\n \nsites_per_team\n \n:\n \nnum_sites\n;\n\n\n    \nKokkos\n::\nparallel_for\n(\nKokkos\n::\nTeamThreadRange\n(\nteam\n,\nstart_idx\n,\nend_idx\n),[\n=\n](\nconst\n \nint\n \nsite\n)\n \n{\n\n\n      \n// Body as per the previous pseudocode\n\n      \nSpinorSiteView\nTST\n \nres_sum\n;\n\n      \nHalfSpinorSiteView\nTST\n \nproj_res\n;\n\n      \nHalfSpinorSiteView\nTST\n \nmult_proj_res\n;\n\n\n      \nfor\n(\nint\n \ncolor\n=\n0\n;\n \ncolor\n \n \n3\n;\n \n++\ncolor\n)\n \n{\n\n        \nfor\n(\nint\n \nspin\n=\n0\n;\n \nspin\n \n \n4\n;\n \n++\nspin\n)\n \n{\n\n          \nComplexZero\n(\nres_sum\n(\ncolor\n,\nspin\n));\n\n        \n}\n\n      \n}\n\n\n      \n// T - minus\n\n      \nKokkosProjectDir3\nST\n,\nTST\n,\nisign\n(\ns_in\n,\n \nproj_res\n,\n\n                             \nneigh_table\n.\nNeighborTMinus\n(\nsite\n,\ntarget_cb\n));\n\n      \nmult_adj_u_halfspinor\nGT\n,\nTST\n(\ng_in_src_cb\n,\nproj_res\n,\nmult_proj_res\n,\n\n                             \nneigh_table\n.\nNeighborTMinus\n(\nsite\n,\ntarget_cb\n),\n3\n);\n\n      \nKokkosRecons23Dir3\nTST\n,\nisign\n(\nmult_proj_res\n,\nres_sum\n);\n\n\n      \n// ... other directions....\n\n    \n});\n\n  \n}\n\n\n};\n\n\n\n\n\nand one can see that the functor operates directly on \nKokkos::View\n data members and that\napart from the ``site``` other indexing parameters such as target_cb are compile time through templates.\n\n\nTo still allow choices of \ntarget_cb\n and \nisign\n (whether to apply Dslash or its hermitian conjugate)\nthe top-level \nDslash\n class looks like:\n\n\ntemplate\ntypename\n \nGT\n,\n \ntypename\n \nST\n,\n \ntypename\n \nTST\n\n\nclass\n \nKokkosDslash\n \n{\n\n\npublic\n:\n\n  \nvoid\n \noperator\n()(\nconst\n \nCBSpinor\nST\n,\n4\n \nspinor_in\n,\n\n                  \nconst\n \nGaugeField\nGT\n \ngauge_in\n,\n   \n// contains CBGaugeField for both cb-s\n\n                  \nCBSpinor\nST\n,\n4\n \nspinor_out\n,\n\n                  \nint\n \nplus_minus\n)\n \nconst\n\n  \n{\n\n    \nint\n \nsource_cb\n \n=\n \nspinor_in\n.\nGetCB\n();\n\n    \nint\n \ntarget_cb\n \n=\n \n(\nsource_cb\n \n==\n \nEVEN\n)\n \n?\n \nODD\n \n:\n \nEVEN\n;\n\n    \nconst\n \nSpinorView\nST\n \ns_in\n \n=\n \nspinor_in\n.\nGetData\n();\n\n    \nconst\n \nGaugeView\nGT\n \ng_in_src_cb\n \n=\n \n(\ngauge_in\n(\nsource_cb\n)).\nGetData\n();\n\n    \nconst\n \nGaugeView\nGT\n  \ng_in_target_cb\n \n=\n \n(\ngauge_in\n(\ntarget_cb\n)).\nGetData\n();\n\n    \nSpinorView\nST\n \ns_out\n \n=\n \nspinor_out_out\n.\nGetData\n();\n\n\n    \n...\n\n\n    \n// Create a Team policy, and inform it of the Vector Length\n\n    \nThreadExecPolicy\n \npolicy\n(\nnum_sites\n/\n_sites_per_team\n,\n \nKokkos\n::\nAUTO\n(),\nVeclen\nST\n::\nvalue\n);\n\n\n    \n// Ugly if-else to turn plus_minus \n target_cb into \n\n    \n// constant template arguments\n\n    \nif\n(\n \nplus_minus\n \n==\n \n1\n \n)\n \n{\n\n      \nif\n \n(\ntarget_cb\n \n==\n \n0\n \n)\n \n{\n\n        \n// Instantiate appropriate functor and construct\n\n        \n// All view initializations will be done by copy\n\n        \n// the values of plus_minus (1) and target_cb (0)\n\n        \n// are made explicit and compile time in the template\n\n        \n// arguments\n\n        \nDslashFunctor\nGT\n,\nST\n,\nTST\n,\n1\n,\n0\n \nf\n \n=\n \n{\ns_in\n,\n \ng_in_src_cb\n,\n \ng_in_target_cb\n,\n \ns_out\n,\nnum_sites\n,\n \n_sites_per_team\n,\n_neigh_table\n};\n\n\n          \n// Dispatch the thread Teams                     \n\n          \nKokkos\n::\nparallel_for\n(\npolicy\n,\n \nf\n);\n \n// Outer Lambda \n\n      \n}\n\n      \nelse\n \n{\n\n        \n...\n \n// Instantiate DslashFunctor for target_cb=1 \n Dispatch        \n\n      \n}\n\n    \n}\n\n    \nelse\n \n{\n \n      \n// plus_minus == -1 case, target_cb=0,1 cases\n\n      \n...\n\n    \n}\n\n  \n}\n\n\n};\n\n\n\n\n\nIn the above, the GPU Threads in the \n\\(X\\)\n direction are created, when we call \nKokkos::parallel_for\n with the \nThreadExecPolicy\n, to which we have given a vector length through the \nVeclen\nST\n::veclen\n type trait. The nested \nif-else\n clauses turn run-time\nparameters into template parameters. Finally, the member data of the \nDslashFunctor\n are initialized by copy.\n\n\nNote that since we linearized the site index, we need to compute the neighboring site indices manually. On architectures with poor integer arithmetic performance this might lead to a significant performance penalty. Therefore, we implement a \nSiteTable\n class with\nmember functions like \nNeighborTMinus(site,target_cb)\n to give the linear site indices of the neighbors. The instance of this site table is the \n_neigh_table\n member in the \nKokkosDslash\n class and is used to initialize the \nneigh_table\n member of the \nDslashFunctor\n. Our implementation is such that \nSiteTable\n either holds a pre-computed neighbor table or computes the site neighbor for a given direction on the fly (a choice made at configuration \n compile time using \n#ifdef\n). In our performance measurements we use the implementation which gives the best performance for a given architecture.\n\n\nComplex Numbers and C++\n\n\nWe want to emphasize a subtle performance pitfall when it comes to complex numbers in C++. The language standards inhibit the compiler to efficiently optimize operations such as\n\n\nc\n \n+=\n \na\n \n*\n \nb\n\n\n\n\n\nwhen \na\n, \nb\n and \nc\n are complex numbers. Naively, this expression could be expanded into\n\n\nre\n(\nc\n)\n \n+=\n \nre\n(\na\n)\n \n*\n \nre\n(\nb\n)\n \n-\n \nim\n(\na\n)\n \n*\n \nim\n(\nb\n)\n\n\nim\n(\nc\n)\n \n+=\n \nre\n(\na\n)\n \n*\n \nim\n(\nb\n)\n \n+\n \nim\n(\na\n)\n \n*\n \nre\n(\nb\n)\n\n\n\n\n\nwhere \nre(.), im(.)\n denote the real and imaginary part of its argument respectively. This expression can nicely be packed into a total of four FMA operations per line. However, in the simplified form above which is usually used in context of operator overloading, the compiler would have to evaluate the right hand side first and then sum the result into \nc\n. This is much less efficient since in that case, only two FMA as well as two multiplications and additions could be used. One has to keep that in mind when doing complex algebra in C++. In many cases it is better to inline code and avoid otherwise useful operator overloading techniques for complex algebra.\n\n\nEnsuring Vectorization\n\n\nVectorization in Kokkos is achieved by a two-level \nnested parallelism\n, where the outer loop spawns threads (pthreads, OpenMP-threads) on the CPU and threads in CUDA-block y-direction on the GPU. The inner loop then applies vectorization pragmas on the CPU or spawns threads in x-direction on the GPU. This is where we have to show some awareness of architectural differences: the spinor work type \nTST\n needs to be a scalar type on the GPU and a vector type on the CPU. Hence we declare the following types on GPU and CPU respectively\n\n\ntemplate\ntypename\n \nT\n,\nN\n\n\nstruct\n \nCPUSIMDComplex\n \n{\n\n  \nKokkos\n::\ncomplex\nT\n \n_data\n[\nN\n];\n\n\n  \nT\n \noperator\n()(\nint\n \nlane\n)\n \n{\n\n    \nreturn\n \n_data\n[\nlane\n];\n\n  \n}\n\n\n  \n...\n\n\n};\n\n\n\ntemplate\ntypename\n \nT\n,\nN\n\n\nstruct\n \nGPUSIMDComplex\n \n{\n\n  \nKokkos\n::\ncomplex\nT\n \n_data\n;\n\n\n  \nT\n \noperator\n()(\nint\n \nlane\n)\n \n{\n\n    \nreturn\n \n_data\n;\n\n  \n}\n\n\n  \n...\n\n\n};\n\n\n\n\n\nThe latter construct might look confusing first, because the access operator ignores the \nlane\n parameter. This is because the SIMT threading is implicit in Kokkos and each SIMT thread is holding it's own data \n_data\n. Nevertheless, it is useful to implement the access operator that way to preserve a common, portable style throughout the rest of the code. An example of manipulating these types\nin a unified way with templates is given below, for the \nComplexCopy\n operation:\n\n\n// T1 must support indexing with operator()\n\n\ntemplate\ntypename\n \nT\n,\n \nint\n \nN\n,\n \n        \ntemplate\n \ntypename\n,\nint\n \nclass\n \nT1\n,\n \n        \ntemplate\n \ntypename\n,\nint\n \nclass\n \nT2\n\n\nKOKKOS_FORCEINLINE_FUNCTION\n\n\nvoid\n \nComplexCopy\n(\nT1\nT\n,\nN\n \nresult\n,\n \nconst\n \nT2\nT\n,\nN\n \nsource\n)\n\n\n{\n\n  \nKokkos\n::\nparallel_for\n(\nVectorPolicy\n(\nN\n),[\n](\nconst\n \nint\n \ni\n)\n \n{\n\n    \nresult\n(\ni\n)\n \n=\n \nsource\n(\ni\n);\n      \n  \n});\n\n\n}\n\n\n\n\n\nSuppose that \nT1\n is \nSIMDComplex\n and \nT2\n is also \nSIMDComplex\n as one would have on a CPU. Then the \nKokkos::parallel_for\n will expand into a loop, possibly decorated with \n#pragma ivdep\n and this code will copy all the actual elements of \nsource\n into \nresult\n. However, on the GPU,  if \nresult\n is of type \nGPUSIMDComplex\n and the \nExecutionPolicy\n set up the warp threads in the X-direction, this code should instead reduce to:\n\n\n{\n\n  \nresult\n.\n_data\n  \n=\n \nsource\n(\nthreadIdx\n.\nx\n);\n\n\n}\n\n\n\nin other words the routine is capable also of splitting the \nSIMDComplex\n into its lanes for the threads in the X-direction \n(and vice versa if \nsource\n and \ndestination\n are interchanged).\n\n\nSpecialization for CPU\n\n\nIn theory, these two types are sufficient for ensuring proper vectorization on both CPU and GPU. In our experiments however, we found that neither Intel nor GNU compiler could vectorize the complex operations inside the spinors properly, leading to a very poor performance. This is not a problem of Kokkos itself, it is merely the inability of compilers to efficiently vectorized complex algebra.\nWe therefore provided a template  specialization for the \nCPUSIMDComplex\n datatype which we implemented by explicitly using AVX512 intrinsics. For example, the datatype then becomes\n\n\ntemplate\n\n\nstruct\n \nCPUSIMDComplex\nfloat\n,\n8\n \n{\n\n  \nexplicit\n \nCPUSIMDComplex\nfloat\n,\n8\n()\n \n{}\n\n\n  \nunion\n \n{\n\n    \nKokkos\n::\ncomplex\nfloat\n \n_data\n[\n8\n];\n\n    \n__m512\n \n_vdata\n;\n\n  \n};\n\n\n  \n...\n\n\n};\n\n\n\n\n\nand, for example, the vectorized multiplication of two complex numbers\n\n\ntemplate\n \nKOKKOS_FORCEINLINE_FUNCTION\n\n\nvoid\n \nComplexCMadd\nfloat\n,\n8\n,\nCPUSIMDComplex\n,\nCPUSIMDComplex\n(\nCPUSIMDComplex\nfloat\n,\n8\n \nres\n,\n\n                                                         \nconst\n \nKokkos\n::\ncomplex\nfloat\n \na\n,\n\n                                                         \nconst\n \nCPUSIMDComplex\nfloat\n,\n8\n \nb\n)\n\n\n{\n \n  \n// Broadcast real and imaginary parts of \na\n into vector registers\n\n  \n__m512\n \navec_re\n \n=\n \n_mm512_set1_ps\n(\n \na\n.\nreal\n()\n \n);\n\n  \n__m512\n \navec_im\n \n=\n \n_mm512_set1_ps\n(\n \na\n.\nimag\n()\n \n);\n\n\n  \n// A vector of alternating +/- 1s;\n\n  \n__m512\n \nsgnvec\n \n=\n \n_mm512_set_ps\n(\n \n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n);\n\n\n  \n// A permutation of b\ns real and imaginary parts, with the sign vector multiplied in.\n\n  \n__m512\n \nperm_b\n \n=\n \n_mm512_mul_ps\n(\nsgnvec\n,\n_mm512_shuffle_ps\n(\nb\n.\n_vdata\n,\nb\n.\n_vdata\n,\n0xb1\n));\n\n\n  \n// 2 FMAs.\n\n  \nres\n.\n_vdata\n \n=\n \n_mm512_fmadd_ps\n(\n \navec_re\n,\n \nb\n.\n_vdata\n,\n \nres\n.\n_vdata\n);\n\n  \nres\n.\n_vdata\n \n=\n \n_mm512_fmadd_ps\n(\n \navec_im\n,\nperm_b\n,\n \nres\n.\n_vdata\n);\n\n\n}\n\n\n\n\n\nNote that we use inter-lane shuffle operations to swap complex and imaginary parts and use vectorized FMA operations. We suspect that compilers are unable to detect the opportunity of performing those inter-lane shuffles and thus fail to properly vectorize the code. The amount of specialization employed here is contained in about 14 functions spreading across 182 lines of code. This is not a huge investment and also does not really destroy portability as most of the code is still written in a portable way.\n\n\nSpecialization for GPU\n\n\nAlthough vectorization issues are usually less severe on SIMT architectures, we ran into problems of vectorized loads and stores of complex numbers. using \nnvprof\n, we found that a load and store of a \nKokkos::complex\n instance created two transactions, i.e. one for the real and one for the imaginary part. The \nnvprof\n screenshot shown below illustrates this issue.\n\n\n\n\nThese \nsplit-transactions\n have the potential of wasting bandwidth and thus should be avoided. A (partial) solution is to use CUDA 9 instead of CUDA 8: apparently, the compiler improved so that it is able to remove at least all the split stores, but not all split loads.\nTo improve that situation, we decide to write our own complex class which we derived from the CUDA \nfloat2\n datatype (this is for single precision, one could use \ndouble2\n for double precision). By doing so, we make sure that the data member has correct alignment properties and thus helps the compiler to issue optimized store and load iterations. The implementation of this class is sketched below.\n\n\ntemplate\n\n\nclass\n \nGPUComplex\nfloat\n \n:\n \npublic\n \nfloat2\n \n{\n \n\npublic\n:\n\n  \nexplicit\n \nKOKKOS_INLINE_FUNCTION\n \nGPUComplex\nfloat\n()\n \n{\n\n    \nx\n \n=\n \n0.\n;\n\n    \ny\n \n=\n \n0.\n;\n\n  \n}\n\n\n  \ntemplate\ntypename\n \nT1\n,\n \ntypename\n \nT2\n\n  \nexplicit\n  \nKOKKOS_INLINE_FUNCTION\n \nGPUComplex\nfloat\n(\nconst\n \nT1\n \nre\n,\n \nconst\n \nT2\n \nim\n)\n \n{\n\n    \nx\n \n=\n \nre\n;\n\n    \ny\n \n=\n \nim\n;\n\n  \n}\n\n\n  \nexplicit\n \nKOKKOS_INLINE_FUNCTION\n \nGPUComplex\nfloat\n(\nconst\n \nfloat\n \nre\n,\n \nconst\n \nfloat\n \nim\n)\n \n{\n\n    \nx\n \n=\n \nre\n;\n \ny\n \n=\n \nim\n;\n\n  \n}\n\n\n  \ntemplate\ntypename\n \nT1\n\n  \nKOKKOS_INLINE_FUNCTION\n \nGPUComplex\nfloat\n \noperator\n=\n(\nconst\n \nGPUComplex\nT1\n \nsrc\n)\n \n{\n\n    \nx\n \n=\n \nsrc\n.\nx\n;\n\n    \ny\n \n=\n \nsrc\n.\ny\n;\n\n    \nreturn\n \n*\nthis\n;\n\n  \n}\n\n\n  \n...\n\n\n};\n\n\n\n\n\nThe part abbreviated by the ellipsis only contains further assignment or access operators, no complex math. Because of the issues with complex arithmetic in C++ mentioned above, we explicitly write those operations in terms of real and imaginary parts.\nUsing this class got rid of all uncoalesced data access issues even in CUDA 8. This can be inferred by looking at the \nnvprof\n output in which the corresponding sections are not marked as hotspots any more.\n\n\nNote that despite our improvements of complex load and store instructions, the kernel performance barely changed. This indicates that on the GPU the performance is still limited by something else, probably memory access latency.\n\n\nIndex Computations\n\n\nInvestigating this issue with \nnvprof\n indicates that performance might be impacted by significant integer calculation overhead. For example, the first screenshot below shows the instruction decomposition for the Wilson operator from the optimized \nQUDA library\n.\n\n\n\n\nIt exhibits that about 55% of all instructions are floating point instructions, there is basically no flow control and only about 28% integer arithmetic. For the Kokkos kernel \nnvprof\n shows the following picture.\n\n\n\n\nThis shows that our floating point instruction ratio is only 28%, whereas integer and control flow counts are notably higher, namely 38% and 15% respectively. We are still investigating, whether the extra integer operations arise from our own integer computations or from those of Kokkos indexing into views. If it is the latter, we may be able to further reduce these by using Kokkos' subview mechanism. Excessive integer operations can hurt performance also on KNL, where integer operations are not vectorized, and indeed on KNL best performance is typically seen when the \nSiteTable\n is operated in lookup table mode.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#kokkos-implementation", 
            "text": "In Kokkos, it is advised to use the  Kokkos::View  datatype as our data container. Therefore, the  spinor and gaugefield classes  become  template typename   ST ,   int   nspin  class   CBSpinor   {  public :  \n     // Define a type handle for the Kokkos View in this class (a Trait) \n     // dims are: site, color, spin \n     using   DataType   =    Kokkos :: View ST * [ 3 ][ nspin ], Layout , MemorySpace ; \n\n     // Get the actual Kokkos views... \n     const   DataType   GetData ()   const   {   return   _data ;   } \n     DataType   GetData   {   return   _data ;   }  private : \n     DataType   _data ;  };  // Convenient Name for the View inside a 4 spinor  template typename   ST  using   SpinorView   =   typename   CBSpinor ST , 4 :: DataType ;   template typename   GT   class   CBGaugeField   {  public : \n     // Define a type handle for the Kokkos View in this class (a Trait) \n     // dims are: site, direction, color, color   \n     using   DataType   =    Kokkos :: View GT * [ 4 ][ 3 ][ 3 ], GaugeLayout , MemorySpace ; \n\n     // Get the actual Kokkos views... \n     const   DataType   GetData ()   const   {   return   _data ;   } \n     DataType   GetData   {   return   _data ;   }  private : \n     DataType   _data ;        };  // A Handle for the View part of the Gauge Field  template typename   GT  using   GaugeView   =   typename   CBGaugeField GT :: DataType ;   In each container class, we defined a  DataType  to be an appropriate  Kokkos::View  for that object.\nNote that in the Views the site index dimension is a runtime dimension (denoted by  * ) whereas the other dimensions - color and spin - are fixed (denoted by  [const] ). Explicitly stating this is recommended by the kokkos developers because it should help the compiler to optimize the code.  Here  Layout ,  GaugeLayout  and  MemorySpace  are policies we set at compile time in \na single header file, based on architecture e.g. for KNL one would have:  using   ExecSpace   =   Kokkos :: OpenMP :: execution_space ;  using   MemorySpace   =   Kokkos :: OpenMP :: memory_space ;  using   Layout   =   Kokkos :: OpenMP :: array_layout ;   using   GaugeLayout   =   Kokkos :: OpenMP :: array_layout ;  using   NeighLayout   =   Kokkos :: OpenMP :: array_layout ;  using   ThreadExecPolicy   =   Kokkos :: TeamPolicy ExecSpace ;   whereas the appropriate settings for Single Right Hand Side Dslash for GPUs were: using   ExecSpace   =   Kokkos :: Cuda :: execution_space ;  using   MemorySpace   =   Kokkos :: Cuda :: memory_space ;  using   Layout   =   Kokkos :: Cuda :: array_layout ;  using   GaugeLayout   =   Kokkos :: Cuda :: array_layout ;  using   NeighLayout   =   Kokkos :: Cuda :: array_layout ;  using   ThreadExecPolicy   =    Kokkos :: TeamPolicy ExecSpace , Kokkos :: LaunchBounds 128 , 1 ;  \nFinally we note that when a VectorType is used (see below) the setting for  Layout ,  GaugeLayout , and  NeighLayout \nwas  Kokkos::LayoutRight  for both CPU and GPU.  Care needs to be taken whether one passes by value or reference, as in an environment involving GPUs\none cannot pass references to data on the host to the GPU (unless one uses Unified Virtual Memory -- UVM).\nAttempting to do so can lead to a run-time error. Also, we have found that one should not execute Kokkos\nparallel regions in the class constructor. Kokkos Views are small objects with a pointer potentially to memory\nin the appropriate  MemorySpace . They are cheap to copy, and it help avoiding programming errors if we can\npass the views directly to our kernels, rather than passing the wrapper  CBSpinor  objects. Hence we define\naccessors:  GetData()  for both our wrapper classes, and we also define the shorthand type aliases  SpinorView  and GaugeView  for the Kokkos views as used in spinors or gauge fields respectively.  It is common in lattice QCD to apply Dslash for both checkerboards and parities, and these are usually given\nas run-time parameters to a DSlash operator. In order to give the compiler maximum information, and to avoid\nunnecessary register spilling on GPUs we wanted to make these parameters essentially compile time. Hence we defined \na functor class  DslashFunctor  whose  operator()  implements the Dslash as discussed in  Wilson operator class .  template typename   GT ,   typename   ST ,   typename   TST ,   const   int   isign ,   const   int   target_cb  struct   DslashFunctor   {  \n   SpinorView ST   s_in ;            // These are the Kokkos::Views \n   GaugeView GT   g_in_src_cb ; \n   GaugeView GT   g_in_target_cb ; \n   SpinorView ST   s_out ; \n   int   num_sites ; \n   int   sites_per_team ; \n   SiteTable   neigh_table ; \n\n   KOKKOS_FORCEINLINE_FUNCTION \n   void   operator ()( const   TeamHandle   team )   const   { \n     const   int   start_idx   =   team . league_rank () * sites_per_team ; \n     const   int   end_idx   =   start_idx   +   sites_per_team      num_sites   ?   start_idx   +   sites_per_team   :   num_sites ; \n\n     Kokkos :: parallel_for ( Kokkos :: TeamThreadRange ( team , start_idx , end_idx ),[ = ]( const   int   site )   { \n\n       // Body as per the previous pseudocode \n       SpinorSiteView TST   res_sum ; \n       HalfSpinorSiteView TST   proj_res ; \n       HalfSpinorSiteView TST   mult_proj_res ; \n\n       for ( int   color = 0 ;   color     3 ;   ++ color )   { \n         for ( int   spin = 0 ;   spin     4 ;   ++ spin )   { \n           ComplexZero ( res_sum ( color , spin )); \n         } \n       } \n\n       // T - minus \n       KokkosProjectDir3 ST , TST , isign ( s_in ,   proj_res , \n                              neigh_table . NeighborTMinus ( site , target_cb )); \n       mult_adj_u_halfspinor GT , TST ( g_in_src_cb , proj_res , mult_proj_res , \n                              neigh_table . NeighborTMinus ( site , target_cb ), 3 ); \n       KokkosRecons23Dir3 TST , isign ( mult_proj_res , res_sum ); \n\n       // ... other directions.... \n     }); \n   }  };   and one can see that the functor operates directly on  Kokkos::View  data members and that\napart from the ``site``` other indexing parameters such as target_cb are compile time through templates.  To still allow choices of  target_cb  and  isign  (whether to apply Dslash or its hermitian conjugate)\nthe top-level  Dslash  class looks like:  template typename   GT ,   typename   ST ,   typename   TST  class   KokkosDslash   {  public : \n   void   operator ()( const   CBSpinor ST , 4   spinor_in , \n                   const   GaugeField GT   gauge_in ,     // contains CBGaugeField for both cb-s \n                   CBSpinor ST , 4   spinor_out , \n                   int   plus_minus )   const \n   { \n     int   source_cb   =   spinor_in . GetCB (); \n     int   target_cb   =   ( source_cb   ==   EVEN )   ?   ODD   :   EVEN ; \n     const   SpinorView ST   s_in   =   spinor_in . GetData (); \n     const   GaugeView GT   g_in_src_cb   =   ( gauge_in ( source_cb )). GetData (); \n     const   GaugeView GT    g_in_target_cb   =   ( gauge_in ( target_cb )). GetData (); \n     SpinorView ST   s_out   =   spinor_out_out . GetData (); \n\n     ... \n\n     // Create a Team policy, and inform it of the Vector Length \n     ThreadExecPolicy   policy ( num_sites / _sites_per_team ,   Kokkos :: AUTO (), Veclen ST :: value ); \n\n     // Ugly if-else to turn plus_minus   target_cb into  \n     // constant template arguments \n     if (   plus_minus   ==   1   )   { \n       if   ( target_cb   ==   0   )   { \n         // Instantiate appropriate functor and construct \n         // All view initializations will be done by copy \n         // the values of plus_minus (1) and target_cb (0) \n         // are made explicit and compile time in the template \n         // arguments \n         DslashFunctor GT , ST , TST , 1 , 0   f   =   { s_in ,   g_in_src_cb ,   g_in_target_cb ,   s_out , num_sites ,   _sites_per_team , _neigh_table }; \n\n           // Dispatch the thread Teams                      \n           Kokkos :: parallel_for ( policy ,   f );   // Outer Lambda  \n       } \n       else   { \n         ...   // Instantiate DslashFunctor for target_cb=1   Dispatch         \n       } \n     } \n     else   {  \n       // plus_minus == -1 case, target_cb=0,1 cases \n       ... \n     } \n   }  };   In the above, the GPU Threads in the  \\(X\\)  direction are created, when we call  Kokkos::parallel_for  with the  ThreadExecPolicy , to which we have given a vector length through the  Veclen ST ::veclen  type trait. The nested  if-else  clauses turn run-time\nparameters into template parameters. Finally, the member data of the  DslashFunctor  are initialized by copy.  Note that since we linearized the site index, we need to compute the neighboring site indices manually. On architectures with poor integer arithmetic performance this might lead to a significant performance penalty. Therefore, we implement a  SiteTable  class with\nmember functions like  NeighborTMinus(site,target_cb)  to give the linear site indices of the neighbors. The instance of this site table is the  _neigh_table  member in the  KokkosDslash  class and is used to initialize the  neigh_table  member of the  DslashFunctor . Our implementation is such that  SiteTable  either holds a pre-computed neighbor table or computes the site neighbor for a given direction on the fly (a choice made at configuration   compile time using  #ifdef ). In our performance measurements we use the implementation which gives the best performance for a given architecture.", 
            "title": "Kokkos Implementation"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#complex-numbers-and-c", 
            "text": "We want to emphasize a subtle performance pitfall when it comes to complex numbers in C++. The language standards inhibit the compiler to efficiently optimize operations such as  c   +=   a   *   b   when  a ,  b  and  c  are complex numbers. Naively, this expression could be expanded into  re ( c )   +=   re ( a )   *   re ( b )   -   im ( a )   *   im ( b )  im ( c )   +=   re ( a )   *   im ( b )   +   im ( a )   *   re ( b )   where  re(.), im(.)  denote the real and imaginary part of its argument respectively. This expression can nicely be packed into a total of four FMA operations per line. However, in the simplified form above which is usually used in context of operator overloading, the compiler would have to evaluate the right hand side first and then sum the result into  c . This is much less efficient since in that case, only two FMA as well as two multiplications and additions could be used. One has to keep that in mind when doing complex algebra in C++. In many cases it is better to inline code and avoid otherwise useful operator overloading techniques for complex algebra.", 
            "title": "Complex Numbers and C++"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#ensuring-vectorization", 
            "text": "Vectorization in Kokkos is achieved by a two-level  nested parallelism , where the outer loop spawns threads (pthreads, OpenMP-threads) on the CPU and threads in CUDA-block y-direction on the GPU. The inner loop then applies vectorization pragmas on the CPU or spawns threads in x-direction on the GPU. This is where we have to show some awareness of architectural differences: the spinor work type  TST  needs to be a scalar type on the GPU and a vector type on the CPU. Hence we declare the following types on GPU and CPU respectively  template typename   T , N  struct   CPUSIMDComplex   { \n   Kokkos :: complex T   _data [ N ]; \n\n   T   operator ()( int   lane )   { \n     return   _data [ lane ]; \n   } \n\n   ...  };  template typename   T , N  struct   GPUSIMDComplex   { \n   Kokkos :: complex T   _data ; \n\n   T   operator ()( int   lane )   { \n     return   _data ; \n   } \n\n   ...  };   The latter construct might look confusing first, because the access operator ignores the  lane  parameter. This is because the SIMT threading is implicit in Kokkos and each SIMT thread is holding it's own data  _data . Nevertheless, it is useful to implement the access operator that way to preserve a common, portable style throughout the rest of the code. An example of manipulating these types\nin a unified way with templates is given below, for the  ComplexCopy  operation:  // T1 must support indexing with operator()  template typename   T ,   int   N ,  \n         template   typename , int   class   T1 ,  \n         template   typename , int   class   T2  KOKKOS_FORCEINLINE_FUNCTION  void   ComplexCopy ( T1 T , N   result ,   const   T2 T , N   source )  { \n   Kokkos :: parallel_for ( VectorPolicy ( N ),[ ]( const   int   i )   { \n     result ( i )   =   source ( i );       \n   });  }   Suppose that  T1  is  SIMDComplex  and  T2  is also  SIMDComplex  as one would have on a CPU. Then the  Kokkos::parallel_for  will expand into a loop, possibly decorated with  #pragma ivdep  and this code will copy all the actual elements of  source  into  result . However, on the GPU,  if  result  is of type  GPUSIMDComplex  and the  ExecutionPolicy  set up the warp threads in the X-direction, this code should instead reduce to:  { \n   result . _data    =   source ( threadIdx . x );  }  \nin other words the routine is capable also of splitting the  SIMDComplex  into its lanes for the threads in the X-direction \n(and vice versa if  source  and  destination  are interchanged).", 
            "title": "Ensuring Vectorization"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#specialization-for-cpu", 
            "text": "In theory, these two types are sufficient for ensuring proper vectorization on both CPU and GPU. In our experiments however, we found that neither Intel nor GNU compiler could vectorize the complex operations inside the spinors properly, leading to a very poor performance. This is not a problem of Kokkos itself, it is merely the inability of compilers to efficiently vectorized complex algebra.\nWe therefore provided a template  specialization for the  CPUSIMDComplex  datatype which we implemented by explicitly using AVX512 intrinsics. For example, the datatype then becomes  template  struct   CPUSIMDComplex float , 8   { \n   explicit   CPUSIMDComplex float , 8 ()   {} \n\n   union   { \n     Kokkos :: complex float   _data [ 8 ]; \n     __m512   _vdata ; \n   }; \n\n   ...  };   and, for example, the vectorized multiplication of two complex numbers  template   KOKKOS_FORCEINLINE_FUNCTION  void   ComplexCMadd float , 8 , CPUSIMDComplex , CPUSIMDComplex ( CPUSIMDComplex float , 8   res , \n                                                          const   Kokkos :: complex float   a , \n                                                          const   CPUSIMDComplex float , 8   b )  {  \n   // Broadcast real and imaginary parts of  a  into vector registers \n   __m512   avec_re   =   _mm512_set1_ps (   a . real ()   ); \n   __m512   avec_im   =   _mm512_set1_ps (   a . imag ()   ); \n\n   // A vector of alternating +/- 1s; \n   __m512   sgnvec   =   _mm512_set_ps (   1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 ); \n\n   // A permutation of b s real and imaginary parts, with the sign vector multiplied in. \n   __m512   perm_b   =   _mm512_mul_ps ( sgnvec , _mm512_shuffle_ps ( b . _vdata , b . _vdata , 0xb1 )); \n\n   // 2 FMAs. \n   res . _vdata   =   _mm512_fmadd_ps (   avec_re ,   b . _vdata ,   res . _vdata ); \n   res . _vdata   =   _mm512_fmadd_ps (   avec_im , perm_b ,   res . _vdata );  }   Note that we use inter-lane shuffle operations to swap complex and imaginary parts and use vectorized FMA operations. We suspect that compilers are unable to detect the opportunity of performing those inter-lane shuffles and thus fail to properly vectorize the code. The amount of specialization employed here is contained in about 14 functions spreading across 182 lines of code. This is not a huge investment and also does not really destroy portability as most of the code is still written in a portable way.", 
            "title": "Specialization for CPU"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#specialization-for-gpu", 
            "text": "Although vectorization issues are usually less severe on SIMT architectures, we ran into problems of vectorized loads and stores of complex numbers. using  nvprof , we found that a load and store of a  Kokkos::complex  instance created two transactions, i.e. one for the real and one for the imaginary part. The  nvprof  screenshot shown below illustrates this issue.   These  split-transactions  have the potential of wasting bandwidth and thus should be avoided. A (partial) solution is to use CUDA 9 instead of CUDA 8: apparently, the compiler improved so that it is able to remove at least all the split stores, but not all split loads.\nTo improve that situation, we decide to write our own complex class which we derived from the CUDA  float2  datatype (this is for single precision, one could use  double2  for double precision). By doing so, we make sure that the data member has correct alignment properties and thus helps the compiler to issue optimized store and load iterations. The implementation of this class is sketched below.  template  class   GPUComplex float   :   public   float2   {   public : \n   explicit   KOKKOS_INLINE_FUNCTION   GPUComplex float ()   { \n     x   =   0. ; \n     y   =   0. ; \n   } \n\n   template typename   T1 ,   typename   T2 \n   explicit    KOKKOS_INLINE_FUNCTION   GPUComplex float ( const   T1   re ,   const   T2   im )   { \n     x   =   re ; \n     y   =   im ; \n   } \n\n   explicit   KOKKOS_INLINE_FUNCTION   GPUComplex float ( const   float   re ,   const   float   im )   { \n     x   =   re ;   y   =   im ; \n   } \n\n   template typename   T1 \n   KOKKOS_INLINE_FUNCTION   GPUComplex float   operator = ( const   GPUComplex T1   src )   { \n     x   =   src . x ; \n     y   =   src . y ; \n     return   * this ; \n   } \n\n   ...  };   The part abbreviated by the ellipsis only contains further assignment or access operators, no complex math. Because of the issues with complex arithmetic in C++ mentioned above, we explicitly write those operations in terms of real and imaginary parts.\nUsing this class got rid of all uncoalesced data access issues even in CUDA 8. This can be inferred by looking at the  nvprof  output in which the corresponding sections are not marked as hotspots any more.  Note that despite our improvements of complex load and store instructions, the kernel performance barely changed. This indicates that on the GPU the performance is still limited by something else, probably memory access latency.", 
            "title": "Specialization for GPU"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#index-computations", 
            "text": "Investigating this issue with  nvprof  indicates that performance might be impacted by significant integer calculation overhead. For example, the first screenshot below shows the instruction decomposition for the Wilson operator from the optimized  QUDA library .   It exhibits that about 55% of all instructions are floating point instructions, there is basically no flow control and only about 28% integer arithmetic. For the Kokkos kernel  nvprof  shows the following picture.   This shows that our floating point instruction ratio is only 28%, whereas integer and control flow counts are notably higher, namely 38% and 15% respectively. We are still investigating, whether the extra integer operations arise from our own integer computations or from those of Kokkos indexing into views. If it is the latter, we may be able to further reduce these by using Kokkos' subview mechanism. Excessive integer operations can hurt performance also on KNL, where integer operations are not vectorized, and indeed on KNL best performance is typically seen when the  SiteTable  is operated in lookup table mode.", 
            "title": "Index Computations"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/", 
            "text": "Results\n\n\nKokkos\n\n\nIn order to assess the performance on KNL and GPU architectures, we compare against the highly optimized libraries \nQPhiX\n and \nQUDA\n respectively. Those two codes should set the upper bar of what can be achieved on the corresponding architectures for the given problem. Note that these libraries can additionally employ some algorithmic improvements which we did not use in our simple test case. However, it is possible to switch most of these optimizations off to allow for better comparisons with our portable code. Two optimization features of the libraries which we have not turned off were i) in the case of QPhiX, cache blocking remains enabled and ii) QUDA performs a wide-ranging autotuning as regards block sizes, and shared memory use to identify its optimal launch parameters. Both these features are integral to these libraries and could not be turned off. The vectorization in both frameworks is performed over lattice sites and not over multiple right hand sides as in our test case. However, QUDA supports a multiple-right hand side implementation which we can compare against, but must bear in mind that it does not rely on vectorization over right hand sides like our implementation. However, it still benefits from gauge field reuse. Due to it not vectorizing over the right hand sides, but over lattice sites, it may have have a different available space of kernel launch parameters and an entirely different performance profile from our implementation in the MRHS case.\n\n\nOn the CPU, we additionally compare our code to a plain C++ as well as to a legacy SSE-optimized Wilson dslash operator, both available through the \nChroma framework\n. Those two codes should act as some kind of lower bar for our performance comparisons. Because of different vectorization behavior in our kokkos dslash. we split our results summary into two parts, i.e. one for the single and one for the multiple right-hand-sites case.\n\n\nSingle Right-Hand-Side (SRHS)\n\n\nThe performance comparison for this case is shown below. These performance measurements were carried out using a lattice of size V=32\n4\n sites, in single precision (\nfloat\n) arithmetic. The KNL results used the KNL nodes from Cori, and the final KNL numbers came from nodes booted into \nflat-quad\n mode and pinned into MCDRAM using \nnumactl -m 1\n. Our GPU results were carried out on the nodes of SummitDev at OLCF, and on in-house resources at NVIDIA by KateClark. Kate found that using CUDA-9RC gave significantly higher performances than CUDA-8, and so we quote the CUDA-9 numbers here.\n\n\n\n\nRecall that in these tests, the Wilson operator acts on a single right-hand-side and we do not use a special, vectorization-friendly data layout such as the virtual node layout of Grid or the tiled layout of QPhiX. This implies that the compiler has to find vectorization opportunities on its own. Our Kokkos code with CPU-backend seems to fail in this respect, whereas for the GPU-backend the SIMT parallelization seems to work, as the code achieves about 80% of the QUDA performance number. It is interesting that the plain \ncpp_wilson_dslash\n, which does neither use sophisticated data layouts nor explicit vectorization statements, does better in this respect. One difference between our code, and \ncpp_wilson_dslash\n is our reliance on the \ncomplex\n datatype in the Kokkos test-code, whereas in \ncpp_wilson_dslash\n this is an explicit array dimension of length 2.  We suspect that the compiler has problems dealing with the templated \nKokkos::complex\n data-type, however, we did a quick and dirty experiment on the CPU replacing \nKokkos::complex\n with \nstd::complex\n from C++. This did not improve things, and so we do not think there is anything Kokkos specific with this issue.  Employing a vectorization and cache friendly data layout, one might be able to drive the Kokkos performance up to the QPhiX value. However, this has to be done carefully in the sense that certain data layouts might help on one architecture but might lead to performance penalties on another architecture. It is worthwhile exploring this aspect in the future.\n\n\nMultiple Right-Hand-Sides (MRHS)\n\n\nThe performance results for this case are displayed below. In this case on the CPU we chose to use 8 right hand sides (8 floating point complex numbers = 16 floats = vector register length), and correspondingly we lowered the volume by a factor of 8 to compensate to V=16\n3\nx32 sites.\nIn our subsequent GPU tests, we kept this same volume, but increased the number of right hand sides to 16 which is the natural warp size (and hence likely 'SIMD' size) for GPUs.\n\n\n\n\nIn this case, we chose a data layout which encourages vectorization over the right-hand-sides (Kokkos Dslash unspecialized) or explicitly do so by using vector intrinsics (Kokkos Dslash - AVX512 specialized) on CPU and (Kokkos Dslash) on GPU. We see a big improvement in the AVX512 specialized case, which suggests that automatic vectorization fails for our Kokkos code. Therefore, currently we encourage using SIMD datatypes or making use of explicit vectorization through intrinsics when using Kokkos. In our case, this required only a small amount of specialization and with this modification the Kokkos code was able to beat the (SRHS) QPhiX benchmark. This was really very encouraging, as it suggested that the impediments to performance did not actually arise from Kokkos itself, but mostly out of the behavior of compilers. \n\n\nOn the GPU however, Kokkos' performance is very low compared to the QUDA benchmark. While Kokkos was able to achieve 80% of the QUDA performance for the SRHS case, it only achieves 43% for the MRHS case. More importantly, the performance for SRHS and MRHS are are similar on the GPU (SRHS ran at 610 GFLOPS, MRHS ran at 674 GFLOPS). This significant performance difference between the QUDA and Kokkos MRHS codes might be attributed to algorithmic differences between the two implementations. As noted earlier, in the QUDA code SIMT `vectorization' is effectively performed over lattice sites, achieved by using a specific layout and QUDA uses a wide ranging performance autotuning mechanism. \n\n\nWe also compared the performances of the Kokkos and QUDA Dslash implementation on both Pascal and Volta architectures -- the measurements were carried out by Kate Clark of NVIDIA using internal NVIDIA V100 hardware.\n\n\n\n\nThe plot shows that our Kokkos kernel achieves significantly bigger fractions of the QUDA performance on Volta than on Pascal. On Pascal output from the Visual Profiler indicated that our Kokkos code was bound by memory and instruction latencies, and that compared to QUDA, we had a relatively lower percentage of Floating point, and higher percentages of Integer and Control instructions than the QUDA benchmark. Volta's design brings significant latency reductions compared to Pascal, which makes it better able to run latency bound codes such as our MRHS Dslash implementation with Kokkos. In any case even though the Kokkos MRHS performance slightly exceeds the Kokkos SRHS performance on the GPUs and the Kokkos MRHS performance on P100 slightly exceeds Kokkos MRHS performance on KNL, nonetheless the GPU Kokkos MRHS implementation does not exploit the performance potential of the GPU to the same extent that the QUDA Library does. Whether this can actually be achieved will require future work.\n\n\nSummary\n\n\nWe could show that Kokkos can be used to write performant Dslash implementations for both CPU and GPU architectures. We summarize our performance achievement in the table below:\n\n\n\n\n\n\n\n\n\n\nGPU\n\n\nKNL\n\n\n\n\n\n\n\n\n\n\nSRHS\n\n\nGood\n\n\nPoor\n\n\n\n\n\n\nMRHS\n\n\nOK\n\n\nGood\n\n\n\n\n\n\n\n\nHowever, it is likely that the poor behavior in the case of KNL SRHS is not due to Kokkos, but to compiler being unable to find sufficient (if any) vectorization opportunity, which also will cause issues for memory traffic, i.e. if data is not read/written using aligned vector loads/stores. The underlying cause for the low performance of MRHS compared to QUDA on GPUs is still being investigated, for example, it is still up in the air as to whether latencies could be reduced by reducing indexing arithmetic (e.g. with Kokkos subviews) or memory latencies could be reduced, e.g. by loading the gauge field into shared memory. It is likely that on the KNL, starting from the multi-right hand side Dslash we could probably move to a better, vectorized SRHS operator, for example by adopting the virtual node layout of Grid. It is also likely that such an operator would have a reasonable (OK) performance on the GPUs based on the measurements made with the MRHS operator on GPUs. However, it is not immediately clear whether it would outperform the most naive implementation for SRHS which achieves 98% of QUDA performance on Volta. This leaves us with a more difficult performance portability question: Even if the code itself is performance portable (similar absolute GFLOPS performance on both CPU and GPU), in terms of extracting maximum available performance from a given architecture, is the algorithm itself performance portable?", 
            "title": "Results"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/#results", 
            "text": "", 
            "title": "Results"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/#kokkos", 
            "text": "In order to assess the performance on KNL and GPU architectures, we compare against the highly optimized libraries  QPhiX  and  QUDA  respectively. Those two codes should set the upper bar of what can be achieved on the corresponding architectures for the given problem. Note that these libraries can additionally employ some algorithmic improvements which we did not use in our simple test case. However, it is possible to switch most of these optimizations off to allow for better comparisons with our portable code. Two optimization features of the libraries which we have not turned off were i) in the case of QPhiX, cache blocking remains enabled and ii) QUDA performs a wide-ranging autotuning as regards block sizes, and shared memory use to identify its optimal launch parameters. Both these features are integral to these libraries and could not be turned off. The vectorization in both frameworks is performed over lattice sites and not over multiple right hand sides as in our test case. However, QUDA supports a multiple-right hand side implementation which we can compare against, but must bear in mind that it does not rely on vectorization over right hand sides like our implementation. However, it still benefits from gauge field reuse. Due to it not vectorizing over the right hand sides, but over lattice sites, it may have have a different available space of kernel launch parameters and an entirely different performance profile from our implementation in the MRHS case.  On the CPU, we additionally compare our code to a plain C++ as well as to a legacy SSE-optimized Wilson dslash operator, both available through the  Chroma framework . Those two codes should act as some kind of lower bar for our performance comparisons. Because of different vectorization behavior in our kokkos dslash. we split our results summary into two parts, i.e. one for the single and one for the multiple right-hand-sites case.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/#single-right-hand-side-srhs", 
            "text": "The performance comparison for this case is shown below. These performance measurements were carried out using a lattice of size V=32 4  sites, in single precision ( float ) arithmetic. The KNL results used the KNL nodes from Cori, and the final KNL numbers came from nodes booted into  flat-quad  mode and pinned into MCDRAM using  numactl -m 1 . Our GPU results were carried out on the nodes of SummitDev at OLCF, and on in-house resources at NVIDIA by KateClark. Kate found that using CUDA-9RC gave significantly higher performances than CUDA-8, and so we quote the CUDA-9 numbers here.   Recall that in these tests, the Wilson operator acts on a single right-hand-side and we do not use a special, vectorization-friendly data layout such as the virtual node layout of Grid or the tiled layout of QPhiX. This implies that the compiler has to find vectorization opportunities on its own. Our Kokkos code with CPU-backend seems to fail in this respect, whereas for the GPU-backend the SIMT parallelization seems to work, as the code achieves about 80% of the QUDA performance number. It is interesting that the plain  cpp_wilson_dslash , which does neither use sophisticated data layouts nor explicit vectorization statements, does better in this respect. One difference between our code, and  cpp_wilson_dslash  is our reliance on the  complex  datatype in the Kokkos test-code, whereas in  cpp_wilson_dslash  this is an explicit array dimension of length 2.  We suspect that the compiler has problems dealing with the templated  Kokkos::complex  data-type, however, we did a quick and dirty experiment on the CPU replacing  Kokkos::complex  with  std::complex  from C++. This did not improve things, and so we do not think there is anything Kokkos specific with this issue.  Employing a vectorization and cache friendly data layout, one might be able to drive the Kokkos performance up to the QPhiX value. However, this has to be done carefully in the sense that certain data layouts might help on one architecture but might lead to performance penalties on another architecture. It is worthwhile exploring this aspect in the future.", 
            "title": "Single Right-Hand-Side (SRHS)"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/#multiple-right-hand-sides-mrhs", 
            "text": "The performance results for this case are displayed below. In this case on the CPU we chose to use 8 right hand sides (8 floating point complex numbers = 16 floats = vector register length), and correspondingly we lowered the volume by a factor of 8 to compensate to V=16 3 x32 sites.\nIn our subsequent GPU tests, we kept this same volume, but increased the number of right hand sides to 16 which is the natural warp size (and hence likely 'SIMD' size) for GPUs.   In this case, we chose a data layout which encourages vectorization over the right-hand-sides (Kokkos Dslash unspecialized) or explicitly do so by using vector intrinsics (Kokkos Dslash - AVX512 specialized) on CPU and (Kokkos Dslash) on GPU. We see a big improvement in the AVX512 specialized case, which suggests that automatic vectorization fails for our Kokkos code. Therefore, currently we encourage using SIMD datatypes or making use of explicit vectorization through intrinsics when using Kokkos. In our case, this required only a small amount of specialization and with this modification the Kokkos code was able to beat the (SRHS) QPhiX benchmark. This was really very encouraging, as it suggested that the impediments to performance did not actually arise from Kokkos itself, but mostly out of the behavior of compilers.   On the GPU however, Kokkos' performance is very low compared to the QUDA benchmark. While Kokkos was able to achieve 80% of the QUDA performance for the SRHS case, it only achieves 43% for the MRHS case. More importantly, the performance for SRHS and MRHS are are similar on the GPU (SRHS ran at 610 GFLOPS, MRHS ran at 674 GFLOPS). This significant performance difference between the QUDA and Kokkos MRHS codes might be attributed to algorithmic differences between the two implementations. As noted earlier, in the QUDA code SIMT `vectorization' is effectively performed over lattice sites, achieved by using a specific layout and QUDA uses a wide ranging performance autotuning mechanism.   We also compared the performances of the Kokkos and QUDA Dslash implementation on both Pascal and Volta architectures -- the measurements were carried out by Kate Clark of NVIDIA using internal NVIDIA V100 hardware.   The plot shows that our Kokkos kernel achieves significantly bigger fractions of the QUDA performance on Volta than on Pascal. On Pascal output from the Visual Profiler indicated that our Kokkos code was bound by memory and instruction latencies, and that compared to QUDA, we had a relatively lower percentage of Floating point, and higher percentages of Integer and Control instructions than the QUDA benchmark. Volta's design brings significant latency reductions compared to Pascal, which makes it better able to run latency bound codes such as our MRHS Dslash implementation with Kokkos. In any case even though the Kokkos MRHS performance slightly exceeds the Kokkos SRHS performance on the GPUs and the Kokkos MRHS performance on P100 slightly exceeds Kokkos MRHS performance on KNL, nonetheless the GPU Kokkos MRHS implementation does not exploit the performance potential of the GPU to the same extent that the QUDA Library does. Whether this can actually be achieved will require future work.", 
            "title": "Multiple Right-Hand-Sides (MRHS)"
        }, 
        {
            "location": "/case_studies/qcd/results_summary/#summary", 
            "text": "We could show that Kokkos can be used to write performant Dslash implementations for both CPU and GPU architectures. We summarize our performance achievement in the table below:      GPU  KNL      SRHS  Good  Poor    MRHS  OK  Good     However, it is likely that the poor behavior in the case of KNL SRHS is not due to Kokkos, but to compiler being unable to find sufficient (if any) vectorization opportunity, which also will cause issues for memory traffic, i.e. if data is not read/written using aligned vector loads/stores. The underlying cause for the low performance of MRHS compared to QUDA on GPUs is still being investigated, for example, it is still up in the air as to whether latencies could be reduced by reducing indexing arithmetic (e.g. with Kokkos subviews) or memory latencies could be reduced, e.g. by loading the gauge field into shared memory. It is likely that on the KNL, starting from the multi-right hand side Dslash we could probably move to a better, vectorized SRHS operator, for example by adopting the virtual node layout of Grid. It is also likely that such an operator would have a reasonable (OK) performance on the GPUs based on the measurements made with the MRHS operator on GPUs. However, it is not immediately clear whether it would outperform the most naive implementation for SRHS which achieves 98% of QUDA performance on Volta. This leaves us with a more difficult performance portability question: Even if the code itself is performance portable (similar absolute GFLOPS performance on both CPU and GPU), in terms of extracting maximum available performance from a given architecture, is the algorithm itself performance portable?", 
            "title": "Summary"
        }, 
        {
            "location": "/case_studies/nek/", 
            "text": "Analysis of Nekbone Performance and Portability\n\n\nIntroduction\n\n\nNekbone is a mini-app derived from the \nNek5000\n CFD code which is a high order, incompressible Navier- Stokes CFD solver based on the spectral element method. It exposes the principal computational kernels of Nek5000 to reveal the essential elements of the algorithmic-architectural coupling that are pertinent to Nek5000.\n\n\nNekbone solves a standard Poisson equation in a 3D box domain with a block spatial domain decomposition among MPI ranks. The volume within a rank is then partitioned into high-order quadrilateral spectral elements. The solution phase consists of conjugate gradient iterations that invoke the main computational kernel which performs operations in an element-by-element fashion. The code is written in Fortran and C, where C routines are used for the nearest neighbor communication and the rest of the routines are in Fortran. It uses hybrid parallelism implemented with MPI and OpenMP.\n\n\nDescription\n\n\nNekbone solves a standard Poisson equation using the spectral element method with an iterative conjugate gradient solver with a simple preconditioner. The computational domain is partitioned into high-order quadrilateral elements. Based on the number of elements, number of processors, and the parameters of a test run, Nekbone allows a decomposition that is either a 1-dimensional array of 3D elements, or a 3-dimensional box of 3D elements. The benchmark is highly scalable and can accommodate a wide range of problem sizes, specified by setting the number of spectral elements per rank and the polynomial order of the elements by editing the appropriate build and input files.\n\n\nThe benchmark consists of a setup phase and a solution phase. The solution phase consists of conjugate gradient iterations that call the main computational kernel, which performs a matrix vector multiplication operation in an element-by-element fashion.  Overall each iteration consists of vector operations, matrix-matrix multiply operations, nearest-neighbor communication, and MPI Allreduce operations. The benchmark is written in Fortran and C, where C routines are used for the nearest neighbor communication and the rest of the compute kernel routines are in Fortran. Note that the CORAL version of the benchmark is implemented using OpenMP and MPI and may be run with a varied mixture of threads and processes.\n\n\nThe benchmark is intrinsically well load balanced, with each process having the same number of spectral elements and therefore the same amount of computational work.  Communication consists of nearest neighbor point-to-point communication with up to 26 surrounding processes and MPI Allreduce operations. While the amount of data communicated with neighboring processes can vary somewhat between processes, depending on proximity to the domain boundaries, on most systems the effects of this imbalance has been observed to be minimal. For systems where MPI reduction operations scale and point-to-point communications are non-interfering between network nodes the performance of the benchmark has been observed to scale nearly linearly with increasing number of nodes across several orders of magnitude. At the node level elements are distributed across threads and communication gather-scatter operations are performed cooperatively between threads. Good scaling to ten\u2019s of OpenMP threads has been observed when elements are distributes evenly across threads and times for OpenMP synchronization operations in the gather-scatter and MPI reduction operations are not excessive.\n\n\nStructure\n\n\nThe diagram below shows a schematic view of the Nekbone solver. While Nekbone is implemented with using MPI and is highly scalable the investigation of performance and portability across node architecture is being performed using a single node build that makes no MPI calls, this serves to isolate the impact of node architecture from network effects. The disabled routines in the single node build are shown as crossed out in the figure.\n\n\n\n\nImplementations\n\n\nComparisons were performed using an OpenMP implementation on the CPU and an OpenACC implementation on the GPU. Further details will be added describing the code modifications but in both cases modifications were not significant, consisting primarily of the addition of pragmas. The OpenMP implementation utilized coarse grained parallelism wherein only a single parallel region is utilized spanning the entire solver and work is apportioned to threads by a static mapping of spectral elements to threads. The OpenACC implementation minimizes data movement between the CPU and GPU by keeping all of the data resident on the GPU.\n\n\nPerformance\n\n\nThe figure below compares the performance of Nekbone on the Intel Xeon Phi \"Knights Landing\" with the performance on the NVIDIA Tesla P100 GPU. On both platforms the same set of cases were run, where the number of spectral elements was increased from 1 to 2048. On the Knights Landing processor OpenMP was utilized to enable thread parallelism, while on the P100 OpenACC was utilized to enable execution on the GPU. On both architectures a minimum problem size was required to saturate the performance on the node, with the GPU requiring a significantly larger problem 32-64x larger than the CPU. On the CPU several implementation of the floating point intensive small matrix multiply kernels were tested, the \"naive\" matrix multiply yielded the worst performance. Notable improvements were seen when the inner matrix multiply loop was handle unrolled, noted as the \"unrolled\" case in the figure. Finally the best CPU performance was seen utilizing customized AVX512 assembly routines from the XSMM library. On the GPU a \"naive\" matrix multiply routine was implemented, which outperforms the naive CPU code at high degrees of freedom, but lags the optimized CPU implementations. Further work on optimized GPU implementations is ongoing and is expected to yield improved GPU performance.", 
            "title": "NekBone"
        }, 
        {
            "location": "/case_studies/nek/#analysis-of-nekbone-performance-and-portability", 
            "text": "", 
            "title": "Analysis of Nekbone Performance and Portability"
        }, 
        {
            "location": "/case_studies/nek/#introduction", 
            "text": "Nekbone is a mini-app derived from the  Nek5000  CFD code which is a high order, incompressible Navier- Stokes CFD solver based on the spectral element method. It exposes the principal computational kernels of Nek5000 to reveal the essential elements of the algorithmic-architectural coupling that are pertinent to Nek5000.  Nekbone solves a standard Poisson equation in a 3D box domain with a block spatial domain decomposition among MPI ranks. The volume within a rank is then partitioned into high-order quadrilateral spectral elements. The solution phase consists of conjugate gradient iterations that invoke the main computational kernel which performs operations in an element-by-element fashion. The code is written in Fortran and C, where C routines are used for the nearest neighbor communication and the rest of the routines are in Fortran. It uses hybrid parallelism implemented with MPI and OpenMP.", 
            "title": "Introduction"
        }, 
        {
            "location": "/case_studies/nek/#description", 
            "text": "Nekbone solves a standard Poisson equation using the spectral element method with an iterative conjugate gradient solver with a simple preconditioner. The computational domain is partitioned into high-order quadrilateral elements. Based on the number of elements, number of processors, and the parameters of a test run, Nekbone allows a decomposition that is either a 1-dimensional array of 3D elements, or a 3-dimensional box of 3D elements. The benchmark is highly scalable and can accommodate a wide range of problem sizes, specified by setting the number of spectral elements per rank and the polynomial order of the elements by editing the appropriate build and input files.  The benchmark consists of a setup phase and a solution phase. The solution phase consists of conjugate gradient iterations that call the main computational kernel, which performs a matrix vector multiplication operation in an element-by-element fashion.  Overall each iteration consists of vector operations, matrix-matrix multiply operations, nearest-neighbor communication, and MPI Allreduce operations. The benchmark is written in Fortran and C, where C routines are used for the nearest neighbor communication and the rest of the compute kernel routines are in Fortran. Note that the CORAL version of the benchmark is implemented using OpenMP and MPI and may be run with a varied mixture of threads and processes.  The benchmark is intrinsically well load balanced, with each process having the same number of spectral elements and therefore the same amount of computational work.  Communication consists of nearest neighbor point-to-point communication with up to 26 surrounding processes and MPI Allreduce operations. While the amount of data communicated with neighboring processes can vary somewhat between processes, depending on proximity to the domain boundaries, on most systems the effects of this imbalance has been observed to be minimal. For systems where MPI reduction operations scale and point-to-point communications are non-interfering between network nodes the performance of the benchmark has been observed to scale nearly linearly with increasing number of nodes across several orders of magnitude. At the node level elements are distributed across threads and communication gather-scatter operations are performed cooperatively between threads. Good scaling to ten\u2019s of OpenMP threads has been observed when elements are distributes evenly across threads and times for OpenMP synchronization operations in the gather-scatter and MPI reduction operations are not excessive.", 
            "title": "Description"
        }, 
        {
            "location": "/case_studies/nek/#structure", 
            "text": "The diagram below shows a schematic view of the Nekbone solver. While Nekbone is implemented with using MPI and is highly scalable the investigation of performance and portability across node architecture is being performed using a single node build that makes no MPI calls, this serves to isolate the impact of node architecture from network effects. The disabled routines in the single node build are shown as crossed out in the figure.", 
            "title": "Structure"
        }, 
        {
            "location": "/case_studies/nek/#implementations", 
            "text": "Comparisons were performed using an OpenMP implementation on the CPU and an OpenACC implementation on the GPU. Further details will be added describing the code modifications but in both cases modifications were not significant, consisting primarily of the addition of pragmas. The OpenMP implementation utilized coarse grained parallelism wherein only a single parallel region is utilized spanning the entire solver and work is apportioned to threads by a static mapping of spectral elements to threads. The OpenACC implementation minimizes data movement between the CPU and GPU by keeping all of the data resident on the GPU.", 
            "title": "Implementations"
        }, 
        {
            "location": "/case_studies/nek/#performance", 
            "text": "The figure below compares the performance of Nekbone on the Intel Xeon Phi \"Knights Landing\" with the performance on the NVIDIA Tesla P100 GPU. On both platforms the same set of cases were run, where the number of spectral elements was increased from 1 to 2048. On the Knights Landing processor OpenMP was utilized to enable thread parallelism, while on the P100 OpenACC was utilized to enable execution on the GPU. On both architectures a minimum problem size was required to saturate the performance on the node, with the GPU requiring a significantly larger problem 32-64x larger than the CPU. On the CPU several implementation of the floating point intensive small matrix multiply kernels were tested, the \"naive\" matrix multiply yielded the worst performance. Notable improvements were seen when the inner matrix multiply loop was handle unrolled, noted as the \"unrolled\" case in the figure. Finally the best CPU performance was seen utilizing customized AVX512 assembly routines from the XSMM library. On the GPU a \"naive\" matrix multiply routine was implemented, which outperforms the naive CPU code at high degrees of freedom, but lags the optimized CPU implementations. Further work on optimized GPU implementations is ongoing and is expected to yield improved GPU performance.", 
            "title": "Performance"
        }, 
        {
            "location": "/case_studies/md/", 
            "text": "Developing a new molecular dynamics code with an eye towards portability\n\n\nClassical molecular dynamics\n \nhas become a ubiquitous\ncomputational modeling tool for a number of disciplines,\nfrom biology and biochemistry, to geochemistry and polymer\nphysics.  Due to intense efforts from a\nnumber of developers over the past 50 years, several MD programs have been highly successful in achieving commendable\nefficiency and overall performance.\n\n\nThe classical molecular dynamics algorithm involves three\nmain components: the integration step, the calculation of\nshort-range forces, and the calculation of long-range\nforces. The integration step is generally the quickest part\nof the calculation, and as it has some memory-intensive\naspects, is often calculated using the CPU, in\nimplementations using heterogeneous architectures. The\nlong-range force calculation, in most implementations,\ninvolves an Ewald sum. This requires the use of Fourier transform\nmethods, which are fast for smaller systems, but do not\nscale well for large systems. This is an active area of\ndevelopment and is not addressed here. The\nmajor bottleneck for all system sizes is the short-range\nnon-bonded forces (SNFs) calculation, as it involves a sum\nof pairwise interactions over multiple subsets of the\nparticle space.\n\n\nAs part of our portable performance studies, we have written\na new SNF kernel, wherein we use directives (OpenACC) to\nimplement the parallel steps of the computation. We have also produced  an alternate\nimplementation where matrix-matrix multiplication is used to \ncalculate pairwise distances in the SNF calculation. This\nalternate implementation, though requiring more\nfloating-point operations, is shown to perform well because\nof the performance of platform-specific BLAS libraries.  \n\n\nDetails of the MD experiment can be found in this\n\nreport\n.", 
            "title": "MD"
        }, 
        {
            "location": "/case_studies/md/#developing-a-new-molecular-dynamics-code-with-an-eye-towards-portability", 
            "text": "Classical molecular dynamics  \nhas become a ubiquitous\ncomputational modeling tool for a number of disciplines,\nfrom biology and biochemistry, to geochemistry and polymer\nphysics.  Due to intense efforts from a\nnumber of developers over the past 50 years, several MD programs have been highly successful in achieving commendable\nefficiency and overall performance.  The classical molecular dynamics algorithm involves three\nmain components: the integration step, the calculation of\nshort-range forces, and the calculation of long-range\nforces. The integration step is generally the quickest part\nof the calculation, and as it has some memory-intensive\naspects, is often calculated using the CPU, in\nimplementations using heterogeneous architectures. The\nlong-range force calculation, in most implementations,\ninvolves an Ewald sum. This requires the use of Fourier transform\nmethods, which are fast for smaller systems, but do not\nscale well for large systems. This is an active area of\ndevelopment and is not addressed here. The\nmajor bottleneck for all system sizes is the short-range\nnon-bonded forces (SNFs) calculation, as it involves a sum\nof pairwise interactions over multiple subsets of the\nparticle space.  As part of our portable performance studies, we have written\na new SNF kernel, wherein we use directives (OpenACC) to\nimplement the parallel steps of the computation. We have also produced  an alternate\nimplementation where matrix-matrix multiplication is used to \ncalculate pairwise distances in the SNF calculation. This\nalternate implementation, though requiring more\nfloating-point operations, is shown to perform well because\nof the performance of platform-specific BLAS libraries.    Details of the MD experiment can be found in this report .", 
            "title": "Developing a new molecular dynamics code with an eye towards portability"
        }, 
        {
            "location": "/perfport/summary/", 
            "text": "Summary and Recommendations\n\n\nWe summarize below some of our high-level findings from the survey of available performance-portability options, the case-studies from the Office of Science \nworkload and from the outcome of recent DOE performance portability workshops.\n\n\nComparison of Leading Approaches\n\n\n\n\n\n\n\n\nApproach\n\n\nBenefits\n\n\nChallenges\n\n\n\n\n\n\n\n\n\n\nLibraries\n\n\nHighly portable, not dependent on compiler implementations.\n\n\nMany GPU libraries (e.g. CUFFT) are C only (requiring explicit interfaces to use in FORTRAN) and don't have common interfaces. May lock-in data layout. In many cases libraries don't exist for problem.\n\n\n\n\n\n\nOpenMP 4.5\n\n\nStandardized. Support for C, C++, FORTRAN and others. Simple to get started.\n\n\nLimited expressibility (particularly on GPUs). Lacks \"views\". Reliant on quality of compiler implementation - which are generally immature on both GPU and CPU systems.\n\n\n\n\n\n\nOpenACC\n\n\nStandardized. Support for C, C++, FORTRAN.\n\n\nLimited support in compilers, especially free compilers (e.g. GNU).\n\n\n\n\n\n\nKokkos\n\n\nAllows significant expressibility (particularly on GPUs.)\n\n\nOnly supports C++. Vector parallelism often left-out on CPUs.\n\n\n\n\n\n\nRaja\n\n\nAllows incremental enhancements to codes. Many back-ends.\n\n\nOnly supports C++. Lacks data \"views\" for more advanced portability requirements.\n\n\n\n\n\n\nDSLs\n\n\nHighest expressibility for appropriate problems\n\n\nLimited to only a small number of communities. Needs to be maintained and supported for new architectures.\n\n\n\n\n\n\n\n\nRecommendations\n\n\nAt present, the reality is that the options for writing performance portable code are limited in scope and are evolving \nrapidly (see individual approach pages for lists of pros and cons). Therefore, as we see in our case studies, it is likely some \nlevel of code divergence (\nIFDEF\n etc.) will be necessary to achieve code that performs near its ceiling on all of Cori, Theta, Titan and Summit. \n\n\nHowever, we've seen that performance-portable approaches and implementations are becoming more and more possible. This rate of \nimprovement makes this a good time to evaluate these approaches in your application with an eye towards devising a long-term strategy. \n\n\nIn general, we have the following recommendations for pursuing performance portable code:\n\n\n\n\n\n\nActively profile your application using our suggested tools to make sure you have identified a minimal set of performance critical regions. We recommend \nthat you read over the \nstrategy page\n page and consider your hotspots map to different strategies \nlayed out. Additionally, before diving into a particular performance portability approach, we recommend making sure your code is using code software-engineer \npractices and that performance-critical regions are abstracted from the bulk of the application.\n\n\n\n\n\n\nIf a well-supported library or DSL is available to address your performance critical regions, use it.\n\n\n\n\n\n\nIf you have an existing code that is \nnot\n written in C++, evaluate whether OpenMP 4.5 can support your application with minimal code differences.\nOpenACC is another possible path, and might be appropriate if you need to interoperate with a limited amount of GPU-specific code (e.g. a small amount of CUDA\nthat is used in a particularly performance-sensitive piece of code). However, the default level of maturity for OpenACC on the non-GPU platforms is an open question. \n\n\n\n\n\n\nIf you have an existing code that \nis\n written in C++, evaluate whether Kokkos, or Raja and OpenMP 4.5 (more incremental) can support your application with minimal code differences, \nconsidering the above discussion on strategy and the pros and cons for each approach. \n\n\n\n\n\n\nReach out to your DOE SC facility with use cases and deficiencies in these options so that we can actively push for changes in the upcoming \nframework releases and advocate in the standards bodies.", 
            "title": "Summary"
        }, 
        {
            "location": "/perfport/summary/#summary-and-recommendations", 
            "text": "We summarize below some of our high-level findings from the survey of available performance-portability options, the case-studies from the Office of Science \nworkload and from the outcome of recent DOE performance portability workshops.", 
            "title": "Summary and Recommendations"
        }, 
        {
            "location": "/perfport/summary/#comparison-of-leading-approaches", 
            "text": "Approach  Benefits  Challenges      Libraries  Highly portable, not dependent on compiler implementations.  Many GPU libraries (e.g. CUFFT) are C only (requiring explicit interfaces to use in FORTRAN) and don't have common interfaces. May lock-in data layout. In many cases libraries don't exist for problem.    OpenMP 4.5  Standardized. Support for C, C++, FORTRAN and others. Simple to get started.  Limited expressibility (particularly on GPUs). Lacks \"views\". Reliant on quality of compiler implementation - which are generally immature on both GPU and CPU systems.    OpenACC  Standardized. Support for C, C++, FORTRAN.  Limited support in compilers, especially free compilers (e.g. GNU).    Kokkos  Allows significant expressibility (particularly on GPUs.)  Only supports C++. Vector parallelism often left-out on CPUs.    Raja  Allows incremental enhancements to codes. Many back-ends.  Only supports C++. Lacks data \"views\" for more advanced portability requirements.    DSLs  Highest expressibility for appropriate problems  Limited to only a small number of communities. Needs to be maintained and supported for new architectures.", 
            "title": "Comparison of Leading Approaches"
        }, 
        {
            "location": "/perfport/summary/#recommendations", 
            "text": "At present, the reality is that the options for writing performance portable code are limited in scope and are evolving \nrapidly (see individual approach pages for lists of pros and cons). Therefore, as we see in our case studies, it is likely some \nlevel of code divergence ( IFDEF  etc.) will be necessary to achieve code that performs near its ceiling on all of Cori, Theta, Titan and Summit.   However, we've seen that performance-portable approaches and implementations are becoming more and more possible. This rate of \nimprovement makes this a good time to evaluate these approaches in your application with an eye towards devising a long-term strategy.   In general, we have the following recommendations for pursuing performance portable code:    Actively profile your application using our suggested tools to make sure you have identified a minimal set of performance critical regions. We recommend \nthat you read over the  strategy page  page and consider your hotspots map to different strategies \nlayed out. Additionally, before diving into a particular performance portability approach, we recommend making sure your code is using code software-engineer \npractices and that performance-critical regions are abstracted from the bulk of the application.    If a well-supported library or DSL is available to address your performance critical regions, use it.    If you have an existing code that is  not  written in C++, evaluate whether OpenMP 4.5 can support your application with minimal code differences.\nOpenACC is another possible path, and might be appropriate if you need to interoperate with a limited amount of GPU-specific code (e.g. a small amount of CUDA\nthat is used in a particularly performance-sensitive piece of code). However, the default level of maturity for OpenACC on the non-GPU platforms is an open question.     If you have an existing code that  is  written in C++, evaluate whether Kokkos, or Raja and OpenMP 4.5 (more incremental) can support your application with minimal code differences, \nconsidering the above discussion on strategy and the pros and cons for each approach.     Reach out to your DOE SC facility with use cases and deficiencies in these options so that we can actively push for changes in the upcoming \nframework releases and advocate in the standards bodies.", 
            "title": "Recommendations"
        }, 
        {
            "location": "/resources/", 
            "text": "Other Resources\n\n\nCross Lab Meetings\n\n\n\n\n\n\nDOE COE Portability Meeting 2016\n\n\n\n\n\n\nDOE COE Portability Meeting 2017\n\n\n\n\n\n\nPortability Across Labs\n\n\n\n\n\n\n\"Exascale Scientific Applications: Scalability and Performance Portability\"\n - A forthcoming book containing a set of case studies drawn from the application portfolios of the ASCR facilities. \n\n\n\n\n\n\nApplication Readiness Case Studies\n\n\n\n\n\n\nNERSC\n\n\n\n\n\n\nNERSC Case Studies\n\n\n\n\n\n\nNERSC Papers\n\n\n\n\n\n\n\n\n\n\nOLCF\n\n\n\n\n\n\nOLCF Center for Accelerated Application Readiness (CAAR)\n\n\n\n\n\n\nOLCF CAAR and performance portability publications\n\n\n\n\n\n\n\n\n\n\nALCF\n\n\n\n\n\n\nCUG 2016\n  presentations\n\n  from ALCF Theta Early Science Program\n\n\n\n\n\n\nALCF Theta Early Science Program Technical Reports - these these\n  describe the code development, porting, and optimization efforts of each\n  of the \n12\n  projects\n\n  in the ALCF Theta \nEarly Science Program\n. \nComing in Fall 2018.", 
            "title": "Other Resources"
        }, 
        {
            "location": "/resources/#other-resources", 
            "text": "", 
            "title": "Other Resources"
        }, 
        {
            "location": "/resources/#cross-lab-meetings", 
            "text": "DOE COE Portability Meeting 2016    DOE COE Portability Meeting 2017    Portability Across Labs    \"Exascale Scientific Applications: Scalability and Performance Portability\"  - A forthcoming book containing a set of case studies drawn from the application portfolios of the ASCR facilities.", 
            "title": "Cross Lab Meetings"
        }, 
        {
            "location": "/resources/#application-readiness-case-studies", 
            "text": "NERSC    NERSC Case Studies    NERSC Papers      OLCF    OLCF Center for Accelerated Application Readiness (CAAR)    OLCF CAAR and performance portability publications      ALCF    CUG 2016\n  presentations \n  from ALCF Theta Early Science Program    ALCF Theta Early Science Program Technical Reports - these these\n  describe the code development, porting, and optimization efforts of each\n  of the  12\n  projects \n  in the ALCF Theta  Early Science Program .  Coming in Fall 2018.", 
            "title": "Application Readiness Case Studies"
        }
    ]
}